{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "# OR if you have a GPU runtime and want GPU support:\n",
        "# !pip install faiss-gpu"
      ],
      "metadata": {
        "id": "q0L0vf7-z_uU"
      },
      "execution_count": null,
      "outputs": [],
      "id": "q0L0vf7-z_uU"
    },
    {
      "cell_type": "code",
      "id": "OgOtT8AbpfDVl120Sx0YEyTw",
      "metadata": {
        "tags": [],
        "id": "OgOtT8AbpfDVl120Sx0YEyTw",
        "cellView": "form"
      },
      "source": [
        "#@title 20 deep research = live search\n",
        "import asyncio\n",
        "import time\n",
        "import json\n",
        "import traceback # For detailed error logging\n",
        "import re # For parsing\n",
        "import numpy as np\n",
        "import os\n",
        "import glob # For listing session files\n",
        "import hashlib\n",
        "from typing import List, Dict, Any, Optional, Set, Tuple, Union\n",
        "\n",
        "# --- Configuration Constants ---\n",
        "VERTEX_PROJECT_ID = \"ofworks-459904\"\n",
        "VERTEX_LOCATION = \"us-central1\"\n",
        "EMBEDDING_API_CALL_DELAY_SECONDS = 5.0 # For individual embedding calls\n",
        "\n",
        "GEMINI_CONFIG = {\n",
        "    'flash_lite': { 'model_name': 'gemini-2.5-flash-preview-05-20', 'max_tokens': 50000, 'temperature': 0.3, 'top_p': 0.95, },\n",
        "    'researcher': { 'model_name': 'gemini-2.5-flash-preview-05-20', 'max_tokens': 50000, 'temperature': 0.3, 'top_p': 0.95, },\n",
        "    'planner': { 'model_name': 'gemini-2.5-flash-preview-05-20', 'max_tokens': 50000, 'temperature': 0.3, 'top_p': 0.95, },\n",
        "    'creator': { # New config for the Creator Agent\n",
        "        'model_name': 'gemini-2.5-flash-preview-05-20', # Use a powerful model\n",
        "        'max_tokens': 50000, # Increased for potentially large artifact generation\n",
        "        'temperature': 0.25, # Lower temperature for more deterministic JSON\n",
        "        'top_p': 0.95,\n",
        "    }\n",
        "}\n",
        "\n",
        "\n",
        "INITIAL_MAX_CYCLES_IF_NO_OTHER_STOP = 6 # As per your last test log\n",
        "FAISS_DIMENSION = 768\n",
        "\n",
        "SESSIONS_DIR = \"research_sessions_data_v8\" # Matching your 16th code\n",
        "GLOBAL_RAG_DIR = os.path.join(SESSIONS_DIR, \"global_rag\")\n",
        "FAISS_INDEX_GCS_BUCKET_NAME = \"faiss_buck\"\n",
        "FAISS_INDEX_GCS_BLOB_NAME = \"research_agent_v8/global_faiss.index\" # Matching _v8\n",
        "GLOBAL_DOC_STORE_GCS_BLOB_NAME = \"research_agent_v8/global_document_store.json\" # Matching _v8\n",
        "GLOBAL_FAISS_MAP_GCS_BLOB_NAME = \"research_agent_v8/global_faiss_map.json\"    # Matching _v8\n",
        "GLOBAL_DOC_STORE_PATH = os.path.join(GLOBAL_RAG_DIR, \"local_fallback_document_store_v8.json\")\n",
        "GLOBAL_FAISS_MAP_PATH = os.path.join(GLOBAL_RAG_DIR, \"local_fallback_faiss_map_v8.json\")\n",
        "TEMP_FAISS_INDEX_PATH = os.path.join(GLOBAL_RAG_DIR, \"temp_faiss_staging_v8.index\")\n",
        "LOCAL_FAISS_INDEX_PATH = os.path.join(GLOBAL_RAG_DIR, \"local_persistent_faiss_v8.index\")\n",
        "\n",
        "\n",
        "PLANNER_RAG_CONTEXT_TOP_K = 100\n",
        "PLANNER_CANDIDATE_REFINE_RAG_TOP_K = 40\n",
        "RAG_TOP_K = 50\n",
        "CREATOR_AGENT_RAG_TOP_K = 80 # Number of blueprint chunks for Creator to retrieve per sub-task query\n",
        "EMBEDDING_BATCH_SIZE = 20 # Kept from your 16th code\n",
        "\n",
        "# --- SDK Availability Flags & Global SDK Variables (Initialized to False/None) ---\n",
        "FAISS_AVAILABLE = False\n",
        "GOOGLE_GENAI_SDK_AVAILABLE = False\n",
        "GCS_AVAILABLE = False\n",
        "VERTEX_AI_SDK_INITIALIZED_SUCCESSFULLY = False\n",
        "TEXT_EMBEDDING_MODEL_CLASS_AVAILABLE = False # True if TextEmbeddingModel class is imported\n",
        "GOOGLE_API_CORE_EXCEPTIONS_AVAILABLE = False\n",
        "\n",
        "# These will hold the imported modules directly\n",
        "faiss = None\n",
        "genai = None\n",
        "genai_types = None\n",
        "storage = None\n",
        "vertexai = None\n",
        "TextEmbeddingModel = None\n",
        "TextEmbeddingInput = None\n",
        "google_api_core_exceptions = None\n",
        "\n",
        "# --- SDK Imports and Global Initialization ---\n",
        "print(\"--- Initializing SDKs ---\")\n",
        "\n",
        "# FAISS\n",
        "try:\n",
        "    import faiss as faiss_imported\n",
        "    faiss = faiss_imported\n",
        "    FAISS_AVAILABLE = True\n",
        "    print(\"Successfully imported faiss.\")\n",
        "except ImportError:\n",
        "    print(\"Warning: faiss library not found. RAG capabilities will be disabled.\")\n",
        "\n",
        "# Google Generative AI (for genai.Client)\n",
        "try:\n",
        "    from google import genai as genai_imported\n",
        "    from google.generativeai import types as genai_types_imported\n",
        "    genai = genai_imported\n",
        "    genai_types = genai_types_imported\n",
        "    GOOGLE_GENAI_SDK_AVAILABLE = True\n",
        "    print(\"Successfully imported google.genai and google.generativeai.types.\")\n",
        "except ImportError:\n",
        "    print(\"CRITICAL: Failed to import Google Generative AI SDK (google.genai). LLM calls via genai.Client will fail.\")\n",
        "\n",
        "# Google Cloud Storage\n",
        "try:\n",
        "    from google.cloud import storage as storage_imported\n",
        "    storage = storage_imported\n",
        "    GCS_AVAILABLE = True\n",
        "    print(\"Successfully imported google-cloud-storage.\")\n",
        "except ImportError:\n",
        "    print(\"Warning: google-cloud-storage library not found. GCS persistence will be disabled.\")\n",
        "\n",
        "# Vertex AI SDK (for Embeddings primarily)\n",
        "try:\n",
        "    import vertexai as vertexai_imported\n",
        "    from vertexai.language_models import TextEmbeddingModel as VertexTextEmbeddingModelImport\n",
        "    from vertexai.language_models import TextEmbeddingInput as VertexTextEmbeddingInputImport\n",
        "\n",
        "    vertexai = vertexai_imported\n",
        "    TextEmbeddingModel = VertexTextEmbeddingModelImport\n",
        "    TextEmbeddingInput = VertexTextEmbeddingInputImport\n",
        "    TEXT_EMBEDDING_MODEL_CLASS_AVAILABLE = True\n",
        "    print(\"Successfully imported Vertex AI SDK components (vertexai, TextEmbeddingModel, TextEmbeddingInput).\")\n",
        "\n",
        "    # Global Vertex AI SDK Initialization (ONLY ONCE and if needed)\n",
        "    if VERTEX_PROJECT_ID and VERTEX_LOCATION:\n",
        "        try:\n",
        "            print(f\"Attempting to initialize Vertex AI SDK globally for project '{VERTEX_PROJECT_ID}' and location '{VERTEX_LOCATION}'...\")\n",
        "            vertexai.init(project=VERTEX_PROJECT_ID, location=VERTEX_LOCATION) # Use the imported 'vertexai'\n",
        "            VERTEX_AI_SDK_INITIALIZED_SUCCESSFULLY = True\n",
        "            print(f\"Vertex AI SDK initialized globally for project '{VERTEX_PROJECT_ID}' and location '{VERTEX_LOCATION}'.\")\n",
        "\n",
        "            # Optional: Test pre-loading of embedding model (using 'gemini-embedding-001' as in your 16th code)\n",
        "            if TEXT_EMBEDDING_MODEL_CLASS_AVAILABLE: # Redundant check, but safe\n",
        "                try:\n",
        "                    print(\"Attempting to test-load embedding model 'gemini-embedding-001' globally...\")\n",
        "                    _ = TextEmbeddingModel.from_pretrained(\"gemini-embedding-001\")\n",
        "                    # VERTEX_AI_EMBEDDING_MODEL_LOADED = True # This flag was in your 16th code, can be removed if not used elsewhere\n",
        "                    print(\"Successfully tested pre-loading of 'gemini-embedding-001'.\")\n",
        "                except Exception as e_preload:\n",
        "                    print(f\"Warning: Could not test-load 'gemini-embedding-001': {e_preload}.\")\n",
        "        except Exception as e_vertex_init_global:\n",
        "            print(f\"CRITICAL ERROR initializing Vertex AI SDK globally: {e_vertex_init_global}\")\n",
        "            print(\"Ensure project/location are correct, Vertex AI API is enabled, and you have correct permissions.\")\n",
        "    else:\n",
        "        print(f\"Vertex AI SDK global initialization skipped: VERTEX_PROJECT_ID or VERTEX_LOCATION not set.\")\n",
        "except ImportError as e_sdk_main:\n",
        "    print(f\"CRITICAL WARNING: Failed to import core Vertex AI SDK components: {e_sdk_main}. Real embeddings will be disabled.\")\n",
        "\n",
        "# Google API Core Exceptions\n",
        "try:\n",
        "    from google.api_core import exceptions as google_api_exceptions_imported\n",
        "    google_api_core_exceptions = google_api_exceptions_imported\n",
        "    GOOGLE_API_CORE_EXCEPTIONS_AVAILABLE = True\n",
        "    print(\"Successfully imported google.api_core.exceptions.\")\n",
        "except ImportError:\n",
        "    print(\"Warning: google.api_core.exceptions not found. Specific API error handling might be less precise.\")\n",
        "\n",
        "# IPython Display Modules\n",
        "IPYTHON_AVAILABLE = False\n",
        "try:\n",
        "    from IPython.display import display, HTML, clear_output\n",
        "    IPYTHON_AVAILABLE = True\n",
        "    print(\"IPython display modules loaded.\")\n",
        "except ImportError:\n",
        "    def clear_output(wait=False): pass\n",
        "    def display(obj): print(obj)\n",
        "    class HTML:\n",
        "        def __init__(self, data): self.data = data\n",
        "        def __str__(self): return str(self.data)\n",
        "        def _repr_html_(self): return str(self.data)\n",
        "    print(\"IPython display modules not found. Using basic print for output.\")\n",
        "\n",
        "print(\"--- SDK Initialization Complete ---\")\n",
        "\n",
        "# --- Utility Functions ---\n",
        "def sanitize_filename(query: str, max_len: int = 50) -> str:\n",
        "    # ... (your sanitize_filename function) ...\n",
        "    s = re.sub(r'[^\\w\\s-]', '', query).strip().lower()\n",
        "    s = re.sub(r'[-\\s]+', '-', s)\n",
        "    return s[:max_len] if len(s) > max_len else (s or \"unnamed_session\")\n",
        "\n",
        "\n",
        "# --- System Prompts --- (Identical to previous version, no changes needed here)\n",
        "BASE_SYSTEM_PROMPT_CORE = \"\"\"You are an Advanced AI Deep Research Agent & Strategic Problem Solver. Your primary mission is to conduct exceptionally comprehensive, multi-faceted, and in-depth research to answer complex user queries with profound understanding and insight. You will simulate an advanced reasoning-based problem-solving expert, leveraging information gathering tools. Your entire cognitive process, from initial query deconstruction to final synthesis and critical self-reflection, must be meticulously detailed within <inner_thoughts> tags. This internal monologue will be invisible to the end-user but will serve as a testament to your rigorous, human-like research methodology.\n",
        "\n",
        "Formatting and Style for <inner_thoughts>:\n",
        "- Granular Step-by-Step Reasoning: Each paragraph strictly one sentence, detailing one micro-step in your thought or action.\n",
        "- Explicit Tool Use: Always state: \"I will use the [Tool Name, e.g., Google Search tool / Internal Faiss Index] with the query/parameters: [...]\". If using Google Search, you will be provided with the results. If using the Internal Faiss Index, the system will provide you with relevant document snippets.\n",
        "- Embrace Failure & Iteration: Document failed search attempts or hypotheses and explain how they inform your next steps. Show the \"tree-like path search\" clearly.\n",
        "- Quantitative & Qualitative Balance: Seek out data when relevant, but also understand and synthesize qualitative information like opinions and arguments.\n",
        "- Relentless Exploration: Do not conclude prematurely. The goal is depth.\n",
        "\n",
        "Final Output Format (Strictly Adhered To):\n",
        "Your final response MUST be in the following format, and ONLY this format:\n",
        "<inner_thoughts>\n",
        "(Your detailed thought process for the current task.)\n",
        "</inner_thoughts>\n",
        "<final_answer>\n",
        "(Your direct output for this specific task, structured as requested by the role. If outputting JSON, it must be RAW JSON within these tags, no markdown fences like ```json.)\n",
        "</final_answer>\n",
        "\"\"\"\n",
        "\n",
        "PLANNER_AGENT_SYSTEM_PROMPT = BASE_SYSTEM_PROMPT_CORE + \"\"\"\n",
        "Your current role is **Strategic Research Planner with Progressive Refinement**.\n",
        "Your task is to analyze the main user query and the comprehensive current state of the research knowledge base (which includes all findings, key insights, and identified knowledge gaps from both web searches and internal document searches).\n",
        "\n",
        "CRITICAL INSTRUCTIONS FOR PROGRESSIVE RESEARCH:\n",
        "1.  **Build Upon Previous Findings**: Every new query you suggest MUST logically follow from and aim to expand upon specific facts, fill identified gaps, or resolve contradictions noted in the 'COMPLETE RESEARCH FINDINGS' or 'Identified Knowledge Gaps' sections of the provided knowledge base summary. Explicitly state in your <inner_thoughts> how your proposed queries relate to previous findings or gaps.\n",
        "2.  **Consider All Knowledge Sources**: The Knowledge Base may contain information from previous web searches AND from an internal document index (Faiss). Your planning should aim to synthesize and build upon information from ALL available sources. If the KB summary indicates strong leads from internal documents, consider if further refinement of internal searches is needed, but your primary output is for WEB search queries.\n",
        "3.  **Avoid Repetition**: Do NOT suggest queries that are substantially similar to ones already listed as 'Completed Sub-Queries' in the knowledge base summary unless you are aiming to verify a specific contradiction or explore a nuanced aspect that previous searches missed. If so, justify this clearly in your <inner_thoughts>.\n",
        "4.  **Drill Down Strategy**: Transition from broad exploratory searches (if any were initial) to more focused, investigative queries. If general topics are covered, your new queries should target detailed aspects, specific entities, or relationships uncovered.\n",
        "5.  **Fill Knowledge Gaps**: Prioritize queries that directly address the 'Identified Knowledge Gaps Needing Investigation' section from the knowledge base summary. Your queries should be formulated to find answers to these specific open questions.\n",
        "6.  **Test Hypotheses/Explore Leads**: If previous research suggests possibilities or unconfirmed leads, you can formulate queries to verify these or investigate them further.\n",
        "\n",
        "QUERY FORMULATION RULES:\n",
        "- Each query should aim to be MORE SPECIFIC or explore a DIFFERENT, LOGICALLY CONNECTED ANGLE than previous related queries.\n",
        "- When possible, phrase queries to seek new information. For example, instead of \"details about X\", if \"X overview\" was done, try \"specific impact of X on Y\" or \"comparative analysis of X and Z\".\n",
        "- Include enough context in your <inner_thoughts> to show the reasoning for each query based on the provided knowledge base.\n",
        "\n",
        "Before suggesting any query, critically ask yourself:\n",
        "- Does this query directly build upon a specific previous finding (from web or internal docs) or clearly address an identified knowledge gap from the KB summary?\n",
        "- Is this query distinct enough from completed queries to yield new, valuable information?\n",
        "- Will this query take the research significantly deeper or broader in a *meaningful* way?\n",
        "- Am I proposing to re-search something that already has a satisfactory answer in the KB?\n",
        "\n",
        "Output 1 to 3 highly targeted queries that progressively deepen understanding. These queries are primarily intended for WEB SEARCH.\n",
        "If, after careful analysis of the *entire* knowledge base summary, you determine that the research is sufficiently comprehensive and no more *valuable progressive* web search queries can be formulated to significantly enhance the answer to the original user query, output an empty list: `[]` within the <final_answer> tags.\n",
        "\n",
        "Example <final_answer> format for new searches:\n",
        "<final_answer>\n",
        "[\n",
        "  \"specific side effects of DrugX in elderly patients with condition Y based on 2023 clinical trials\",\n",
        "  \"comparative efficacy of DrugX versus DrugZ for treating condition Y as reported in meta-analyses\",\n",
        "  \"long-term patient outcomes for DrugX condition Y after 5 years of treatment\"\n",
        "]\n",
        "</final_answer>\n",
        "\n",
        "Example <final_answer> format if research is complete:\n",
        "<final_answer>\n",
        "[]\n",
        "</final_answer>\n",
        "\"\"\"\n",
        "\n",
        "SEARCH_ANALYZER_AGENT_SYSTEM_PROMPT = BASE_SYSTEM_PROMPT_CORE + \"\"\"\n",
        "Your current role is **Comprehensive Search Result Analyzer & Multi-Source Information Synthesizer**.\n",
        "\n",
        "You will be given the following inputs:\n",
        "1.  A specific `search_query_executed` that you need to analyze.\n",
        "2.  `INTERNAL KNOWLEDGE BASE (FAISS INDEX) FINDINGS`: A JSON list of objects, where each object represents a potentially relevant document snippet from an internal knowledge base. This list might be empty if no relevant internal documents were found.\n",
        "3.  `EXTERNAL GOOGLE SEARCH FINDINGS`: A JSON list of objects, where each object represents a (simulated) Google Search result. This list will always be provided.\n",
        "\n",
        "YOUR PRIMARY TASK IS TO:\n",
        "1.  **Critically Analyze ALL Provided Information:**\n",
        "    *   **Internal Faiss Index Results:** Review the provided document summaries/snippets. Extract key information *directly relevant* to the `search_query_executed`. For each relevant internal document, note its Document ID. Assess its relevance to the query (0.0 to 1.0).\n",
        "    *   **Google Search Results:** Review the provided search result summaries. For each promising external source (select up to 3-5 most relevant from the provided list):\n",
        "        *   Identify the source URL and title.\n",
        "        *   Extract the most critical pieces of information or direct quotes that are *directly relevant* to the `search_query_executed`.\n",
        "        *   Assess its relevance to the query (0.0 to 1.0) and briefly comment on its potential credibility if discernible.\n",
        "2.  **Synthesize Findings:**\n",
        "    *   Combine and synthesize the extracted information from *both* the Internal Faiss Index results AND the Google Search results into a coherent `overall_synthesis_for_this_query`. This synthesis should directly address the `search_query_executed`.\n",
        "    *   Clearly attribute information to its source within the synthesis if helpful (e.g., \"Internal document X suggests..., while external source Y states...\").\n",
        "3.  **Identify New Questions/Leads:**\n",
        "    *   Based on the combined findings from all sources, identify any new questions, contradictions, or specific avenues for further research that arise directly from your analysis of *this specific `search_query_executed`*.\n",
        "\n",
        "OUTPUT FORMAT:\n",
        "YOUR ENTIRE RESPONSE WITHIN THE <final_answer> TAGS MUST BE A SINGLE, VALID JSON OBJECT.\n",
        "DO NOT OUTPUT A LIST OF STRINGS OR ANY OTHER FORMAT.\n",
        "THE JSON OBJECT MUST STRICTLY ADHERE TO THE FOLLOWING STRUCTURE, CONTAINING ALL SPECIFIED KEYS, even if some values are empty lists or null where appropriate (e.g., if no relevant internal documents were found, `internal_faiss_results_summary` would be `[]`).\n",
        "\n",
        "The `google_search_needed_and_performed` field in your JSON output MUST always be `true`.\n",
        "\n",
        "EXAMPLE OF USER CONTENT AND YOUR EXPECTED <final_answer>:\n",
        "\n",
        "Hypothetical User Content Provided to You:\n",
        "```\n",
        "You are analyzing the query: \"Impact of quantum computing on RSA encryption\"\n",
        "\n",
        "INTERNAL KNOWLEDGE BASE (FAISS INDEX) FINDINGS (Review and incorporate if relevant):\n",
        "[\n",
        "  {\n",
        "    \"internal_doc_id\": \"doc_qc_001\",\n",
        "    \"summary_of_internal_doc_content\": \"Shor's algorithm, runnable on a sufficiently powerful quantum computer, can factor large integers, breaking RSA. Current quantum computers are too small and error-prone for practical RSA cracking.\",\n",
        "    \"relevance_score_from_faiss\": 0.95\n",
        "  },\n",
        "  {\n",
        "    \"internal_doc_id\": \"doc_crypto_standards_v2\",\n",
        "    \"summary_of_internal_doc_content\": \"Post-quantum cryptography (PQC) standards are being developed by NIST. Several lattice-based and hash-based signature schemes are candidates. Transitioning will be a multi-year effort.\",\n",
        "    \"relevance_score_from_faiss\": 0.7\n",
        "  }\n",
        "]\n",
        "\n",
        "EXTERNAL GOOGLE SEARCH FINDINGS (Review, select most relevant, and incorporate):\n",
        "[\n",
        "  {\n",
        "    \"source_url\": \"http://quantumthreat.example.com/shors_explained_rsa\",\n",
        "    \"source_title\": \"Shor's Algorithm: The RSA Doomsday Machine - Explained\",\n",
        "    \"key_info_provisional\": \"Detailed explanation of how Shor's algorithm exploits quantum parallelism to factor integers exponentially faster than classical algorithms, making RSA vulnerable. Discusses the number of qubits required.\",\n",
        "    \"snippet_from_search_engine\": \"Shor's algorithm, if run on a large-scale quantum computer, would break RSA...\"\n",
        "  },\n",
        "  {\n",
        "    \"source_url\": \"http://nist.pqc.example.gov/updates_2024\",\n",
        "    \"source_title\": \"NIST PQC Standardization Update - March 2024\",\n",
        "    \"key_info_provisional\": \"NIST has selected several algorithms for standardization in its Post-Quantum Cryptography project, including CRYSTALS-Kyber and CRYSTALS-Dilithium. Draft standards are expected soon.\",\n",
        "    \"snippet_from_search_engine\": \"NIST announces new PQC algorithm selections for future cryptographic standards...\"\n",
        "  },\n",
        "  {\n",
        "    \"source_url\": \"http://quantumrealitycheck.example.org/timeline_skeptic\",\n",
        "    \"source_title\": \"Quantum Computing: Hype vs. Reality for Cryptography\",\n",
        "    \"key_info_provisional\": \"Argues that while theoretically possible, building a quantum computer capable of breaking RSA-2048 is still decades away due to immense challenges in qubit coherence, error correction, and scaling.\",\n",
        "    \"snippet_from_search_engine\": \"Debate continues on the practical timeline for quantum computers breaking RSA...\"\n",
        "  }\n",
        "]\n",
        "\n",
        "YOUR TASK:\n",
        "Critically analyze ALL the provided information (Internal Faiss Index findings AND External Google Search findings).\n",
        "Synthesize these findings into the comprehensive JSON output format specified by your role.\n",
        "Remember, the 'google_search_needed_and_performed' field in your JSON output MUST be `true`.\n",
        "```\n",
        "\n",
        "Your <final_answer> for the example above:\n",
        "<final_answer>\n",
        "{\n",
        "  \"search_query_executed\": \"Impact of quantum computing on RSA encryption\",\n",
        "  \"internal_faiss_results_summary\": [\n",
        "    {\n",
        "      \"internal_doc_id\": \"doc_qc_001\",\n",
        "      \"relevance_score_to_query\": 0.95,\n",
        "      \"key_extracted_info_from_internal\": \"Shor's algorithm, when run on a sufficiently powerful quantum computer, can break RSA by factoring large integers. However, current quantum computers are too small and error-prone for this to be a practical threat yet.\"\n",
        "    },\n",
        "    {\n",
        "      \"internal_doc_id\": \"doc_crypto_standards_v2\",\n",
        "      \"relevance_score_to_query\": 0.7,\n",
        "      \"key_extracted_info_from_internal\": \"NIST is actively developing Post-Quantum Cryptography (PQC) standards, with lattice-based and hash-based schemes as leading candidates, to prepare for the eventual quantum threat. Transitioning to PQC will be a significant, multi-year undertaking.\"\n",
        "    }\n",
        "  ],\n",
        "  \"google_search_needed_and_performed\": true,\n",
        "  \"google_search_results_summary\": [\n",
        "    {\n",
        "      \"source_url\": \"http://quantumthreat.example.com/shors_explained_rsa\",\n",
        "      \"source_title\": \"Shor's Algorithm: The RSA Doomsday Machine - Explained\",\n",
        "      \"relevance_score\": 0.9,\n",
        "      \"key_extracted_info\": \"Shor's algorithm leverages quantum parallelism for exponential speedup in integer factorization, rendering RSA vulnerable given a capable quantum computer. The number of stable qubits required is substantial.\"\n",
        "    },\n",
        "    {\n",
        "      \"source_url\": \"http://nist.pqc.example.gov/updates_2024\",\n",
        "      \"source_title\": \"NIST PQC Standardization Update - March 2024\",\n",
        "      \"relevance_score\": 0.85,\n",
        "      \"key_extracted_info\": \"NIST has advanced its Post-Quantum Cryptography project by selecting algorithms like CRYSTALS-Kyber (for key establishment) and CRYSTALS-Dilithium (for digital signatures) for standardization, with draft standards anticipated.\"\n",
        "    },\n",
        "    {\n",
        "      \"source_url\": \"http://quantumrealitycheck.example.org/timeline_skeptic\",\n",
        "      \"source_title\": \"Quantum Computing: Hype vs. Reality for Cryptography\",\n",
        "      \"relevance_score\": 0.75,\n",
        "      \"key_extracted_info\": \"Significant debate exists regarding the practical timeline for quantum computers to break widely used RSA encryption (e.g., RSA-2048), with some experts estimating it is still decades away due to major challenges in qubit coherence, error correction, and overall system scaling.\"\n",
        "    }\n",
        "  ],\n",
        "  \"overall_synthesis_for_this_query\": \"Quantum computing, particularly via Shor's algorithm, poses a fundamental long-term threat to RSA encryption's security by enabling efficient factorization of large integers. This is confirmed by both internal knowledge (doc_qc_001) and external sources (quantumthreat.example.com). While the theoretical vulnerability is clear, practical exploitation is currently hindered by the limited scale and high error rates of existing quantum computers, with timelines for cryptographically relevant quantum computers estimated to be at least a decade or more away (quantumrealitycheck.example.org, doc_qc_001). In response, standardization bodies like NIST are actively developing and selecting post-quantum cryptographic algorithms (e.g., CRYSTALS-Kyber, CRYSTALS-Dilithium) to ensure future security (doc_crypto_standards_v2, nist.pqc.example.gov).\",\n",
        "  \"new_questions_or_leads_from_this_query\": [\n",
        "    \"What are the specific qubit count and coherence time requirements for Shor's algorithm to break RSA-2048 or RSA-4096?\",\n",
        "    \"What is the current status of the NIST PQC draft standards and their projected adoption timeline?\",\n",
        "    \"Are there any emerging quantum algorithms, other than Shor's, that could potentially threaten current asymmetric cryptography?\"\n",
        "  ],\n",
        "  \"tool_errors\": null\n",
        "}\n",
        "</final_answer>\n",
        "\n",
        "If no relevant internal Faiss documents were found, `internal_faiss_results_summary` should be `[]`.\n",
        "If the provided Google Search results are deemed not relevant after your analysis, `google_search_results_summary` should be `[]`.\n",
        "However, `google_search_needed_and_performed` MUST always be `true`.\n",
        "\"\"\"\n",
        "\n",
        "CONSOLIDATOR_AGENT_SYSTEM_PROMPT = BASE_SYSTEM_PROMPT_CORE + \"\"\"\n",
        "Your current role is **Research Consolidator & Final Reporter**.\n",
        "You will be given the complete research log, which includes the original user query, all search queries executed (if any research phase occurred), and the analyzed findings or created artifacts from those processes. This information will be presented as a comprehensive \"Knowledge Base Summary\".\n",
        "\n",
        "YOUR TASK:\n",
        "1.  **Understand the Original User Query:** Identify the core question(s) or objective(s) the user wants to achieve.\n",
        "2.  **Synthesize ALL Available Information:** Review the *entire* \"Knowledge Base Summary\". This summary contains:\n",
        "    *   The original user query.\n",
        "    *   A log of detailed findings from any research (SearchAnalyzer outputs).\n",
        "    *   A collection of artifacts generated by the Creator Agent (if that phase was triggered).\n",
        "    *   A list of completed and pending sub-queries.\n",
        "    *   Any errors encountered during the process.\n",
        "3.  **Construct the Final Answer:**\n",
        "    *   If the primary goal was **research and answering questions**, synthesize the findings from the `detailed_findings` log into a comprehensive, well-structured, and insightful answer to the original user query. Address all aspects of the query. Highlight key findings, different perspectives, and any limitations or gaps in the available information based on the research performed.\n",
        "    *   If the primary goal was **creation/elaboration of design artifacts** (and the Creator Agent produced output now in `created_artifacts` within the KB summary), your main task is to present these created artifacts clearly. You might provide a brief introduction and conclusion around the presented artifacts. If the Creator Agent reported limitations or errors, summarize those as well.\n",
        "    *   **Clarity and Attribution:** Clearly attribute information if sources are distinct and relevant (e.g., \"Based on internal document X...\", \"External source Y suggested...\"). For created artifacts, the structure itself is the primary output.\n",
        "4.  **Output Format:** The output should be the final, polished answer intended for the end-user. It should be directly readable and understandable, presented within the `<final_answer>` tags. For complex outputs like architectural designs, ensure the structure is clear.\n",
        "\n",
        "CRITICAL: Your output within `<final_answer>` should be the direct response to the user. If the process involved creating specific artifacts (like diagrams or formalizations), ensure these are clearly presented as part of your answer. If the research phase was skipped (e.g., for a pure creation task based on a loaded blueprint) or failed, focus on presenting the created artifacts or explaining the limitations.\n",
        "\"\"\"\n",
        "\n",
        "CREATOR_PLANNER_SYSTEM_PROMPT = BASE_SYSTEM_PROMPT_CORE + \"\"\"\n",
        "Your current role is **AI Design Project Manager (Creator - Planning Phase)**.\n",
        "You are given:\n",
        "1.  A high-level `elaboration_task` (the user's original query) that requires creating multiple detailed design artifacts for an existing AI architecture blueprint.\n",
        "2.  A `blueprint_summary` containing the details of this AI architecture (e.g., \"CognitoAbstractor V1\").\n",
        "3.  A `max_creation_cycles` number indicating how many iterative cycles are available for the creation process.\n",
        "\n",
        "YOUR TASK:\n",
        "1.  **Deconstruct the `elaboration_task`**: Identify all distinct design artifacts requested (e.g., high-level diagram, specific interaction protocol, CIAL formalization, knowledge schema, motivation signal logic).\n",
        "2.  **Create a Step-by-Step Plan**: Formulate a plan to generate these artifacts iteratively over the available `max_creation_cycles`. Each step in your plan should be a manageable sub-task focused on generating one specific artifact or a significant portion of one.\n",
        "    *   For each sub-task in your plan, briefly describe:\n",
        "        *   `sub_task_id`: A unique, descriptive ID for the sub-task (e.g., \"artifact_1_diagram_high_level\").\n",
        "        *   `sub_task_description`: What specific artifact or part will be created.\n",
        "        *   `estimated_cycles`: How many cycles this sub-task might take (usually 1).\n",
        "        *   `required_blueprint_info_keywords`: 2-3 keywords or phrases that, if used in a RAG query against the `blueprint_summary`, would retrieve the most relevant information from the blueprint needed for *this specific sub-task*. This helps focus information retrieval.\n",
        "3.  **Output Format**: Your entire response within the <final_answer> tags MUST be a SINGLE, VALID JSON OBJECT.\n",
        "    This JSON object MUST have a **single top-level key named \"creation_plan\"**.\n",
        "    The value of \"creation_plan\" MUST be a JSON LIST of sub-task objects, where each sub-task object follows the schema described above.\n",
        "\n",
        "CRITICAL OUTPUT EXAMPLE (Illustrates the full JSON object structure):\n",
        "<final_answer>\n",
        "{\n",
        "  \"creation_plan\": [\n",
        "    {\n",
        "      \"sub_task_id\": \"artifact_1_diagram\",\n",
        "      \"sub_task_description\": \"Generate MermaidJS syntax for the high-level architectural diagram of CognitoAbstractor V1, showing main components and primary data/control flows based on the blueprint's description of the 'Central Workspace/Blackboard'.\",\n",
        "      \"estimated_cycles\": 1,\n",
        "      \"required_blueprint_info_keywords\": [\"CognitoAbstractor V1 components\", \"Central Workspace data flow\", \"module interactions diagram\"]\n",
        "    },\n",
        "    {\n",
        "      \"sub_task_id\": \"artifact_2_protocol_novelty\",\n",
        "      \"sub_task_description\": \"Detail the interaction protocol for the 'Novelty Detection & Causal Hypothesis Testing' scenario, focusing on steps 1-3: NPPRM novelty detection, DLSM curiosity trigger, and SRKIE initial symbolic grounding.\",\n",
        "      \"estimated_cycles\": 1,\n",
        "      \"required_blueprint_info_keywords\": [\"Novelty Detection protocol\", \"NPPRM function\", \"DLSM intrinsic motivation\", \"SRKIE symbolic grounding\"]\n",
        "    }\n",
        "    // ... potentially more sub-tasks if requested and cycles allow\n",
        "  ]\n",
        "}\n",
        "</final_answer>\n",
        "\n",
        "Ensure your output is ONLY this JSON object. Do not include any other text before or after the JSON structure within the <final_answer> tags.\n",
        "\"\"\"\n",
        "\n",
        "CREATOR_EXECUTOR_SYSTEM_PROMPT = BASE_SYSTEM_PROMPT_CORE + \"\"\"\n",
        "Your current role is **AI Design Artifact Generator (Creator - Execution Phase)**.\n",
        "You are given:\n",
        "1.  A specific `current_sub_task` to complete, which is part of a larger design elaboration. This sub-task will describe a specific design artifact you need to create.\n",
        "2.  `relevant_blueprint_context`: Snippets from the \"CognitoAbstractor V1\" architectural blueprint retrieved via RAG, specifically relevant to your `current_sub_task`. This is your primary information source.\n",
        "3.  The original `elaboration_task_overview` for overall context.\n",
        "\n",
        "YOUR TASK:\n",
        "1.  **Understand the `current_sub_task`**: Clearly identify what specific artifact or piece of an artifact you need to generate.\n",
        "2.  **Leverage `relevant_blueprint_context`**: Use the provided blueprint snippets as the factual basis for your generation. Your output MUST be consistent with and directly elaborate upon this context.\n",
        "3.  **Generate the Artifact Piece**: Create the content for the `current_sub_task`.\n",
        "    *   If the sub-task asks for a diagram (e.g., MermaidJS, PlantUML), provide the complete textual syntax for that diagram as a string.\n",
        "    *   If it asks for a protocol, formalism, schema, or conceptual equation, provide a clear, detailed textual description.\n",
        "4.  **Output Format**: Your entire response within the <final_answer> tags MUST be a SINGLE, VALID JSON OBJECT containing:\n",
        "    *   `artifact_id`: The `sub_task_id` you were given.\n",
        "    *   `generated_content`: The actual content of the design artifact piece you created (e.g., the Mermaid string, the textual description of the formalism).\n",
        "    *   `confidence_score`: Your confidence (0.0-1.0) that you successfully completed the sub-task based *only* on the provided blueprint context.\n",
        "    *   `notes_or_questions_for_next_step`: (Optional) Any brief notes, or if critical information was missing from the `relevant_blueprint_context` that is needed for *this specific sub-task*, phrase it as a question that could guide a RAG query against the full blueprint.\n",
        "\n",
        "Example <final_answer> if generating a Mermaid diagram:\n",
        "<final_answer>\n",
        "{\n",
        "  \"artifact_id\": \"artifact_1_diagram\",\n",
        "  \"generated_content\": \"graph TD\\\\n    A[CognitoAbstractor V1 Central Workspace] --> B(NPPRM);\\\\n    B --> A;\\\\n    A --> C(SRKIE);\\\\n    C --> A;\\\\n    A --> D(CIAL);\\\\n    D --> A;\\\\n    A --> E(ARM);\\\\n    E --> A;\\\\n    A --> F(DLSM);\\\\n    F --> A;\",\n",
        "  \"confidence_score\": 0.9,\n",
        "  \"notes_or_questions_for_next_step\": \"Blueprint context was clear for main components. Need more detail on specific data types exchanged for full sequence diagrams.\"\n",
        "}\n",
        "</final_answer>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# --- Helper Function for Parsing LLM Response ---\n",
        "def parse_llm_response(response_text: str) -> Tuple[str, str]:\n",
        "    # ... (Keep your existing parse_llm_response function from 16th code) ...\n",
        "    if not isinstance(response_text, str):\n",
        "        print(f\"Warning (parse_llm_response): Expected string input, got {type(response_text)}. Returning empty.\")\n",
        "        return \"\", f\"ERROR: Invalid input to parser (expected string, got {type(response_text)})\"\n",
        "    thoughts = \"\"\n",
        "    final_answer_str = \"\"\n",
        "    thought_match = re.search(r\"<inner_thoughts>(.*?)</inner_thoughts>\", response_text, re.DOTALL | re.IGNORECASE)\n",
        "    text_after_thoughts = response_text\n",
        "    if thought_match:\n",
        "        thoughts = thought_match.group(1).strip()\n",
        "        text_after_thoughts = response_text[thought_match.end():].strip()\n",
        "    else:\n",
        "        text_after_thoughts = response_text.strip()\n",
        "    answer_tag_match = re.search(r\"<final_answer>(.*?)</final_answer>\", text_after_thoughts, re.DOTALL | re.IGNORECASE)\n",
        "    content_within_final_answer_tag = \"\"\n",
        "    if answer_tag_match:\n",
        "        content_within_final_answer_tag = answer_tag_match.group(1).strip()\n",
        "    if content_within_final_answer_tag:\n",
        "        temp_str = content_within_final_answer_tag\n",
        "        if temp_str.startswith(\"```json\"): temp_str = temp_str[len(\"```json\"):].strip()\n",
        "        if temp_str.endswith(\"```\"): temp_str = temp_str[:-len(\"```\")].strip()\n",
        "        try: json.loads(temp_str); final_answer_str = temp_str\n",
        "        except json.JSONDecodeError:\n",
        "            embedded_json_match = re.search(r\"(\\{[\\s\\S]*?\\}|\\[[\\s\\S]*?\\])\", temp_str)\n",
        "            if embedded_json_match:\n",
        "                potential_json = embedded_json_match.group(0).strip()\n",
        "                try: json.loads(potential_json); final_answer_str = potential_json\n",
        "                except json.JSONDecodeError: final_answer_str = temp_str\n",
        "            else: final_answer_str = temp_str\n",
        "    if not final_answer_str or not (final_answer_str.strip().startswith(\"{\") or final_answer_str.strip().startswith(\"[\")):\n",
        "        search_area = text_after_thoughts if answer_tag_match else response_text\n",
        "        json_fenced_block_match = re.search(r\"```json\\s*(\\{[\\s\\S]*?\\}|\\[[\\s\\S]*?\\])\\s*```\", search_area, re.DOTALL)\n",
        "        if json_fenced_block_match:\n",
        "            potential_json_from_fence = json_fenced_block_match.group(1).strip()\n",
        "            try: json.loads(potential_json_from_fence); final_answer_str = potential_json_from_fence\n",
        "            except json.JSONDecodeError:\n",
        "                if not final_answer_str: final_answer_str = content_within_final_answer_tag or search_area.strip()\n",
        "    if not final_answer_str or not (final_answer_str.strip().startswith(\"{\") or final_answer_str.strip().startswith(\"[\")):\n",
        "        search_area = text_after_thoughts if answer_tag_match else response_text\n",
        "        raw_json_matches = list(re.finditer(r\"(\\{[\\s\\S]*?\\}|\\[[\\s\\S]*?\\])\", search_area))\n",
        "        for match_obj in reversed(raw_json_matches):\n",
        "            potential_json = match_obj.group(0).strip()\n",
        "            try: json.loads(potential_json); final_answer_str = potential_json; break\n",
        "            except json.JSONDecodeError: continue\n",
        "        if not (final_answer_str and (final_answer_str.strip().startswith(\"{\") or final_answer_str.strip().startswith(\"[\"))):\n",
        "             if not final_answer_str: final_answer_str = content_within_final_answer_tag or search_area.strip()\n",
        "    if not final_answer_str and response_text.strip().startswith(\"ERROR: LLM call failed\"):\n",
        "        final_answer_str = response_text.strip()\n",
        "    if not final_answer_str and not thoughts and response_text.strip() and not response_text.strip().startswith(\"ERROR:\"):\n",
        "        print(f\"Warning (parse_llm_response): No <final_answer> or JSON found. Using entire response: '{response_text[:100]}...'\")\n",
        "        final_answer_str = response_text.strip()\n",
        "    return thoughts, final_answer_str.strip()\n",
        "\n",
        "\n",
        "\n",
        "# --- KnowledgeBase Class (Session Specific) ---\n",
        "class KnowledgeBase:\n",
        "    def __init__(self, original_query: str, session_id: str):\n",
        "        self.session_id: str = session_id\n",
        "        self.original_query: str = original_query\n",
        "        self.main_topics_identified: List[str] = []\n",
        "        self.detailed_findings: List[Dict[str, Any]] = []\n",
        "        self.pending_sub_queries: List[str] = [original_query]\n",
        "        self.completed_sub_queries: Set[str] = set()\n",
        "        self.session_sources_referenced: Set[Tuple[str, str]] = set()\n",
        "        self.errors: List[Dict[str, Any]] = []\n",
        "        self.query_dependencies: Dict[str, List[str]] = {}\n",
        "        self.query_generation_reasons: Dict[str, str] = {}\n",
        "        self.global_faiss_doc_count: int = 0\n",
        "        self.creator_plan: List[Dict[str, Any]] = [] # Stores sub-tasks for the creator\n",
        "        self.creator_completed_subtasks_count: int = 0\n",
        "        self.created_artifacts: Dict[str, Any] = {} # Stores {artifact_name: content}\n",
        "\n",
        "    def add_search_analysis(self, query_executed: str, analysis_data: Dict[str, Any]):\n",
        "        try:\n",
        "            source_type = \"unknown_source\"\n",
        "            if \"internal_faiss_results_summary\" in analysis_data and \"google_search_results_summary\" in analysis_data :\n",
        "                 source_type = \"search_analyzer_synthesis\"\n",
        "            elif \"internal_faiss_results_summary\" in analysis_data:\n",
        "                source_type = \"rag_pre_analysis\"\n",
        "            elif \"google_search_results_summary\" in analysis_data:\n",
        "                source_type = \"web_search_analysis\"\n",
        "\n",
        "\n",
        "            finding = {\"query\": query_executed, \"analysis\": analysis_data, \"source_type\": source_type, \"timestamp\": time.time()}\n",
        "            self.detailed_findings.append(finding)\n",
        "\n",
        "            if query_executed in self.pending_sub_queries:\n",
        "                self.pending_sub_queries.remove(query_executed)\n",
        "            self.completed_sub_queries.add(query_executed)\n",
        "\n",
        "            if isinstance(analysis_data.get(\"internal_faiss_results_summary\"), list):\n",
        "                for item in analysis_data[\"internal_faiss_results_summary\"]:\n",
        "                    if isinstance(item, dict) and \"internal_doc_id\" in item:\n",
        "                        self.session_sources_referenced.add((\"faiss\", item[\"internal_doc_id\"]))\n",
        "            if isinstance(analysis_data.get(\"google_search_results_summary\"), list):\n",
        "                for item in analysis_data[\"google_search_results_summary\"]:\n",
        "                    if isinstance(item, dict) and \"source_url\" in item:\n",
        "                        self.session_sources_referenced.add((\"google\", item[\"source_url\"]))\n",
        "\n",
        "            new_leads_key = \"new_questions_or_leads_from_this_query\"\n",
        "\n",
        "            if isinstance(analysis_data.get(new_leads_key), list):\n",
        "                for q_new_candidate in analysis_data[new_leads_key]:\n",
        "                    if isinstance(q_new_candidate, str) and q_new_candidate.strip():\n",
        "                        q_new = q_new_candidate.strip()\n",
        "                        self.add_dependent_query(\n",
        "                            new_query=q_new,\n",
        "                            parent_queries=[query_executed],\n",
        "                            reason=f\"Identified as a new lead from {source_type} of '{query_executed}'\"\n",
        "                        )\n",
        "            if self.original_query in self.completed_sub_queries and self.original_query in self.pending_sub_queries:\n",
        "                 self.pending_sub_queries.remove(self.original_query)\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error in add_search_analysis for query '{query_executed}': {e}\"\n",
        "            print(error_msg); traceback.print_exc()\n",
        "            self.add_error(\"KnowledgeBase\", error_msg, {\"query\": query_executed, \"analysis_data_preview\": str(analysis_data)[:200]})\n",
        "\n",
        "    def add_dependent_query(self, new_query: str, parent_queries: List[str], reason: str):\n",
        "        try:\n",
        "            if not isinstance(new_query, str) or not new_query.strip(): return\n",
        "            valid_parent_queries = [p for p in parent_queries if isinstance(p, str) and p.strip()]\n",
        "            self.query_dependencies[new_query] = valid_parent_queries\n",
        "            self.query_generation_reasons[new_query] = reason if isinstance(reason, str) else \"No specific reason provided.\"\n",
        "            if new_query not in self.pending_sub_queries and new_query not in self.completed_sub_queries:\n",
        "                self.pending_sub_queries.append(new_query)\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error in add_dependent_query for '{new_query}': {e}\"; print(error_msg)\n",
        "            self.add_error(\"KnowledgeBase\", error_msg, {\"new_query\": new_query, \"parent_queries\": parent_queries})\n",
        "\n",
        "    def add_error(self, agent_name: str, error_message: str, details: Optional[Dict[str, Any]] = None):\n",
        "        self.errors.append({\"agent\": agent_name, \"message\": error_message, \"details\": details or {}, \"timestamp\": time.time()})\n",
        "\n",
        "    def get_summary_for_planner(self, max_len: int = 8000) -> str:\n",
        "        try:\n",
        "            summary = f\"Current Research Session ID: {self.session_id}\\nOriginal Query for this Session: {self.original_query}\\n\"\n",
        "            summary += f\"Main Topics Initially Identified (if any): {', '.join(self.main_topics_identified) or 'None yet'}\\n\\n\"\n",
        "            summary += f\"=== CURRENT KNOWLEDGE BASE STATE (Session: {self.session_id}) ===\\n\"\n",
        "            summary += f\"Pending Sub-Queries ({len(self.pending_sub_queries)}): {self.pending_sub_queries[:5]}{'...' if len(self.pending_sub_queries) > 5 else ''}\\n\"\n",
        "            completed_queries_list = list(self.completed_sub_queries)\n",
        "            summary += f\"Completed Sub-Queries ({len(completed_queries_list)}): {completed_queries_list[:10]}{'...' if len(completed_queries_list) > 10 else ''}\\n\"\n",
        "            summary += f\"Total Unique Sources Referenced This Session: {len(self.session_sources_referenced)}.\\n\"\n",
        "            summary += f\"Total Documents in Global Faiss Store: {self.global_faiss_doc_count}.\\n\\n\"\n",
        "\n",
        "            summary += f\"=== COMPLETE RESEARCH FINDINGS & INSIGHTS (Most recent first, this session) ===\\n\"\n",
        "            key_insights_found = []\n",
        "            all_identified_gaps = []\n",
        "            if not self.detailed_findings:\n",
        "                summary += \"No detailed research findings have been logged in this session yet.\\n\"\n",
        "            for finding_idx, finding in enumerate(reversed(self.detailed_findings)):\n",
        "                query_executed = finding.get(\"query\", \"N/A\")\n",
        "                analysis = finding.get(\"analysis\", {})\n",
        "                source_type = finding.get(\"source_type\", \"unknown_source\")\n",
        "\n",
        "                synthesis = analysis.get(\"overall_synthesis_for_this_query\", \"No synthesis for this finding.\")\n",
        "\n",
        "                summary += f\"\\n--- Finding from {source_type.replace('_',' ').title()} for Query: \\\"{query_executed}\\\" ---\\n\"\n",
        "                summary += f\"Synthesis: {synthesis[:500]}{'...' if len(synthesis) > 500 else ''}\\n\"\n",
        "                if synthesis and len(synthesis) > 30:\n",
        "                    key_insights_found.append(f\"From '{query_executed}' ({source_type}): {synthesis[:300]}...\")\n",
        "\n",
        "                internal_src_count = len(analysis.get(\"internal_faiss_results_summary\", [])) if isinstance(analysis.get(\"internal_faiss_results_summary\"),list) else 0\n",
        "                google_src_count = len(analysis.get(\"google_search_results_summary\", [])) if isinstance(analysis.get(\"google_search_results_summary\"),list) else 0\n",
        "\n",
        "                if internal_src_count > 0: summary += f\"  (Consulted {internal_src_count} internal docs)\\n\"\n",
        "                if google_src_count > 0: summary += f\"  (Consulted {google_src_count} Google Search results)\\n\"\n",
        "\n",
        "                new_leads_or_gaps = analysis.get(\"new_questions_or_leads_from_this_query\", [])\n",
        "                if new_leads_or_gaps:\n",
        "                    summary += f\"  Leads/Gaps from this query: {new_leads_or_gaps[:3]}{'...' if len(new_leads_or_gaps)>3 else ''}\\n\"\n",
        "                    all_identified_gaps.extend(q for q in new_leads_or_gaps if isinstance(q, str) and q.strip())\n",
        "                if finding_idx >= 9:\n",
        "                    summary += \"\\n...(older findings omitted for planner summary brevity)...\\n\"; break\n",
        "\n",
        "            summary += f\"\\n=== SUMMARY OF KEY INSIGHTS DISCOVERED (Overall - up to 10 recent) ===\\n\"\n",
        "            if key_insights_found:\n",
        "                for insight in key_insights_found[:10]: summary += f\" {insight}\\n\"\n",
        "            else: summary += \"No significant key insights extracted yet in this session.\\n\"\n",
        "\n",
        "            summary += f\"\\n=== IDENTIFIED KNOWLEDGE GAPS OR LEADS FOR FURTHER INVESTIGATION (Overall - unique, up to 8) ===\\n\"\n",
        "            unique_gaps = list(set(g for g in all_identified_gaps if isinstance(g, str) and g.strip()))\n",
        "            if unique_gaps:\n",
        "                for gap_idx, gap in enumerate(unique_gaps[:8]): summary += f\" {gap}\\n\"\n",
        "            else: summary += \"No specific knowledge gaps or new leads identified from analyses in this session yet.\\n\"\n",
        "\n",
        "            summary += f\"\\n=== STRATEGIC GUIDANCE FOR NEXT SEARCHES (Reminder for Planner Role) ===\\n\"\n",
        "            summary += \"- Focus on building upon specific facts/findings from the 'COMPLETE RESEARCH FINDINGS' section.\\n\"\n",
        "            summary += \"- Prioritize queries that directly address the 'IDENTIFIED KNOWLEDGE GAPS'.\\n\"\n",
        "            summary += \"- Ensure new queries are more specific or explore new, connected angles, avoiding repetition of 'Completed Sub-Queries'.\\n\"\n",
        "            summary += \"- Drill down into details rather than re-exploring broad topics already covered.\\n\"\n",
        "\n",
        "            if not self.pending_sub_queries and self.completed_sub_queries:\n",
        "                summary += \"\\nSTATUS: All previously identified sub-queries for this session seem to be completed. Evaluate carefully if the original query is fully addressed or if deeper, more specific drill-down queries are needed based on the insights and remaining subtle gaps.\\n\"\n",
        "            return summary[:max_len]\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating planner summary: {e}\"; print(error_msg)\n",
        "            self.add_error(\"KBSummaryPlanner\", error_msg)\n",
        "            return f\"Original Query: {self.original_query}\\nError: Unable to generate detailed summary.\"\n",
        "\n",
        "    def get_summary_for_consolidator(self, max_len: int = 30000) -> str:\n",
        "        try:\n",
        "            summary = f\"Current Research Session ID: {self.session_id}\\nOriginal User Query for this session: {self.original_query}\\n\\n\"\n",
        "            summary += f\"Total Documents in Global RAG Index: {self.global_faiss_doc_count}\\n\\n\"\n",
        "            summary += \"=== DETAILED FINDINGS LOG (This Session) ===\\n\"\n",
        "            if not self.detailed_findings:\n",
        "                summary += \"No detailed research findings have been logged in this session.\\n\"\n",
        "\n",
        "            for i, finding in enumerate(self.detailed_findings):\n",
        "                query_executed = finding.get(\"query\", \"N/A\")\n",
        "                analysis = finding.get(\"analysis\", {})\n",
        "                source_type = finding.get(\"source_type\", \"unknown_source\")\n",
        "\n",
        "                synthesis = analysis.get(\"overall_synthesis_for_this_query\", \"No synthesis for this finding.\")\n",
        "\n",
        "\n",
        "                summary += f\"\\n--- Finding {i+1} for Query: \\\"{query_executed}\\\" (Source Type: {source_type.replace('_',' ').title()}) ---\\n\"\n",
        "                summary += f\"Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(finding.get('timestamp', 0)))}\\n\"\n",
        "                summary += f\"Overall Synthesis from this Query's Analysis: {synthesis}\\n\"\n",
        "\n",
        "                internal_sources = analysis.get(\"internal_faiss_results_summary\", [])\n",
        "                if isinstance(internal_sources, list) and internal_sources:\n",
        "                    summary += \"  Internal Faiss Sources Consulted (up to 2 shown):\\n\"\n",
        "                    for src_idx, src in enumerate(internal_sources[:2]):\n",
        "                        doc_id = src.get(\"internal_doc_id\", \"N/A\")\n",
        "                        extracted = src.get(\"key_extracted_info_from_internal\", \"N/A\")\n",
        "                        relevance = src.get(\"relevance_score_to_query\", \"N/A\")\n",
        "                        summary += f\"    Source {src_idx+1}: Internal Doc ID: {doc_id} (Relevance: {relevance})\\n      Extracted Info: {extracted[:200]}...\\n\"\n",
        "\n",
        "                google_sources = analysis.get(\"google_search_results_summary\", [])\n",
        "                if isinstance(google_sources, list) and google_sources:\n",
        "                    summary += \"  Google Search Sources Consulted (up to 3 shown):\\n\"\n",
        "                    for src_idx, source in enumerate(google_sources[:3]):\n",
        "                        title = source.get(\"source_title\", \"N/A\"); url = source.get(\"source_url\", \"N/A\")\n",
        "                        relevance = source.get(\"relevance_score\", \"N/A\")\n",
        "                        extracted_info = source.get(\"key_extracted_info\", \"N/A\")\n",
        "                        direct_quote = source.get(\"direct_quote\", \"\")\n",
        "\n",
        "                        summary += f\"    Source {src_idx+1}: {title} (Relevance: {relevance})\\n\"\n",
        "                        summary += f\"      URL: {url}\\n\"\n",
        "                        summary += f\"      Extracted Info: {extracted_info[:300]}{'...' if len(extracted_info) > 300 else ''}\\n\"\n",
        "                        if direct_quote:\n",
        "                             summary += f\"      Direct Quote: \\\"{direct_quote[:200]}{'...' if len(direct_quote) > 200 else ''}\\\"\\n\"\n",
        "                    if len(google_sources) > 3:\n",
        "                        summary += f\"    ...and {len(google_sources)-3} more Google source(s) analyzed for this query (details omitted for brevity).\\n\"\n",
        "\n",
        "                new_leads = analysis.get(\"new_questions_or_leads_from_this_query\", [])\n",
        "                if new_leads:\n",
        "                    summary += f\"  New Leads/Questions identified: {', '.join(q for q in new_leads if isinstance(q,str))}\\n\"\n",
        "                tool_errors = analysis.get(\"tool_errors\")\n",
        "                if tool_errors:\n",
        "                     summary += f\"  Tool Errors during this Query's Analysis: {tool_errors}\\n\"\n",
        "\n",
        "            summary += f\"\\n\\n=== OVERALL RESEARCH STATUS FOR THIS SESSION ({self.session_id}) ===\\n\"\n",
        "            summary += f\"Total Unique Sources Referenced This Session: {len(self.session_sources_referenced)}\\n\"\n",
        "            summary += f\"All Completed Queries This Session ({len(self.completed_sub_queries)}): {list(self.completed_sub_queries)}\\n\"\n",
        "            summary += f\"Queries Still Pending This Session: {self.pending_sub_queries if self.pending_sub_queries else 'None'}\\n\"\n",
        "\n",
        "            if self.errors:\n",
        "                summary += f\"\\n=== ERRORS LOGGED IN KNOWLEDGE BASE THIS SESSION ({len(self.errors)}) ===\\n\"\n",
        "                for err_idx, err in enumerate(self.errors[-5:]):\n",
        "                    summary += f\"  KB Error {err_idx+1}: Agent '{err.get('agent', 'N/A')}' - {err.get('message', 'N/A')}\\n\"\n",
        "                    if err.get('details'): summary += f\"    Details: {str(err.get('details'))[:200]}...\\n\"\n",
        "                if len(self.errors) > 5: summary += \"  ...more errors logged in KB (omitted for brevity).\\n\"\n",
        "\n",
        "            return summary[:max_len]\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error generating consolidator summary: {e}\"; print(error_msg)\n",
        "            self.add_error(\"KBSummaryConsolidator\", error_msg)\n",
        "            return f\"Original Query: {self.original_query}\\nError: Unable to generate full consolidator summary.\"\n",
        "\n",
        "    def to_dict(self) -> Dict[str, Any]:\n",
        "        return {\n",
        "            \"session_id\": self.session_id,\n",
        "            \"original_query\": self.original_query,\n",
        "            \"main_topics_identified\": self.main_topics_identified,\n",
        "            \"detailed_findings\": self.detailed_findings,\n",
        "            \"pending_sub_queries\": self.pending_sub_queries,\n",
        "            \"completed_sub_queries\": list(self.completed_sub_queries),\n",
        "            \"session_sources_referenced\": [list(s) for s in self.session_sources_referenced],\n",
        "            \"errors\": self.errors,\n",
        "            \"query_dependencies\": self.query_dependencies,\n",
        "            \"query_generation_reasons\": self.query_generation_reasons,\n",
        "            \"global_faiss_doc_count\": self.global_faiss_doc_count,\n",
        "            \"creator_plan\": self.creator_plan,\n",
        "            \"creator_completed_subtasks_count\": self.creator_completed_subtasks_count,\n",
        "            \"created_artifacts\": self.created_artifacts\n",
        "        }\n",
        "\n",
        "    @classmethod\n",
        "    def from_dict(cls, data: Dict[str, Any], default_session_id: Optional[str] = None) -> 'KnowledgeBase':\n",
        "        session_id_from_data = data.get(\"session_id\")\n",
        "        original_query_from_data = data.get(\"original_query\", \"Unknown Query - KB Deserialization\")\n",
        "\n",
        "        if session_id_from_data:\n",
        "            final_session_id = session_id_from_data\n",
        "        elif default_session_id:\n",
        "            final_session_id = default_session_id\n",
        "        else:\n",
        "            final_session_id = sanitize_filename(original_query_from_data) + \"_\" + str(int(time.time()))\n",
        "\n",
        "        kb = cls(original_query_from_data, final_session_id)\n",
        "        kb.main_topics_identified = data.get(\"main_topics_identified\", [])\n",
        "        kb.detailed_findings = data.get(\"detailed_findings\", [])\n",
        "        pending_sq = data.get(\"pending_sub_queries\", [])\n",
        "        kb.pending_sub_queries = pending_sq if pending_sq else [original_query_from_data]\n",
        "\n",
        "        kb.completed_sub_queries = set(data.get(\"completed_sub_queries\", []))\n",
        "        kb.session_sources_referenced = set(tuple(s) for s in data.get(\"session_sources_referenced\", []) if isinstance(s, list) and len(s) == 2)\n",
        "        kb.errors = data.get(\"errors\", [])\n",
        "        kb.query_dependencies = data.get(\"query_dependencies\", {})\n",
        "        kb.query_generation_reasons = data.get(\"query_generation_reasons\", {})\n",
        "        kb.global_faiss_doc_count = data.get(\"global_faiss_doc_count\", 0)\n",
        "        kb.creator_plan = data.get(\"creator_plan\", [])\n",
        "        kb.creator_completed_subtasks_count = data.get(\"creator_completed_subtasks_count\", 0)\n",
        "        kb.created_artifacts = data.get(\"created_artifacts\", {})\n",
        "        return kb\n",
        "\n",
        "# --- Faiss Manager (Operates on Global Shared Index) ---\n",
        "class SimpleFaissManager:\n",
        "    def __init__(self, dimension: int,\n",
        "                 global_document_store_ref: Dict[str, Dict[str, Any]],\n",
        "                 global_faiss_map_ref: List[str],\n",
        "                 embedding_fn_ref: callable,\n",
        "                 gcs_bucket_name: Optional[str] = None, gcs_blob_name: Optional[str] = None,\n",
        "                 force_new_index: bool = False):\n",
        "        self.dimension = dimension\n",
        "        self.index: Optional[faiss.Index] = None\n",
        "        self.global_doc_store = global_document_store_ref\n",
        "        self.global_faiss_map = global_faiss_map_ref\n",
        "        self.embedding_function = embedding_fn_ref\n",
        "\n",
        "        self.gcs_bucket_name = gcs_bucket_name\n",
        "        self.gcs_blob_name = gcs_blob_name\n",
        "        self.local_temp_index_path = TEMP_FAISS_INDEX_PATH\n",
        "\n",
        "        if not FAISS_AVAILABLE:\n",
        "            print(\"SimpleFaissManager: Faiss library not available. RAG operations will be disabled.\")\n",
        "            return\n",
        "\n",
        "        if force_new_index:\n",
        "            print(\"SimpleFaissManager: `force_new_index` is true. Creating new, empty Faiss index object.\")\n",
        "            if FAISS_AVAILABLE: self.index = faiss.IndexFlatL2(self.dimension)\n",
        "            else: print(\"SimpleFaissManager CRITICAL: FAISS_AVAILABLE false but reached index creation.\"); return\n",
        "        else:\n",
        "            loaded_successfully = False\n",
        "            if self.gcs_bucket_name and self.gcs_blob_name and GCS_AVAILABLE and storage:\n",
        "                if self._load_index_from_gcs_internal(): loaded_successfully = True\n",
        "            if not loaded_successfully and os.path.exists(LOCAL_FAISS_INDEX_PATH):\n",
        "                if self._load_index_from_local_internal(LOCAL_FAISS_INDEX_PATH): loaded_successfully = True\n",
        "            if not loaded_successfully:\n",
        "                print(f\"SimpleFaissManager: No existing Faiss index found. Creating new empty index object.\")\n",
        "                if FAISS_AVAILABLE: self.index = faiss.IndexFlatL2(self.dimension)\n",
        "                else: print(\"SimpleFaissManager CRITICAL: FAISS_AVAILABLE false after load fail.\"); return\n",
        "\n",
        "        if self.index:\n",
        "            print(f\"SimpleFaissManager initialized. Index has {self.index.ntotal} embeddings. Store: {len(self.global_doc_store)} docs, Map: {len(self.global_faiss_map)} IDs.\")\n",
        "            if not force_new_index and self.index.ntotal > 0 and self.index.ntotal != len(self.global_faiss_map):\n",
        "                print(f\"  WARNING: Faiss index count ({self.index.ntotal}) != map size ({len(self.global_faiss_map)}). Potential desync.\")\n",
        "        elif FAISS_AVAILABLE:\n",
        "            print(\"SimpleFaissManager CRITICAL: Faiss index is None after init. Creating fallback empty index.\")\n",
        "            self.index = faiss.IndexFlatL2(self.dimension)\n",
        "\n",
        "    def _get_gcs_blob(self):\n",
        "        if not GCS_AVAILABLE or not storage or not self.gcs_bucket_name or not self.gcs_blob_name: return None\n",
        "        try:\n",
        "            storage_client = storage.Client(project=VERTEX_PROJECT_ID if VERTEX_PROJECT_ID else None)\n",
        "            bucket = storage_client.bucket(self.gcs_bucket_name.replace(\"gs://\", \"\").rstrip(\"/\"))\n",
        "            blob_name = self.gcs_blob_name\n",
        "            if blob_name.startswith(f\"gs://{bucket.name}/\"): blob_name = blob_name[len(f\"gs://{bucket.name}/\"):]\n",
        "            elif blob_name.startswith(f\"{bucket.name}/\"): blob_name = blob_name[len(f\"{bucket.name}/\"):]\n",
        "            return bucket.blob(blob_name)\n",
        "        except Exception as e: print(f\"GCS Error getting blob gs://{self.gcs_bucket_name}/{self.gcs_blob_name}: {e}\"); return None\n",
        "\n",
        "    def _load_index_from_gcs_internal(self) -> bool:\n",
        "        print(f\"Faiss(GCS): Attempting to load index from gs://{self.gcs_bucket_name}/{self.gcs_blob_name}...\")\n",
        "        blob = self._get_gcs_blob()\n",
        "        if blob and blob.exists():\n",
        "            try:\n",
        "                os.makedirs(os.path.dirname(self.local_temp_index_path), exist_ok=True)\n",
        "                blob.download_to_filename(self.local_temp_index_path)\n",
        "                self.index = faiss.read_index(self.local_temp_index_path)\n",
        "                print(f\"Faiss(GCS): Index loaded. Size: {self.index.ntotal if self.index else 'None'}\")\n",
        "                if os.path.exists(self.local_temp_index_path): os.remove(self.local_temp_index_path)\n",
        "                return True\n",
        "            except Exception as e: print(f\"Faiss(GCS): Error loading index: {e}.\");\n",
        "            if os.path.exists(self.local_temp_index_path): os.remove(self.local_temp_index_path)\n",
        "        else: print(f\"Faiss(GCS): Index not found.\")\n",
        "        return False\n",
        "\n",
        "    def _load_index_from_local_internal(self, index_file_path: str) -> bool:\n",
        "        try:\n",
        "            print(f\"Faiss(Local): Attempting to load index from {index_file_path}...\")\n",
        "            self.index = faiss.read_index(index_file_path)\n",
        "            print(f\"Faiss(Local): Index loaded. Size: {self.index.ntotal if self.index else 'None'}\")\n",
        "            return True\n",
        "        except Exception as e: print(f\"Faiss(Local): Error loading index: {e}.\"); return False\n",
        "\n",
        "    def save_index(self):\n",
        "        if not FAISS_AVAILABLE or not self.index: print(\"Faiss: Index not available/init. Cannot save.\"); return\n",
        "        os.makedirs(os.path.dirname(self.local_temp_index_path), exist_ok=True)\n",
        "        if self.gcs_bucket_name and self.gcs_blob_name and GCS_AVAILABLE and storage:\n",
        "            blob = self._get_gcs_blob()\n",
        "            if not blob: self._save_index_locally_internal(); return\n",
        "            try:\n",
        "                faiss.write_index(self.index, self.local_temp_index_path)\n",
        "                print(f\"Faiss(GCS): Index temp saved to {self.local_temp_index_path} for upload ({self.index.ntotal} vectors).\")\n",
        "                blob.upload_from_filename(self.local_temp_index_path)\n",
        "                print(f\"Faiss(GCS): Index saved to GCS.\")\n",
        "                if os.path.exists(self.local_temp_index_path): os.remove(self.local_temp_index_path)\n",
        "            except Exception as e:\n",
        "                print(f\"Faiss(GCS): Error saving index to GCS: {e}. Attempting local save.\")\n",
        "                if os.path.exists(self.local_temp_index_path): os.remove(self.local_temp_index_path)\n",
        "                self._save_index_locally_internal()\n",
        "        else:\n",
        "            self._save_index_locally_internal()\n",
        "\n",
        "    def _save_index_locally_internal(self):\n",
        "        if not FAISS_AVAILABLE or not self.index: return\n",
        "        try:\n",
        "            os.makedirs(GLOBAL_RAG_DIR, exist_ok=True)\n",
        "            print(f\"Faiss(Local): Saving index to {LOCAL_FAISS_INDEX_PATH} ({self.index.ntotal} vectors)...\")\n",
        "            faiss.write_index(self.index, LOCAL_FAISS_INDEX_PATH)\n",
        "            print(f\"Faiss(Local): Index saved.\")\n",
        "        except Exception as e: print(f\"Faiss(Local): Error saving index: {e}\")\n",
        "\n",
        "\n",
        "    async def add_documents_from_kb_async(self):\n",
        "        if not FAISS_AVAILABLE: print(\"Faiss: Not available. Skipping add_docs.\"); return\n",
        "        if not self.index:\n",
        "            if FAISS_AVAILABLE: self.index = faiss.IndexFlatL2(self.dimension); print(\"Faiss: Index was None, created new for add_docs.\")\n",
        "            else: return\n",
        "        if not self.embedding_function: print(\"Faiss: Embedding function missing. Cannot add docs.\"); return\n",
        "\n",
        "        docs_to_embed_content: List[str] = []\n",
        "        doc_ids_for_new_embeddings: List[str] = []\n",
        "        doc_titles_for_new_embeddings: List[str] = []\n",
        "        current_indexed_doc_ids = set(self.global_faiss_map)\n",
        "\n",
        "        for doc_id, doc_data in self.global_doc_store.items():\n",
        "            if doc_id not in current_indexed_doc_ids:\n",
        "                content = doc_data.get('content')\n",
        "                if content:\n",
        "                    docs_to_embed_content.append(content)\n",
        "                    doc_ids_for_new_embeddings.append(doc_id)\n",
        "                    title = doc_data.get('summary', content[:100] if content else \"Untitled Document\")\n",
        "                    doc_titles_for_new_embeddings.append(title)\n",
        "\n",
        "        if docs_to_embed_content:\n",
        "            print(f\"Faiss: Embedding {len(docs_to_embed_content)} new docs from global store for indexing...\")\n",
        "            embedding_vectors_list = await self.embedding_function(\n",
        "                texts=docs_to_embed_content, task_type=\"RETRIEVAL_DOCUMENT\",\n",
        "                titles=doc_titles_for_new_embeddings, output_dimensionality=self.dimension\n",
        "            )\n",
        "            successful_embeddings_np_list = []\n",
        "            final_doc_ids_for_map_extension = []\n",
        "            for i, emb_floats in enumerate(embedding_vectors_list):\n",
        "                doc_id = doc_ids_for_new_embeddings[i]\n",
        "                if emb_floats and len(emb_floats) == self.dimension:\n",
        "                    successful_embeddings_np_list.append(np.array(emb_floats, dtype='float32'))\n",
        "                    final_doc_ids_for_map_extension.append(doc_id)\n",
        "                else:\n",
        "                    print(f\"  WARNING: Failed/invalid embedding for doc ID '{doc_id}'. Will not be indexed.\")\n",
        "            if successful_embeddings_np_list:\n",
        "                embeddings_array = np.vstack(successful_embeddings_np_list)\n",
        "                self.add_new_embeddings(embeddings_array, final_doc_ids_for_map_extension)\n",
        "        else:\n",
        "            print(\"Faiss: No new documents in global store to add to index based on current map.\")\n",
        "\n",
        "    def add_new_embeddings(self, embeddings_array: np.ndarray, new_doc_ids_for_map_extension: List[str]):\n",
        "        if not FAISS_AVAILABLE: print(\"Faiss: Unavailable. Cannot add embeddings.\"); return\n",
        "        if not self.index: self.index = faiss.IndexFlatL2(self.dimension); print(\"Faiss: Index was None, created new.\")\n",
        "        if embeddings_array.shape[0] == 0: print(\"Faiss: No new embeddings to add.\"); return\n",
        "        if embeddings_array.ndim == 1: embeddings_array = embeddings_array.reshape(1, -1)\n",
        "        if embeddings_array.shape[1] != self.dimension: print(f\"Faiss Error: Dim mismatch. Expected {self.dimension}, got {embeddings_array.shape[1]}.\"); return\n",
        "\n",
        "        self.index.add(embeddings_array.astype('float32'))\n",
        "        self.global_faiss_map.extend(new_doc_ids_for_map_extension)\n",
        "        print(f\"Faiss: Added {embeddings_array.shape[0]} embeddings. Total: {self.index.ntotal}. Map size: {len(self.global_faiss_map)}.\")\n",
        "        self.save_index()\n",
        "\n",
        "    async def search_global_index_async(self, query: str, top_k: int = RAG_TOP_K) -> List[Dict[str, Any]]:\n",
        "        if not FAISS_AVAILABLE: print(\"Faiss: Not available. Cannot search.\"); return []\n",
        "        if not self.index or self.index.ntotal == 0: print(f\"Faiss: Index empty or None. Cannot search.\"); return []\n",
        "        if not self.embedding_function: print(\"Faiss: Embedding function missing. Cannot search.\"); return []\n",
        "\n",
        "        print(f\"Faiss: Async searching for '{query[:50]}...' (top_k={top_k}, index size={self.index.ntotal})\")\n",
        "        try:\n",
        "            embedding_vectors_list = await self.embedding_function(\n",
        "                texts=[query], task_type=\"RETRIEVAL_QUERY\", titles=None, output_dimensionality=self.dimension\n",
        "            )\n",
        "            if not embedding_vectors_list or not embedding_vectors_list[0] or len(embedding_vectors_list[0]) != self.dimension:\n",
        "                print(f\"Faiss Error: Invalid embedding for query '{query}'.\"); return []\n",
        "\n",
        "            query_embedding_np = np.array(embedding_vectors_list[0], dtype='float32').reshape(1, -1)\n",
        "            actual_top_k = min(top_k, self.index.ntotal)\n",
        "            if actual_top_k == 0 : return []\n",
        "\n",
        "            distances, faiss_indices_sequential = self.index.search(query_embedding_np, actual_top_k)\n",
        "            results = []\n",
        "            if faiss_indices_sequential.size > 0 and len(faiss_indices_sequential[0]) > 0:\n",
        "                for i, seq_idx in enumerate(faiss_indices_sequential[0]):\n",
        "                    if not (0 <= seq_idx < len(self.global_faiss_map)): continue\n",
        "                    doc_id = self.global_faiss_map[seq_idx]\n",
        "                    doc_data = self.global_doc_store.get(doc_id)\n",
        "                    if doc_data:\n",
        "                        dist = float(distances[0][i])\n",
        "                        results.append({\n",
        "                            \"internal_doc_id\": doc_id,\n",
        "                            \"summary\": doc_data.get(\"summary\", doc_data.get(\"content\",\"\"))[:250]+\"...\", # Use content if summary missing\n",
        "                            \"content_preview\": doc_data.get(\"content\", \"\")[:250] + \"...\", # Keep content_preview for consistency\n",
        "                            \"similarity_score\": 1 / (1 + dist) if dist >=0 else 0\n",
        "                        })\n",
        "            print(f\"  Faiss Async: Found {len(results)} results for query '{query[:50]}...'\")\n",
        "            return results\n",
        "        except Exception as e: print(f\"Faiss Error during async search: {e}\"); traceback.print_exc(); return []\n",
        "\n",
        "    def clear_global_index_data(self):\n",
        "        if not FAISS_AVAILABLE: print(\"Faiss not available.\"); return\n",
        "        if self.index: self.index.reset(); print(\"Faiss index reset.\")\n",
        "        self.global_doc_store.clear(); self.global_faiss_map.clear()\n",
        "        print(\"Global doc store and Faiss map cleared.\")\n",
        "        if FAISS_AVAILABLE: self.index = faiss.IndexFlatL2(self.dimension); print(\"New empty Faiss index created.\")\n",
        "\n",
        "# --- Agent System Class (Main orchestrator) ---\n",
        "class DeepResearchSystem:\n",
        "    def __init__(self, project_id: str, location: str, load_previous_session_id: Optional[str] = None, force_new_faiss_and_global_stores: bool = False):\n",
        "        self.project_id = project_id\n",
        "        self.location = location\n",
        "        self.vertex_client = None\n",
        "        self.embedding_model: Optional[TextEmbeddingModel] = None # Explicitly type hint\n",
        "        self.research_log: List[Dict[str, Any]] = []\n",
        "        self.knowledge_base: Optional[KnowledgeBase] = None\n",
        "        self.faiss_manager: Optional[SimpleFaissManager] = None\n",
        "\n",
        "        self.master_document_store: Dict[str, Dict[str, Any]] = {}\n",
        "        self.master_faiss_idx_to_doc_id_map: List[str] = []\n",
        "\n",
        "        if GOOGLE_GENAI_SDK_AVAILABLE and genai:\n",
        "            try:\n",
        "                self.vertex_client = genai.Client(vertexai=True, project=self.project_id, location=self.location)\n",
        "                print(f\"DeepResearchSystem: Vertex AI Client (genai.Client) initialized for text models using project '{self.project_id}' and location '{self.location}'.\")\n",
        "            except ValueError as ve:\n",
        "                print(f\"DeepResearchSystem: CRITICAL ValueError during genai.Client initialization: {ve}\")\n",
        "                print(\"Ensure VERTEX_PROJECT_ID and VERTEX_LOCATION are correctly set and the client has permissions.\")\n",
        "                self.vertex_client = None\n",
        "            except Exception as e_client:\n",
        "                print(f\"DeepResearchSystem: CRITICAL ERROR initializing genai.Client: {e_client}\")\n",
        "                traceback.print_exc()\n",
        "                self.vertex_client = None\n",
        "        else:\n",
        "            print(\"DeepResearchSystem: google.genai SDK not available. Text generation models will NOT work.\")\n",
        "            self.vertex_client = None\n",
        "\n",
        "        if VERTEX_AI_SDK_INITIALIZED_SUCCESSFULLY and TEXT_EMBEDDING_MODEL_CLASS_AVAILABLE and TextEmbeddingModel:\n",
        "            try:\n",
        "                print(f\"DeepResearchSystem: Attempting to load embedding model 'gemini-embedding-001'...\")\n",
        "                self.embedding_model = TextEmbeddingModel.from_pretrained(\"gemini-embedding-001\")\n",
        "                print(\"DeepResearchSystem: Successfully loaded embedding model 'gemini-embedding-001'.\")\n",
        "            except Exception as e_load_embed:\n",
        "                print(f\"DeepResearchSystem: WARNING - Failed to load embedding model 'gemini-embedding-001': {e_load_embed}\")\n",
        "                traceback.print_exc()\n",
        "                self.embedding_model = None\n",
        "        else:\n",
        "            if not VERTEX_AI_SDK_INITIALIZED_SUCCESSFULLY:\n",
        "                print(\"DeepResearchSystem: Embedding model not loaded because global Vertex AI SDK initialization failed or was skipped.\")\n",
        "            elif not TEXT_EMBEDDING_MODEL_CLASS_AVAILABLE or not TextEmbeddingModel:\n",
        "                print(\"DeepResearchSystem: Embedding model not loaded because TextEmbeddingModel class was not imported successfully.\")\n",
        "            self.embedding_model = None\n",
        "\n",
        "        os.makedirs(SESSIONS_DIR, exist_ok=True)\n",
        "        os.makedirs(GLOBAL_RAG_DIR, exist_ok=True)\n",
        "        self._load_global_rag_stores()\n",
        "        self._initialize_faiss_manager_globally(force_new_index=force_new_faiss_and_global_stores)\n",
        "\n",
        "        if load_previous_session_id:\n",
        "            if not self._load_session_state(load_previous_session_id):\n",
        "                print(f\"Failed to load session {load_previous_session_id}. Will proceed as new if query provided later in run_deep_research.\")\n",
        "                self.knowledge_base = None\n",
        "                self.research_log = []\n",
        "\n",
        "        if self.knowledge_base:\n",
        "            if self.faiss_manager and self.faiss_manager.index:\n",
        "                self.knowledge_base.global_faiss_doc_count = self.faiss_manager.index.ntotal\n",
        "            else:\n",
        "                self.knowledge_base.global_faiss_doc_count = 0\n",
        "        print(f\"DeepResearchSystem initialized. Global Faiss Index has {self.faiss_manager.index.ntotal if self.faiss_manager and self.faiss_manager.index else 'N/A (Faiss unavailable or not initialized)'} docs.\")\n",
        "        print(f\"Master document store has {len(self.master_document_store)} entries. Master Faiss map has {len(self.master_faiss_idx_to_doc_id_map)} entries.\")\n",
        "\n",
        "    def _load_global_rag_stores(self):\n",
        "        os.makedirs(GLOBAL_RAG_DIR, exist_ok=True)\n",
        "        print(\"--- Loading Global RAG Stores ---\")\n",
        "        loaded_doc_store_from_gcs = False\n",
        "        if GCS_AVAILABLE and storage and FAISS_INDEX_GCS_BUCKET_NAME and GLOBAL_DOC_STORE_GCS_BLOB_NAME:\n",
        "            print(f\"Attempting to load Global Document Store from GCS: gs://{FAISS_INDEX_GCS_BUCKET_NAME}/{GLOBAL_DOC_STORE_GCS_BLOB_NAME}\")\n",
        "            try:\n",
        "                gcs_project_id = self.project_id if self.project_id else os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "                if not gcs_project_id:\n",
        "                    print(\"Warning (GCS Load): Project ID for GCS client is not set. Using default project if available.\")\n",
        "                client = storage.Client(project=gcs_project_id)\n",
        "                bucket = client.bucket(FAISS_INDEX_GCS_BUCKET_NAME.replace(\"gs://\", \"\").rstrip(\"/\"))\n",
        "                blob_name_cleaned = GLOBAL_DOC_STORE_GCS_BLOB_NAME\n",
        "                if blob_name_cleaned.startswith(f\"gs://{bucket.name}/\"): blob_name_cleaned = blob_name_cleaned[len(f\"gs://{bucket.name}/\"):]\n",
        "                elif blob_name_cleaned.startswith(f\"{bucket.name}/\"): blob_name_cleaned = blob_name_cleaned[len(f\"{bucket.name}/\"):]\n",
        "                blob = bucket.blob(blob_name_cleaned)\n",
        "                if blob.exists():\n",
        "                    content = blob.download_as_string()\n",
        "                    self.master_document_store = json.loads(content)\n",
        "                    print(f\"Global document store loaded from GCS: {len(self.master_document_store)} documents.\")\n",
        "                    loaded_doc_store_from_gcs = True\n",
        "                else:\n",
        "                    print(f\"Global document store not found in GCS at gs://{bucket.name}/{blob_name_cleaned}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading global document store from GCS: {e}. Will try local.\")\n",
        "                traceback.print_exc()\n",
        "        if not loaded_doc_store_from_gcs:\n",
        "            if os.path.exists(GLOBAL_DOC_STORE_PATH):\n",
        "                print(f\"Attempting to load Global Document Store from local fallback: {GLOBAL_DOC_STORE_PATH}\")\n",
        "                try:\n",
        "                    with open(GLOBAL_DOC_STORE_PATH, 'r', encoding='utf-8') as f: self.master_document_store = json.load(f)\n",
        "                    print(f\"Global document store loaded from local fallback: {len(self.master_document_store)} documents.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading global document store from local fallback: {e}. Initializing empty store.\")\n",
        "                    self.master_document_store = {}\n",
        "            else:\n",
        "                 print(\"Global document store not found in GCS or local fallback. Initializing empty store.\")\n",
        "                 self.master_document_store = {}\n",
        "        loaded_faiss_map_from_gcs = False\n",
        "        if GCS_AVAILABLE and storage and FAISS_INDEX_GCS_BUCKET_NAME and GLOBAL_FAISS_MAP_GCS_BLOB_NAME:\n",
        "            print(f\"Attempting to load Global Faiss Map from GCS: gs://{FAISS_INDEX_GCS_BUCKET_NAME}/{GLOBAL_FAISS_MAP_GCS_BLOB_NAME}\")\n",
        "            try:\n",
        "                gcs_project_id = self.project_id if self.project_id else os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "                client = storage.Client(project=gcs_project_id)\n",
        "                bucket = client.bucket(FAISS_INDEX_GCS_BUCKET_NAME.replace(\"gs://\", \"\").rstrip(\"/\"))\n",
        "                blob_name_cleaned = GLOBAL_FAISS_MAP_GCS_BLOB_NAME\n",
        "                if blob_name_cleaned.startswith(f\"gs://{bucket.name}/\"): blob_name_cleaned = blob_name_cleaned[len(f\"gs://{bucket.name}/\"):]\n",
        "                elif blob_name_cleaned.startswith(f\"{bucket.name}/\"): blob_name_cleaned = blob_name_cleaned[len(f\"{bucket.name}/\"):]\n",
        "                blob = bucket.blob(blob_name_cleaned)\n",
        "                if blob.exists():\n",
        "                    content = blob.download_as_string()\n",
        "                    self.master_faiss_idx_to_doc_id_map = json.loads(content)\n",
        "                    print(f\"Global Faiss ID map loaded from GCS: {len(self.master_faiss_idx_to_doc_id_map)} mappings.\")\n",
        "                    loaded_faiss_map_from_gcs = True\n",
        "                else:\n",
        "                    print(f\"Global Faiss map not found in GCS at gs://{bucket.name}/{blob_name_cleaned}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading global Faiss map from GCS: {e}. Will try local.\")\n",
        "                traceback.print_exc()\n",
        "        if not loaded_faiss_map_from_gcs:\n",
        "            if os.path.exists(GLOBAL_FAISS_MAP_PATH):\n",
        "                print(f\"Attempting to load Global Faiss Map from local fallback: {GLOBAL_FAISS_MAP_PATH}\")\n",
        "                try:\n",
        "                    with open(GLOBAL_FAISS_MAP_PATH, 'r', encoding='utf-8') as f: self.master_faiss_idx_to_doc_id_map = json.load(f)\n",
        "                    print(f\"Global Faiss ID map loaded from local fallback: {len(self.master_faiss_idx_to_doc_id_map)} mappings.\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading global Faiss ID map from local fallback: {e}. Initializing empty map.\")\n",
        "                    self.master_faiss_idx_to_doc_id_map = []\n",
        "            else:\n",
        "                print(\"Global Faiss map not found in GCS or local fallback. Initializing empty map.\")\n",
        "                self.master_faiss_idx_to_doc_id_map = []\n",
        "        print(\"--- Finished Loading Global RAG Stores ---\")\n",
        "\n",
        "    def _save_global_rag_stores(self):\n",
        "        os.makedirs(GLOBAL_RAG_DIR, exist_ok=True)\n",
        "        print(\"--- Saving Global RAG Stores ---\")\n",
        "        try:\n",
        "            doc_store_json_str = json.dumps(self.master_document_store, indent=2)\n",
        "            gcs_doc_store_saved = False\n",
        "            if GCS_AVAILABLE and storage and FAISS_INDEX_GCS_BUCKET_NAME and GLOBAL_DOC_STORE_GCS_BLOB_NAME:\n",
        "                print(f\"Attempting to save Global Document Store to GCS: gs://{FAISS_INDEX_GCS_BUCKET_NAME}/{GLOBAL_DOC_STORE_GCS_BLOB_NAME}\")\n",
        "                try:\n",
        "                    gcs_project_id = self.project_id if self.project_id else os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "                    client = storage.Client(project=gcs_project_id)\n",
        "                    bucket = client.bucket(FAISS_INDEX_GCS_BUCKET_NAME.replace(\"gs://\", \"\").rstrip(\"/\"))\n",
        "                    blob_name_cleaned = GLOBAL_DOC_STORE_GCS_BLOB_NAME\n",
        "                    if blob_name_cleaned.startswith(f\"gs://{bucket.name}/\"): blob_name_cleaned = blob_name_cleaned[len(f\"gs://{bucket.name}/\"):]\n",
        "                    elif blob_name_cleaned.startswith(f\"{bucket.name}/\"): blob_name_cleaned = blob_name_cleaned[len(f\"{bucket.name}/\"):]\n",
        "                    blob = bucket.blob(blob_name_cleaned)\n",
        "                    blob.upload_from_string(doc_store_json_str, content_type='application/json')\n",
        "                    print(f\"Global document store ({len(self.master_document_store)} entries) saved to GCS.\")\n",
        "                    gcs_doc_store_saved = True\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving global document store to GCS: {e}.\")\n",
        "                    traceback.print_exc()\n",
        "            try:\n",
        "                with open(GLOBAL_DOC_STORE_PATH, 'w', encoding='utf-8') as f: f.write(doc_store_json_str)\n",
        "                print(f\"Global document store {'also ' if gcs_doc_store_saved else ''}saved locally to {GLOBAL_DOC_STORE_PATH}.\")\n",
        "            except Exception as e_local:\n",
        "                print(f\"Error saving global document store locally: {e_local}\")\n",
        "\n",
        "            faiss_map_json_str = json.dumps(self.master_faiss_idx_to_doc_id_map, indent=2)\n",
        "            gcs_faiss_map_saved = False\n",
        "            if GCS_AVAILABLE and storage and FAISS_INDEX_GCS_BUCKET_NAME and GLOBAL_FAISS_MAP_GCS_BLOB_NAME:\n",
        "                print(f\"Attempting to save Global Faiss Map to GCS: gs://{FAISS_INDEX_GCS_BUCKET_NAME}/{GLOBAL_FAISS_MAP_GCS_BLOB_NAME}\")\n",
        "                try:\n",
        "                    gcs_project_id = self.project_id if self.project_id else os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
        "                    client = storage.Client(project=gcs_project_id)\n",
        "                    bucket = client.bucket(FAISS_INDEX_GCS_BUCKET_NAME.replace(\"gs://\", \"\").rstrip(\"/\"))\n",
        "                    blob_name_cleaned = GLOBAL_FAISS_MAP_GCS_BLOB_NAME\n",
        "                    if blob_name_cleaned.startswith(f\"gs://{bucket.name}/\"): blob_name_cleaned = blob_name_cleaned[len(f\"gs://{bucket.name}/\"):]\n",
        "                    elif blob_name_cleaned.startswith(f\"{bucket.name}/\"): blob_name_cleaned = blob_name_cleaned[len(f\"{bucket.name}/\"):]\n",
        "                    blob = bucket.blob(blob_name_cleaned)\n",
        "                    blob.upload_from_string(faiss_map_json_str, content_type='application/json')\n",
        "                    print(f\"Global Faiss map ({len(self.master_faiss_idx_to_doc_id_map)} entries) saved to GCS.\")\n",
        "                    gcs_faiss_map_saved = True\n",
        "                except Exception as e:\n",
        "                    print(f\"Error saving global Faiss map to GCS: {e}.\")\n",
        "                    traceback.print_exc()\n",
        "            try:\n",
        "                with open(GLOBAL_FAISS_MAP_PATH, 'w', encoding='utf-8') as f: f.write(faiss_map_json_str)\n",
        "                print(f\"Global Faiss map {'also ' if gcs_faiss_map_saved else ''}saved locally to {GLOBAL_FAISS_MAP_PATH}.\")\n",
        "            except Exception as e_local:\n",
        "                print(f\"Error saving global Faiss map locally: {e_local}\")\n",
        "        except Exception as e_outer_save:\n",
        "            print(f\"Outer error during _save_global_rag_stores: {e_outer_save}\")\n",
        "            traceback.print_exc()\n",
        "        print(\"--- Finished Saving Global RAG Stores ---\")\n",
        "\n",
        "    def _initialize_faiss_manager_globally(self, force_new_index: bool = False):\n",
        "        if not FAISS_AVAILABLE:\n",
        "            print(\"Faiss library not available, skipping Faiss manager initialization.\")\n",
        "            self.faiss_manager = None\n",
        "            if self.knowledge_base: self.knowledge_base.global_faiss_doc_count = 0\n",
        "            return\n",
        "\n",
        "        if self.faiss_manager and not force_new_index:\n",
        "            print(\"Global Faiss Manager already initialized and not forcing new.\")\n",
        "            if self.faiss_manager.index and self.knowledge_base:\n",
        "                 self.knowledge_base.global_faiss_doc_count = self.faiss_manager.index.ntotal\n",
        "            return\n",
        "\n",
        "        if force_new_index:\n",
        "            print(\"Forcing new GLOBAL Faiss index and clearing global document/map stores.\")\n",
        "            if GCS_AVAILABLE and storage and FAISS_INDEX_GCS_BUCKET_NAME and FAISS_INDEX_GCS_BLOB_NAME:\n",
        "                try:\n",
        "                    # Create a temporary SimpleFaissManager instance just for its GCS blob access logic\n",
        "                    # This dummy embedding function will not be called if we're just getting a blob for deletion.\n",
        "                    async def dummy_embedding_fn_for_delete(texts, task_type, titles, output_dimensionality): return [None]*len(texts)\n",
        "\n",
        "                    temp_manager_for_delete = SimpleFaissManager(\n",
        "                        FAISS_DIMENSION, {}, [],\n",
        "                        embedding_fn_ref=dummy_embedding_fn_for_delete,\n",
        "                        gcs_bucket_name=FAISS_INDEX_GCS_BUCKET_NAME,\n",
        "                        gcs_blob_name=FAISS_INDEX_GCS_BLOB_NAME,\n",
        "                        force_new_index=True # This ensures it doesn't try to load if GCS is primary\n",
        "                    )\n",
        "                    blob_to_delete = temp_manager_for_delete._get_gcs_blob() # Use its internal GCS logic\n",
        "                    if blob_to_delete and blob_to_delete.exists():\n",
        "                        print(f\"Attempting to delete existing GCS Faiss index: gs://{FAISS_INDEX_GCS_BUCKET_NAME}/{FAISS_INDEX_GCS_BLOB_NAME}\")\n",
        "                        blob_to_delete.delete()\n",
        "                        print(f\"Deleted existing GCS Faiss index.\")\n",
        "                    else:\n",
        "                        print(f\"No GCS Faiss index found at gs://{FAISS_INDEX_GCS_BUCKET_NAME}/{FAISS_INDEX_GCS_BLOB_NAME} to delete.\")\n",
        "                except Exception as e_gcs_delete:\n",
        "                    print(f\"Could not delete GCS Faiss index during force_new: {e_gcs_delete}\")\n",
        "                    traceback.print_exc()\n",
        "\n",
        "            if os.path.exists(LOCAL_FAISS_INDEX_PATH):\n",
        "                try: os.remove(LOCAL_FAISS_INDEX_PATH); print(f\"Deleted local Faiss index: {LOCAL_FAISS_INDEX_PATH}\")\n",
        "                except Exception as e: print(f\"Could not delete local Faiss index: {e}\")\n",
        "\n",
        "            self.master_document_store.clear()\n",
        "            self.master_faiss_idx_to_doc_id_map.clear()\n",
        "\n",
        "            if os.path.exists(GLOBAL_DOC_STORE_PATH):\n",
        "                try: os.remove(GLOBAL_DOC_STORE_PATH); print(f\"Deleted local doc store: {GLOBAL_DOC_STORE_PATH}\")\n",
        "                except Exception as e: print(f\"Could not delete local doc store: {e}\")\n",
        "            if os.path.exists(GLOBAL_FAISS_MAP_PATH):\n",
        "                try: os.remove(GLOBAL_FAISS_MAP_PATH); print(f\"Deleted local Faiss map: {GLOBAL_FAISS_MAP_PATH}\")\n",
        "                except Exception as e: print(f\"Could not delete local Faiss map: {e}\")\n",
        "            self._save_global_rag_stores() # Save the now-empty stores\n",
        "\n",
        "        # Crucial: Pass the actual embedding function from DeepResearchSystem\n",
        "        self.faiss_manager = SimpleFaissManager(\n",
        "            dimension=FAISS_DIMENSION,\n",
        "            global_document_store_ref=self.master_document_store,\n",
        "            global_faiss_map_ref=self.master_faiss_idx_to_doc_id_map,\n",
        "            embedding_fn_ref=self._get_vertex_embeddings_batch, # Pass the method from self\n",
        "            gcs_bucket_name=FAISS_INDEX_GCS_BUCKET_NAME,\n",
        "            gcs_blob_name=FAISS_INDEX_GCS_BLOB_NAME,\n",
        "            force_new_index=force_new_index\n",
        "        )\n",
        "        if self.knowledge_base:\n",
        "            if self.faiss_manager and self.faiss_manager.index:\n",
        "                self.knowledge_base.global_faiss_doc_count = self.faiss_manager.index.ntotal\n",
        "            else: self.knowledge_base.global_faiss_doc_count = 0\n",
        "        print(f\"Faiss Manager initialized. Global Faiss Index has {self.faiss_manager.index.ntotal if self.faiss_manager and self.faiss_manager.index else 'N/A'} docs.\")\n",
        "\n",
        "    def _load_session_state(self, session_id: str) -> bool:\n",
        "        session_dir = os.path.join(SESSIONS_DIR, session_id)\n",
        "        session_kb_path = os.path.join(session_dir, \"knowledge_base.json\")\n",
        "        session_log_path = os.path.join(session_dir, \"research_log.json\")\n",
        "        kb_loaded = False\n",
        "\n",
        "        if not os.path.isdir(session_dir):\n",
        "            print(f\"Session directory {session_dir} does not exist.\")\n",
        "            return False\n",
        "\n",
        "        if os.path.exists(session_kb_path):\n",
        "            try:\n",
        "                with open(session_kb_path, 'r', encoding='utf-8') as f:\n",
        "                    kb_data = json.load(f)\n",
        "                self.knowledge_base = KnowledgeBase.from_dict(kb_data, default_session_id=session_id)\n",
        "                print(f\"KnowledgeBase for session '{session_id}' loaded. Original query: '{self.knowledge_base.original_query}'\")\n",
        "                if self.faiss_manager and self.faiss_manager.index:\n",
        "                    self.knowledge_base.global_faiss_doc_count = self.faiss_manager.index.ntotal\n",
        "                else:\n",
        "                    self.knowledge_base.global_faiss_doc_count = len(self.master_document_store) if self.master_document_store else 0\n",
        "                kb_loaded = True\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading KnowledgeBase for session '{session_id}': {e}.\")\n",
        "                traceback.print_exc()\n",
        "        else:\n",
        "            print(f\"No KnowledgeBase file found for session '{session_id}'.\")\n",
        "\n",
        "        if os.path.exists(session_log_path):\n",
        "            try:\n",
        "                with open(session_log_path, 'r', encoding='utf-8') as f:\n",
        "                    self.research_log = json.load(f)\n",
        "                print(f\"Research log for session '{session_id}' loaded: {len(self.research_log)} entries.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading research log for session '{session_id}': {e}.\")\n",
        "                traceback.print_exc()\n",
        "        else:\n",
        "            self.research_log = []\n",
        "            print(f\"No research log file found for session '{session_id}'. Initializing empty log.\")\n",
        "\n",
        "        return kb_loaded\n",
        "\n",
        "    def _save_current_session_state(self):\n",
        "        if not self.knowledge_base:\n",
        "            print(\"No active KnowledgeBase to save.\")\n",
        "            return\n",
        "\n",
        "        session_id = self.knowledge_base.session_id\n",
        "        session_dir = os.path.join(SESSIONS_DIR, session_id)\n",
        "        os.makedirs(session_dir, exist_ok=True)\n",
        "        print(f\"\\n--- Saving state for session '{session_id}' ---\")\n",
        "\n",
        "        try:\n",
        "            with open(os.path.join(session_dir, \"knowledge_base.json\"), 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.knowledge_base.to_dict(), f, indent=2, ensure_ascii=False)\n",
        "            print(f\"Session KnowledgeBase saved to {os.path.join(session_dir, 'knowledge_base.json')}.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving session KnowledgeBase: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "        if self.research_log:\n",
        "            try:\n",
        "                with open(os.path.join(session_dir, \"research_log.json\"), 'w', encoding='utf-8') as f:\n",
        "                    json.dump(self.research_log, f, indent=2, ensure_ascii=False)\n",
        "                print(f\"Session research log saved to {os.path.join(session_dir, 'research_log.json')}.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving session research log: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "        if self.faiss_manager:\n",
        "            self.faiss_manager.save_index()\n",
        "        self._save_global_rag_stores()\n",
        "\n",
        "\n",
        "\n",
        "    async def _attempt_json_self_correction_with_llm(self, original_malformed_json_str: str, original_query_context: str, expected_type: str = \"object\") -> str:\n",
        "        print(f\"   Calling JSON Fixer LLM (expecting: {expected_type})...\")\n",
        "\n",
        "        if expected_type == \"list\":\n",
        "            json_fixer_system_prompt = (\n",
        "                \"You are a specialized AI assistant. Your SOLE task is to correct a malformed string that was INTENDED to be a JSON LIST of strings. \"\n",
        "                \"Examine the provided text. If it looks like a list of search queries, ensure it is formatted as a valid JSON list of strings. \"\n",
        "                \"Output ONLY the corrected, valid JSON list (e.g., [\\\"query1\\\", \\\"query2\\\"]). \"\n",
        "                \"If the input is unsalvageable as a JSON list or seems to be a different JSON type, output an empty JSON list []. \"\n",
        "                \"DO NOT add any explanations or conversational text. ONLY the JSON list.\"\n",
        "            )\n",
        "            empty_fallback = \"[]\"\n",
        "        else: # Default to object\n",
        "            json_fixer_system_prompt = (\n",
        "                \"You are a specialized AI assistant. Your SOLE task is to correct a malformed string that was INTENDED to be a single JSON OBJECT. \"\n",
        "                \"The expected schema might have keys like 'search_query_executed', 'internal_faiss_results_summary', 'google_search_needed_and_performed', 'google_search_results_summary', 'overall_synthesis_for_this_query', 'new_questions_or_leads_from_this_query', 'tool_errors' OR keys like 'artifact_id', 'generated_content', 'confidence_score', 'notes_or_questions_for_next_step'. \"\n",
        "                \"Examine the provided text. Ensure it is a single, valid JSON object. \"\n",
        "                \"CRITICAL: If the input text appears to be a JSON LIST (e.g., starts with '[' and ends with ']'), you MUST attempt to convert it into a meaningful JSON OBJECT if possible (e.g., if it's a list of questions, make it `{\\\"new_questions_or_leads_from_this_query\\\": [questions_list]}`), OR if it cannot be converted to a relevant object, output an empty JSON object {}. \"\n",
        "                \"Output ONLY the corrected, valid JSON OBJECT. \"\n",
        "                \"If the input is unsalvageable as a JSON object, output an empty JSON object {}. \"\n",
        "                \"DO NOT add any explanations or conversational text. ONLY the JSON object.\"\n",
        "            )\n",
        "            empty_fallback = \"{}\"\n",
        "\n",
        "\n",
        "        fixer_user_content = (\n",
        "            f\"The following text was an attempt to generate a JSON {expected_type.upper()} for the query context: '{original_query_context[:150]}...'\\n\"\n",
        "            f\"However, it is malformed or not the correct JSON type. Please correct it to be a valid JSON {expected_type.upper()}.\\n\\n\"\n",
        "            f\"Malformed/Incorrect-Type text:\\n```text\\n{original_malformed_json_str}\\n```\\nOutput ONLY the corrected, valid JSON {expected_type.upper()}.\"\n",
        "        )\n",
        "\n",
        "        corrected_response_text, _ = await self._call_llm(json_fixer_system_prompt, fixer_user_content, \"Auxiliary_Lite_Task\")\n",
        "        _, corrected_json_str = parse_llm_response(corrected_response_text)\n",
        "\n",
        "        if corrected_json_str.startswith(\"ERROR:\"):\n",
        "            print(f\"   JSON Fixer LLM call failed: {corrected_json_str}\")\n",
        "            return empty_fallback # Return a valid empty JSON of the expected type\n",
        "        try:\n",
        "            # Validate if the corrected string is actually the expected type\n",
        "            parsed_fixed_json = json.loads(corrected_json_str)\n",
        "            if expected_type == \"list\" and not isinstance(parsed_fixed_json, list):\n",
        "                print(f\"   JSON Fixer output for expected list was not a list: {type(parsed_fixed_json)}. Returning empty list.\")\n",
        "                return \"[]\"\n",
        "            elif expected_type == \"object\" and not isinstance(parsed_fixed_json, dict):\n",
        "                print(f\"   JSON Fixer output for expected object was not an object: {type(parsed_fixed_json)}. Returning empty object.\")\n",
        "                return \"{}\"\n",
        "\n",
        "            print(f\"   JSON self-correction attempt resulted in: {corrected_json_str[:200]}...\")\n",
        "            return corrected_json_str\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"   JSON Fixer output also failed JSON parsing: {e}. Returning empty {expected_type}.\")\n",
        "            return empty_fallback\n",
        "        except Exception as e_gen: # Catch any other unexpected errors during validation\n",
        "            print(f\"   Unexpected error during JSON Fixer validation: {e_gen}. Returning empty {expected_type}.\")\n",
        "            return empty_fallback\n",
        "\n",
        "\n",
        "\n",
        "    async def _process_and_potentially_ingest_web_findings(self, search_analyzer_output: Dict[str, Any], original_query_that_led_to_this: str):\n",
        "        if not self.faiss_manager:\n",
        "            print(\"   Skipping dynamic RAG ingestion: Faiss manager not initialized.\")\n",
        "            return\n",
        "        if not isinstance(search_analyzer_output, dict):\n",
        "            print(\"   Skipping dynamic RAG ingestion: Invalid SearchAnalyzer output format.\")\n",
        "            return\n",
        "\n",
        "        web_results = search_analyzer_output.get(\"google_search_results_summary\", [])\n",
        "        if not web_results or not isinstance(web_results, list):\n",
        "            print(\"   No web results found in SearchAnalyzer output for dynamic RAG ingestion.\")\n",
        "            return\n",
        "\n",
        "        docs_for_ingestion: List[Dict[str, Any]] = []\n",
        "        print(f\"   Considering {len(web_results)} web results for dynamic RAG ingestion (from query: '{original_query_that_led_to_this[:30]}...')...\")\n",
        "\n",
        "        for item_idx, item_data in enumerate(web_results):\n",
        "            if not isinstance(item_data, dict):\n",
        "                continue\n",
        "\n",
        "            url = item_data.get(\"source_url\")\n",
        "            info = item_data.get(\"key_extracted_info\")\n",
        "            title = item_data.get(\"source_title\", f\"WebDocument_{item_idx}_{int(time.time())}\")\n",
        "            score_val = item_data.get(\"relevance_score\", 0.0)\n",
        "            score = float(score_val) if isinstance(score_val, (int, float, str)) and str(score_val).replace('.', '', 1).isdigit() else 0.0\n",
        "\n",
        "\n",
        "            if info and url and score >= 0.75:\n",
        "                try:\n",
        "                    url_hash = hashlib.md5(url.encode('utf-8', 'ignore')).hexdigest()[:12]\n",
        "                except Exception as e_hash:\n",
        "                    url_hash = f\"nohash{item_idx}\"\n",
        "                    print(f\"      Error generating hash for URL '{url}': {e_hash}\")\n",
        "\n",
        "                ts_suffix = str(int(time.time()*1000))[-6:]\n",
        "                doc_id = f\"web_{url_hash}_{sanitize_filename(title, 15)}_{ts_suffix}\"\n",
        "\n",
        "                if doc_id in self.master_document_store:\n",
        "                    print(f\"     Skipping already existing RAG document ID '{doc_id}'.\")\n",
        "                    continue\n",
        "\n",
        "                content = f\"Source URL: {url}\\nTitle: {title}\\n\\nExtracted Info:\\n{info}\"\n",
        "                summary = f\"From {url}: {title} - (Snippet: {info[:80]}...)\"\n",
        "\n",
        "                docs_for_ingestion.append({\n",
        "                    \"id\": doc_id,\n",
        "                    \"content\": content,\n",
        "                    \"summary\": summary,\n",
        "                    \"metadata\": {\n",
        "                        \"source_type\": \"dynamic_web_ingestion\",\n",
        "                        \"original_web_source_url\": url,\n",
        "                        \"discovered_during_query\": original_query_that_led_to_this,\n",
        "                        \"search_analyzer_relevance\": score,\n",
        "                        \"ingestion_timestamp\": time.time()\n",
        "                    }\n",
        "                })\n",
        "                print(f\"     Queued for RAG (ID: {doc_id}): '{title[:50]}...'\")\n",
        "\n",
        "        if docs_for_ingestion:\n",
        "            print(f\"   Dynamically ingesting {len(docs_for_ingestion)} new web findings into RAG store...\")\n",
        "            await self.ingest_documents_into_rag(docs_for_ingestion)\n",
        "        else:\n",
        "            print(\"   No new web findings met criteria for dynamic RAG ingestion.\")\n",
        "\n",
        "    async def ingest_documents_into_rag(self, documents: List[Dict[str, Any]]):\n",
        "        if not FAISS_AVAILABLE:\n",
        "            print(\"Faiss library not available. Skipping RAG ingestion.\")\n",
        "            return\n",
        "        if not self.faiss_manager:\n",
        "            print(\"Faiss manager not initialized. Attempting to initialize for ingestion.\")\n",
        "            self._initialize_faiss_manager_globally(force_new_index=False)\n",
        "            if not self.faiss_manager:\n",
        "                print(\"Failed to initialize Faiss manager. Skipping RAG ingestion.\")\n",
        "                return\n",
        "\n",
        "        if not documents:\n",
        "            print(\"No documents provided for ingestion.\")\n",
        "            return\n",
        "\n",
        "        print(f\"DeepResearchSystem: Ingesting {len(documents)} documents into Global RAG Store...\")\n",
        "\n",
        "        added_to_master_store_count = 0\n",
        "        for doc_data in documents:\n",
        "            content = doc_data.get('content')\n",
        "            doc_id = doc_data.get('id')\n",
        "            summary = doc_data.get('summary', (content[:150] + \"...\") if content else \"No summary.\")\n",
        "\n",
        "            if not doc_id or not content:\n",
        "                print(f\"Skipping document due to missing ID or content: {str(doc_data)[:100]}...\")\n",
        "                continue\n",
        "\n",
        "            if doc_id not in self.master_document_store:\n",
        "                self.master_document_store[doc_id] = {\n",
        "                    \"content\": content,\n",
        "                    \"summary\": summary,\n",
        "                    \"metadata\": doc_data.get(\"metadata\", {})\n",
        "                }\n",
        "                added_to_master_store_count += 1\n",
        "\n",
        "        if added_to_master_store_count > 0:\n",
        "            print(f\"Added {added_to_master_store_count} new documents to master document store. Triggering Faiss update...\")\n",
        "            if hasattr(self.faiss_manager, 'add_documents_from_kb_async'):\n",
        "                await self.faiss_manager.add_documents_from_kb_async()\n",
        "            else:\n",
        "                print(\"ERROR: Faiss manager is missing the 'add_documents_from_kb_async' method.\")\n",
        "            self._save_global_rag_stores()\n",
        "        else:\n",
        "            print(\"No new documents were added to the master document store (all might exist or none provided with content/ID).\")\n",
        "\n",
        "        if self.knowledge_base:\n",
        "            if self.faiss_manager and self.faiss_manager.index:\n",
        "                self.knowledge_base.global_faiss_doc_count = self.faiss_manager.index.ntotal\n",
        "            else:\n",
        "                self.knowledge_base.global_faiss_doc_count = 0\n",
        "        print(f\"Global Faiss index count after ingestion attempt: {self.knowledge_base.global_faiss_doc_count if self.knowledge_base else (self.faiss_manager.index.ntotal if self.faiss_manager and self.faiss_manager.index else 'N/A')}\")\n",
        "\n",
        "    async def _call_llm(self, system_prompt_text: str, user_content_text: str, agent_type: str, use_search_tool: bool = False, stream_override: Optional[bool] = None) -> Tuple[str, Optional[Dict[str, Any]]]:\n",
        "        model_name_to_use = \"UnknownModel\"\n",
        "        full_text_response = \"\"\n",
        "        function_call_details = None # This will capture API-level function calls if explicitly returned by API\n",
        "\n",
        "        try:\n",
        "            if not self.vertex_client:\n",
        "                return \"ERROR: Vertex AI Client (genai.Client) not initialized.\", None\n",
        "            if not GOOGLE_GENAI_SDK_AVAILABLE or not genai or not genai_types:\n",
        "                return \"ERROR: Google GenAI SDK or its types module not imported.\", None\n",
        "\n",
        "            model_config_key_map = {\n",
        "                'Planner': 'planner',\n",
        "                'SearchAnalyzer': 'researcher', # This agent will use the search tool\n",
        "                'Consolidator': 'researcher',\n",
        "                'Auxiliary_Lite_Task': 'flash_lite',\n",
        "                'CreatorAgent_Planner': 'creator',\n",
        "                'CreatorAgent_Executor': 'creator',\n",
        "                'JSON_FIXER_TASK': 'flash_lite'\n",
        "            }\n",
        "            model_config_key = model_config_key_map.get(agent_type, 'researcher')\n",
        "\n",
        "            if model_config_key not in GEMINI_CONFIG:\n",
        "                print(f\"Warning: Config key '{model_config_key}' for agent '{agent_type}' not in GEMINI_CONFIG. Defaulting to 'researcher'.\")\n",
        "                model_config_key = 'researcher'\n",
        "            if model_config_key not in GEMINI_CONFIG: # Final fallback\n",
        "                 model_config_key = 'flash_lite'\n",
        "                 if 'flash_lite' not in GEMINI_CONFIG: return \"ERROR: No valid LLM config.\", None\n",
        "\n",
        "            active_model_config = GEMINI_CONFIG[model_config_key]\n",
        "            model_name_to_use = active_model_config['model_name']\n",
        "            max_tokens = active_model_config.get('max_tokens', 15000)\n",
        "            temperature = active_model_config.get('temperature', 0.25)\n",
        "            top_p = active_model_config.get('top_p', 0.95)\n",
        "\n",
        "            print(f\"\\n Calling LLM (Agent: {agent_type}, Model: {model_name_to_use}). User content len: {len(user_content_text)}. Use Search Tool: {use_search_tool}\")\n",
        "\n",
        "            sys_instruct = {\"role\": \"system\", \"parts\": [{\"text\": str(system_prompt_text)}]}\n",
        "            usr_content = {\"role\": \"user\", \"parts\": [{\"text\": str(user_content_text)}]}\n",
        "            contents_for_api = [usr_content]\n",
        "\n",
        "            safety_settings_list = []\n",
        "            # ... (safety settings population as in your \"20th code\" - ensure it's correct)\n",
        "            if genai_types and hasattr(genai_types, 'HarmCategory') and hasattr(genai_types, 'HarmBlockThreshold'):\n",
        "                try:\n",
        "                    safety_settings_list = [\n",
        "                        {\"category\": genai_types.HarmCategory.HARM_CATEGORY_HATE_SPEECH.name, \"threshold\": genai_types.HarmBlockThreshold.BLOCK_NONE.name},\n",
        "                        {\"category\": genai_types.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT.name, \"threshold\": genai_types.HarmBlockThreshold.BLOCK_NONE.name},\n",
        "                        {\"category\": genai_types.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT.name, \"threshold\": genai_types.HarmBlockThreshold.BLOCK_NONE.name},\n",
        "                        {\"category\": genai_types.HarmCategory.HARM_CATEGORY_HARASSMENT.name, \"threshold\": genai_types.HarmBlockThreshold.BLOCK_NONE.name} ]\n",
        "                except AttributeError: safety_settings_list = [{\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"}, {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"}, {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"}, {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"}]\n",
        "            else: safety_settings_list = [{\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_NONE\"}, {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_NONE\"}, {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_NONE\"}, {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_NONE\"}]\n",
        "\n",
        "\n",
        "            api_call_config_dict = {\n",
        "                \"temperature\": temperature, \"top_p\": top_p, \"max_output_tokens\": max_tokens,\n",
        "                \"safety_settings\": safety_settings_list,\n",
        "                \"system_instruction\": sys_instruct # Correct for genai.Client\n",
        "            }\n",
        "\n",
        "            if use_search_tool:\n",
        "                print(\"   DEBUG_LLM: Enabling 'google_search' tool in API call config.\")\n",
        "                api_call_config_dict[\"tools\"] = [{\"google_search\": {}}]\n",
        "\n",
        "            should_stream_this_call = stream_override if stream_override is not None else (agent_type == \"Consolidator\")\n",
        "\n",
        "            if should_stream_this_call:\n",
        "                # ... (Streaming logic - ensure it robustly gets text and handles potential errors)\n",
        "                print(f\"   DEBUG_LLM: Streaming from Vertex Client (model: {model_name_to_use})...\")\n",
        "                streamed_parts = []\n",
        "                iterator = await asyncio.to_thread(\n",
        "                    self.vertex_client.models.generate_content_stream,\n",
        "                    model=model_name_to_use,\n",
        "                    contents=contents_for_api,\n",
        "                    config=api_call_config_dict\n",
        "                )\n",
        "                def consume_stream(it):\n",
        "                    nonlocal full_text_response # Only need to build full_text_response for streaming\n",
        "                    for chunk in it:\n",
        "                        if hasattr(chunk, 'text') and chunk.text:\n",
        "                            print(chunk.text, end=\"\", flush=True)\n",
        "                            streamed_parts.append(chunk.text)\n",
        "                        # For model-executed search, we don't expect tool_call_details back here for SearchAnalyzer\n",
        "                await asyncio.to_thread(consume_stream, iterator)\n",
        "                full_text_response = \"\".join(streamed_parts)\n",
        "                print(\"\\n   --- End of Stream ---\")\n",
        "\n",
        "            else: # Non-streaming call\n",
        "                print(f\"   DEBUG_LLM: Non-streaming call to Vertex Client (model: {model_name_to_use})...\")\n",
        "                response_object = None\n",
        "                try:\n",
        "                    response_object = await asyncio.to_thread(\n",
        "                        self.vertex_client.models.generate_content,\n",
        "                        model=model_name_to_use,\n",
        "                        contents=contents_for_api, # contents_for_api was from 6th code\n",
        "                        config=api_call_config_dict\n",
        "                    )\n",
        "                except Exception as e_gen_content:\n",
        "                    error_message = f\"ERROR: LLM generate_content call failed - {type(e_gen_content).__name__}: {e_gen_content}\"\n",
        "                    print(f\"ERROR_LLM: {error_message}\")\n",
        "                    full_text_response = error_message\n",
        "                    # ... (logging)\n",
        "                    return full_text_response, None # Return None for function_call_details\n",
        "\n",
        "                if response_object:\n",
        "                    full_text_response = response_object.text if hasattr(response_object, 'text') and response_object.text is not None else \"\"\n",
        "                    if full_text_response:\n",
        "                         print(f\"  DEBUG_LLM (ModelExecSearch): Received text response (len {len(full_text_response)}): '{full_text_response[:100]}...'\")\n",
        "\n",
        "                    # Check for API-level function call (might still be there for other tools or future use)\n",
        "                    if hasattr(response_object, 'candidates') and response_object.candidates and \\\n",
        "                       response_object.candidates[0].content and response_object.candidates[0].content.parts:\n",
        "                        for part_data_obj in response_object.candidates[0].content.parts:\n",
        "                            if hasattr(part_data_obj, 'function_call') and part_data_obj.function_call:\n",
        "                                fc = part_data_obj.function_call\n",
        "                                fc_name_val = getattr(fc, 'name', None)\n",
        "                                fc_args_raw_val = getattr(fc, 'args', None)\n",
        "                                if fc_name_val:\n",
        "                                     fc_args_converted = dict(fc_args_raw_val) if fc_args_raw_val is not None else {}\n",
        "                                     function_call_details = {\"name\": str(fc_name_val), \"args\": fc_args_converted}\n",
        "                                     print(f\" DEBUG_LLM (ModelExecSearch): API-Level Tool Call DETECTED: Name: {fc_name_val}, Args: {fc_args_converted}\")\n",
        "                                     break\n",
        "\n",
        "                    if not full_text_response and not function_call_details:\n",
        "                        if hasattr(response_object, 'prompt_feedback'):\n",
        "                            # ... (block reason handling) ...\n",
        "                            feedback = response_object.prompt_feedback\n",
        "                            if hasattr(feedback, 'block_reason') and feedback.block_reason:\n",
        "                                error_message = f\"LLM call blocked. Reason: {feedback.block_reason}\"\n",
        "                                if hasattr(feedback, 'block_reason_message') and feedback.block_reason_message: error_message += f\" Message: {feedback.block_reason_message}\"\n",
        "                                print(f\"ERROR_LLM: {error_message}\")\n",
        "                                full_text_response = f\"ERROR: {error_message}\"\n",
        "                            else: full_text_response = \"\" # Ensure empty if no text, no tool, no block\n",
        "                        else: full_text_response = \"\"\n",
        "                else:\n",
        "                    print(\"ERROR_LLM: response_object is None after non-streaming API call.\")\n",
        "                    full_text_response = \"ERROR: LLM API call resulted in no response object.\"\n",
        "\n",
        "            print(f\"  DEBUG_LLM_RETURN: About to return from _call_llm. Response text (len {len(full_text_response)}): '{full_text_response[:100]}...'. API Parsed Function Call Details: {function_call_details}\")\n",
        "            return full_text_response, function_call_details\n",
        "\n",
        "        except Exception as e:\n",
        "            error_message = f\"ERROR: LLM call failed (Outer Catch - ModelExecSearch) - {type(e).__name__}: {e}\"\n",
        "            # ... (outer catch logging as before) ...\n",
        "            print(f\"ERROR_LLM: {error_message}\"); traceback.print_exc()\n",
        "            log_details = {\"model_used\": model_name_to_use, \"agent_type\": agent_type, \"use_search_tool\": use_search_tool, \"user_content_snippet\": user_content_text[:200] + \"...\"}\n",
        "            specific_error_type = \"\"\n",
        "            if GOOGLE_API_CORE_EXCEPTIONS_AVAILABLE and google_api_core_exceptions:\n",
        "                if isinstance(e, google_api_core_exceptions.ResourceExhausted): specific_error_type = \"ResourceExhausted\"\n",
        "                elif isinstance(e, google_api_core_exceptions.InvalidArgument): specific_error_type = \"InvalidArgument\"\n",
        "                elif isinstance(e, google_api_core_exceptions.NotFound): specific_error_type = f\"NotFound (Model {model_name_to_use})\"\n",
        "            if specific_error_type: error_message = f\"ERROR: {specific_error_type}. Details: {e}\"\n",
        "            self.research_log.append({\"agent\": \"LLM_CALLER_VERTEX\", \"error\": error_message, \"details\": log_details, \"timestamp\": time.time()})\n",
        "            if self.knowledge_base: self.knowledge_base.add_error(f\"LLM_CALLER_VERTEX ({agent_type})\", error_message, log_details)\n",
        "            print(f\"  DEBUG_LLM_RETURN: About to return from EXCEPTION. error_message: {error_message[:100]}. function_call_details: None\")\n",
        "            return error_message if isinstance(error_message, str) else str(error_message), None\n",
        "\n",
        "\n",
        "\n",
        "    async def _get_vertex_embeddings_batch(self,\n",
        "                                           texts: List[str],\n",
        "                                           task_type: str,\n",
        "                                           titles: Optional[List[Optional[str]]] = None,\n",
        "                                           output_dimensionality: Optional[int] = None,\n",
        "                                           max_retries: int = 3,\n",
        "                                           initial_backoff_seconds: float = 2.0\n",
        "                                           ) -> List[Optional[List[float]]]:\n",
        "        if not self.embedding_model:\n",
        "            error_msg = \"ERROR (_get_vertex_embeddings_batch): Embedding model (self.embedding_model) is not available.\"\n",
        "            print(error_msg)\n",
        "            self.research_log.append({\n",
        "                \"agent\": \"EmbeddingSystem\", \"error\": \"Embedding model not loaded\",\n",
        "                \"details\": \"self.embedding_model was None.\", \"timestamp\": time.time()\n",
        "            })\n",
        "            if self.knowledge_base: self.knowledge_base.add_error(\"EmbeddingSystem\", error_msg, {\"detail\": \"self.embedding_model was None.\"})\n",
        "            return [None] * len(texts)\n",
        "\n",
        "        if not VERTEX_AI_SDK_INITIALIZED_SUCCESSFULLY:\n",
        "            error_msg = \"ERROR (_get_vertex_embeddings_batch): Global Vertex AI SDK was not initialized successfully.\"\n",
        "            print(error_msg)\n",
        "            self.research_log.append({\n",
        "                \"agent\": \"EmbeddingSystem\", \"error\": \"Vertex SDK not initialized\",\n",
        "                \"details\": \"VERTEX_AI_SDK_INITIALIZED_SUCCESSFULLY is False.\", \"timestamp\": time.time()\n",
        "            })\n",
        "            if self.knowledge_base: self.knowledge_base.add_error(\"EmbeddingSystem\", error_msg, {\"detail\": \"VERTEX_AI_SDK_INITIALIZED_SUCCESSFULLY is False.\"})\n",
        "            return [None] * len(texts)\n",
        "\n",
        "        if not TEXT_EMBEDDING_MODEL_CLASS_AVAILABLE or not TextEmbeddingInput:\n",
        "            error_msg = \"ERROR (_get_vertex_embeddings_batch): Vertex AI TextEmbeddingModel or TextEmbeddingInput class not available.\"\n",
        "            print(error_msg)\n",
        "            self.research_log.append({\n",
        "                \"agent\": \"EmbeddingSystem\", \"error\": \"SDK classes missing\",\n",
        "                \"details\": \"TextEmbeddingModel or TextEmbeddingInput not imported.\", \"timestamp\": time.time()\n",
        "            })\n",
        "            if self.knowledge_base: self.knowledge_base.add_error(\"EmbeddingSystem\", error_msg, {\"detail\": \"TextEmbeddingModel or TextEmbeddingInput not imported.\"})\n",
        "            return [None] * len(texts)\n",
        "\n",
        "        if not texts:\n",
        "            return []\n",
        "\n",
        "        all_embeddings_results: List[Optional[List[float]]] = [None] * len(texts)\n",
        "        effective_output_dim = output_dimensionality if output_dimensionality is not None else FAISS_DIMENSION\n",
        "\n",
        "        num_texts_to_process = len(texts)\n",
        "        embedding_model_display_name = \"gemini-embedding-001 (via TextEmbeddingModel)\"\n",
        "        print(f\"   _get_vertex_embeddings_batch: Preparing to process {num_texts_to_process} texts INDIVIDUALLY for '{embedding_model_display_name}', task: {task_type}, target_dim: {effective_output_dim}\")\n",
        "\n",
        "        for i, text_content in enumerate(texts):\n",
        "            current_title = titles[i] if titles and i < len(titles) else None\n",
        "            print(f\"      Processing text {i + 1}/{num_texts_to_process}: '{text_content[:50]}...'\")\n",
        "\n",
        "            instance_payload: Dict[str, Any] = {\"text\": text_content, \"task_type\": task_type}\n",
        "            if task_type == \"RETRIEVAL_DOCUMENT\" and current_title:\n",
        "                instance_payload[\"title\"] = current_title\n",
        "\n",
        "            instance_to_embed: Optional[TextEmbeddingInput] = None\n",
        "            try:\n",
        "                instance_to_embed = TextEmbeddingInput(**instance_payload)\n",
        "            except Exception as e_input_create:\n",
        "                err_msg = f\"ERROR creating TextEmbeddingInput for text index {i} ('{text_content[:50]}...'): {e_input_create}\"\n",
        "                print(f\"    {err_msg}\")\n",
        "                self.research_log.append({\n",
        "                    \"agent\": \"EmbeddingSystem\", \"error\": \"TextEmbeddingInput creation failed\",\n",
        "                    \"details\": {\"text_index\": i, \"exception\": str(e_input_create)}, \"timestamp\": time.time()\n",
        "                })\n",
        "                if self.knowledge_base: self.knowledge_base.add_error(\"EmbeddingSystem\", f\"TextEmbeddingInput creation failed for text {i}\", {\"exception\": str(e_input_create)})\n",
        "                all_embeddings_results[i] = None\n",
        "                if i < num_texts_to_process - 1 and EMBEDDING_API_CALL_DELAY_SECONDS > 0: # Apply delay before next text\n",
        "                    print(f\"      Pausing for {EMBEDDING_API_CALL_DELAY_SECONDS:.2f}s after TextEmbeddingInput creation error...\")\n",
        "                    await asyncio.sleep(EMBEDDING_API_CALL_DELAY_SECONDS)\n",
        "                continue\n",
        "\n",
        "            current_retries = 0\n",
        "            current_backoff = initial_backoff_seconds\n",
        "            call_successful_for_this_text = False\n",
        "\n",
        "            while current_retries <= max_retries:\n",
        "                try:\n",
        "                    api_kwargs = {\"auto_truncate\": True}\n",
        "                    if effective_output_dim:\n",
        "                        api_kwargs[\"output_dimensionality\"] = effective_output_dim\n",
        "\n",
        "                    print(f\"        Attempting API call for text {i + 1} (Retry {current_retries})...\")\n",
        "\n",
        "                    embedding_responses = await asyncio.to_thread(\n",
        "                        self.embedding_model.get_embeddings,\n",
        "                        [instance_to_embed],\n",
        "                        **api_kwargs\n",
        "                    )\n",
        "\n",
        "                    if embedding_responses and len(embedding_responses) == 1:\n",
        "                        response_part = embedding_responses[0]\n",
        "                        if hasattr(response_part, 'values') and response_part.values is not None:\n",
        "                            all_embeddings_results[i] = response_part.values\n",
        "                            call_successful_for_this_text = True\n",
        "                            print(f\"        Text {i + 1} embedding successful.\")\n",
        "                            break\n",
        "                        else:\n",
        "                            print(f\"    WARNING: Received no embedding values for text index {i}. Response part: {response_part}\")\n",
        "                            all_embeddings_results[i] = None\n",
        "                            break\n",
        "                    else:\n",
        "                        print(f\"    WARNING: Received unexpected response structure or count for text {i + 1}. Expected 1 response, got {len(embedding_responses) if embedding_responses else 0}.\")\n",
        "                        break\n",
        "\n",
        "                except google_api_core_exceptions.InvalidArgument as e_invalid_arg:\n",
        "                    err_msg = f\"ERROR (InvalidArgument) for text {i + 1}: {e_invalid_arg}\"\n",
        "                    print(f\"    {err_msg}\")\n",
        "                    self.research_log.append({\n",
        "                        \"agent\": \"EmbeddingSystem\", \"error\": \"InvalidArgument for single text\",\n",
        "                        \"details\": {\"text_index\": i, \"exception\": str(e_invalid_arg)}, \"timestamp\": time.time()\n",
        "                    })\n",
        "                    if self.knowledge_base: self.knowledge_base.add_error(\"EmbeddingSystem\", f\"InvalidArgument for text {i}\", {\"exception\": str(e_invalid_arg)})\n",
        "                    all_embeddings_results[i] = None\n",
        "                    break\n",
        "                except google_api_core_exceptions.TooManyRequests as e_quota:\n",
        "                    err_msg = f\"INFO (TooManyRequests) for text {i + 1}. Attempt {current_retries + 1}/{max_retries + 1}. Error: {e_quota}\"\n",
        "                    print(f\"    {err_msg}\")\n",
        "                    self.research_log.append({\n",
        "                        \"agent\": \"EmbeddingSystem\", \"warning\": \"TooManyRequests for text\",\n",
        "                        \"details\": {\"text_index\": i, \"retry_attempt\": current_retries + 1, \"exception\": str(e_quota)}, \"timestamp\": time.time()\n",
        "                    })\n",
        "                    current_retries += 1\n",
        "                    if current_retries > max_retries:\n",
        "                        print(f\"      Max retries reached for text {i + 1} due to quota. Marking as failed.\")\n",
        "                        if self.knowledge_base: self.knowledge_base.add_error(\"EmbeddingSystem\", f\"Max retries (quota) for text {i}\", {\"exception\": str(e_quota)})\n",
        "                        break\n",
        "                    print(f\"      Retrying text {i + 1} in {current_backoff:.2f} seconds...\")\n",
        "                    await asyncio.sleep(current_backoff)\n",
        "                    current_backoff = min(current_backoff * 2, 60.0)\n",
        "                except Exception as e_embed:\n",
        "                    error_str_lower = str(e_embed).lower()\n",
        "                    is_generic_quota_error = False\n",
        "                    if GOOGLE_API_CORE_EXCEPTIONS_AVAILABLE is False: # Fallback check if specific exceptions weren't imported\n",
        "                        if \"quota\" in error_str_lower or \"429\" in error_str_lower or \"toomanyrequests\" in str(type(e_embed).__name__).lower():\n",
        "                            is_generic_quota_error = True\n",
        "\n",
        "                    if is_generic_quota_error:\n",
        "                        err_msg = f\"INFO (Likely Quota Exceeded - Generic Catch) for text {i + 1}. Attempt {current_retries + 1}/{max_retries + 1}. Error: {e_embed}\"\n",
        "                        print(f\"    {err_msg}\")\n",
        "                        self.research_log.append({\n",
        "                            \"agent\": \"EmbeddingSystem\", \"warning\": \"Likely Quota Exceeded (Generic)\",\n",
        "                            \"details\": {\"text_index\": i, \"retry_attempt\": current_retries + 1, \"exception\": str(e_embed)}, \"timestamp\": time.time()\n",
        "                        })\n",
        "                        current_retries += 1\n",
        "                        if current_retries > max_retries:\n",
        "                            if self.knowledge_base: self.knowledge_base.add_error(\"EmbeddingSystem\", f\"Max retries (generic quota) for text {i}\", {\"exception\": str(e_embed)})\n",
        "                            break\n",
        "                        print(f\"      Retrying text {i + 1} in {current_backoff:.2f} seconds (generic catch)...\")\n",
        "                        await asyncio.sleep(current_backoff); current_backoff = min(current_backoff * 2, 60.0)\n",
        "                        continue # Continue to next retry attempt\n",
        "\n",
        "                    # If not a recognized quota error, treat as unhandled\n",
        "                    err_msg = f\"ERROR (Unhandled Exception) for text {i + 1}. Error: {type(e_embed).__name__}: {e_embed}\"\n",
        "                    print(f\"    {err_msg}\"); traceback.print_exc()\n",
        "                    self.research_log.append({\n",
        "                        \"agent\": \"EmbeddingSystem\", \"error\": \"Unhandled exception in single text embedding\",\n",
        "                        \"details\": {\"text_index\": i, \"exception_type\": type(e_embed).__name__, \"exception\": str(e_embed)}, \"timestamp\": time.time()\n",
        "                    })\n",
        "                    if self.knowledge_base: self.knowledge_base.add_error(\"EmbeddingSystem\", f\"Unhandled exception for text {i}\", {\"exception_type\": type(e_embed).__name__, \"exception\": str(e_embed)})\n",
        "                    break # Break from retry loop for this text\n",
        "\n",
        "            if not call_successful_for_this_text:\n",
        "                print(f\"    Failed to embed text {i + 1}. It will have None embedding.\")\n",
        "                all_embeddings_results[i] = None\n",
        "\n",
        "            if i < num_texts_to_process - 1 and EMBEDDING_API_CALL_DELAY_SECONDS > 0 :\n",
        "                print(f\"      Pausing for {EMBEDDING_API_CALL_DELAY_SECONDS:.2f}s before processing next text...\")\n",
        "                await asyncio.sleep(EMBEDDING_API_CALL_DELAY_SECONDS)\n",
        "\n",
        "        successful_final_count = sum(1 for emb in all_embeddings_results if emb is not None)\n",
        "        print(f\"   _get_vertex_embeddings_batch: Finished processing all texts. Successfully embedded {successful_final_count}/{num_texts_to_process} texts.\")\n",
        "        return all_embeddings_results\n",
        "\n",
        "\n",
        "    async def planner_agent_turn(self, current_cycle_num: int, max_cycles_for_run: int) -> List[str]:\n",
        "        try:\n",
        "            print(f\"\\n---  Planner Agent Turn (Cycle {current_cycle_num}/{max_cycles_for_run}, V4: Full History Aware Batch RAG-Refinement) ---\")\n",
        "            if not self.knowledge_base:\n",
        "                error_msg = \"Planner: KnowledgeBase not initialized\"\n",
        "                print(error_msg); self.research_log.append({\"agent\": \"Planner\", \"error_details\": error_msg, \"timestamp\": time.time()})\n",
        "                return []\n",
        "            if not self.vertex_client:\n",
        "                error_msg = \"Planner: Vertex AI Client not initialized for LLM.\"\n",
        "                print(error_msg); self.research_log.append({\"agent\": \"Planner\", \"error_details\": error_msg, \"timestamp\": time.time()})\n",
        "                if self.knowledge_base: self.knowledge_base.add_error(\"Planner\", error_msg)\n",
        "                return []\n",
        "\n",
        "            if self.faiss_manager and self.faiss_manager.index:\n",
        "                self.knowledge_base.global_faiss_doc_count = self.faiss_manager.index.ntotal\n",
        "            elif self.knowledge_base:\n",
        "                 self.knowledge_base.global_faiss_doc_count = 0\n",
        "\n",
        "            kb_summary_for_phase1 = self.knowledge_base.get_summary_for_planner()\n",
        "\n",
        "            cycle_awareness_prompt = f\"CURRENT CYCLE STATUS: You are in planning Cycle {current_cycle_num} of {max_cycles_for_run} total cycles for this research run. Plan your queries strategically.\"\n",
        "            if current_cycle_num >= max_cycles_for_run -1 :\n",
        "                cycle_awareness_prompt += \" You are in or nearing the final cycles. Focus on queries that will lead to a strong consolidation. Prioritize critical missing pieces over broad new explorations unless essential.\"\n",
        "            print(f\"   Planner: {cycle_awareness_prompt}\")\n",
        "\n",
        "            print(\"   Planner Phase 1: Generating initial candidate web search queries...\")\n",
        "            fresh_rag_insights_str_phase1 = \"No specific fresh RAG insights for initial planning (RAG not available, empty, or no results).\"\n",
        "            pre_planning_rag_query_used_phase1 = \"N/A\"\n",
        "            pre_planning_rag_results_for_log = []\n",
        "\n",
        "            if FAISS_AVAILABLE and self.faiss_manager and hasattr(self.faiss_manager, 'search_global_index_async') and self.faiss_manager.index and self.faiss_manager.index.ntotal > 0:\n",
        "                gap_section_match = re.search(r\"=== IDENTIFIED KNOWLEDGE GAPS OR LEADS FOR FURTHER INVESTIGATION ===\\n(.*?)\\n===\", kb_summary_for_phase1, re.DOTALL | re.IGNORECASE)\n",
        "                identified_gaps_text = gap_section_match.group(1).strip() if gap_section_match else \"\"\n",
        "\n",
        "                if identified_gaps_text and identified_gaps_text.lower() != \"no specific knowledge gaps or new leads identified from analyses in this session yet.\":\n",
        "                    top_gaps = [line.strip(\" \").strip() for line in identified_gaps_text.splitlines() if line.strip().startswith(\" \")][:3]\n",
        "                    if top_gaps:\n",
        "                        rag_query_for_planner_context = f\"Explore further details or solutions related to these identified knowledge gaps: {'; '.join(top_gaps)}\"\n",
        "                        pre_planning_rag_query_used_phase1 = f\"Synthesized from Top KB Gaps: {rag_query_for_planner_context[:100]}...\"\n",
        "                    else:\n",
        "                        rag_query_for_planner_context = self.knowledge_base.original_query\n",
        "                        pre_planning_rag_query_used_phase1 = f\"Original User Query (fallback for Phase 1 RAG): {self.knowledge_base.original_query[:100]}...\"\n",
        "                elif self.knowledge_base.pending_sub_queries and self.knowledge_base.pending_sub_queries[0] != self.knowledge_base.original_query :\n",
        "                    rag_query_for_planner_context = \" \".join(self.knowledge_base.pending_sub_queries[:2])\n",
        "                    pre_planning_rag_query_used_phase1 = f\"From Pending Queries: {rag_query_for_planner_context[:100]}...\"\n",
        "                else:\n",
        "                    rag_query_for_planner_context = self.knowledge_base.original_query\n",
        "                    pre_planning_rag_query_used_phase1 = f\"Original User Query: {self.knowledge_base.original_query[:100]}...\"\n",
        "\n",
        "                print(f\"     Planner Phase 1: Performing pre-planning RAG with: '{pre_planning_rag_query_used_phase1}' (top_k={PLANNER_RAG_CONTEXT_TOP_K})\")\n",
        "\n",
        "                pre_planning_rag_results = await self.faiss_manager.search_global_index_async(\n",
        "                    rag_query_for_planner_context,\n",
        "                    top_k=PLANNER_RAG_CONTEXT_TOP_K\n",
        "                )\n",
        "                pre_planning_rag_results_for_log = pre_planning_rag_results\n",
        "\n",
        "                if pre_planning_rag_results:\n",
        "                    insights_parts = [\"FRESHLY RETRIEVED INTERNAL INSIGHTS (Consider for initial query generation):\"]\n",
        "                    for res_idx, res in enumerate(pre_planning_rag_results):\n",
        "                        insights_parts.append(f\"- Doc ID {res.get('internal_doc_id', 'N/A')} (Sim: {res.get('similarity_score', 0.0):.2f}): {res.get('summary', 'N/A')[:200]}...\")\n",
        "                    fresh_rag_insights_str_phase1 = \"\\n\".join(insights_parts)\n",
        "                    print(f\"     Planner Phase 1: Found {len(pre_planning_rag_results)} RAG insights for initial context.\")\n",
        "            else:\n",
        "                 print(f\"     Planner Phase 1: Pre-planning RAG skipped. Conditions not met (FAISS_AVAILABLE={FAISS_AVAILABLE}, FaissManager Ready={bool(self.faiss_manager and self.faiss_manager.index and self.faiss_manager.index.ntotal > 0)})\")\n",
        "\n",
        "            full_session_query_history_str = \"FULL SESSION QUERY HISTORY (Completed and Pending - Generate DRAFT queries distinct from these):\\n\"\n",
        "            all_session_queries = list(self.knowledge_base.completed_sub_queries) + self.knowledge_base.pending_sub_queries\n",
        "            unique_session_queries = sorted(list(set(q for q in all_session_queries if isinstance(q, str) and q.strip())))\n",
        "\n",
        "            if unique_session_queries:\n",
        "                max_history_items = 30\n",
        "                history_to_show = unique_session_queries[-max_history_items:]\n",
        "                for idx_hist, hist_q in enumerate(history_to_show):\n",
        "                    status = \"Completed\" if hist_q in self.knowledge_base.completed_sub_queries else \"Pending\"\n",
        "                    full_session_query_history_str += f\"- ({status}) {hist_q[:150]}...\\n\"\n",
        "                if len(unique_session_queries) > max_history_items:\n",
        "                    full_session_query_history_str += f\"...and {len(unique_session_queries) - max_history_items} older queries.\\n\"\n",
        "            else:\n",
        "                full_session_query_history_str += \"  No queries processed or pending in this session yet.\\n\"\n",
        "\n",
        "            user_content_phase1 = (\n",
        "                f\"Original Query for this session: {self.knowledge_base.original_query}\\n\\n\"\n",
        "                f\"{cycle_awareness_prompt}\\n\\n\"\n",
        "                f\"{full_session_query_history_str}\\n\"\n",
        "                f\"{fresh_rag_insights_str_phase1}\\n\\n\"\n",
        "                f\"Comprehensive Knowledge Base Summary (Session ID: {self.knowledge_base.session_id}):\\n{kb_summary_for_phase1}\\n\\n\"\n",
        "                \"TASK: Based on ALL the information above (original query, cycle status, FULL SESSION QUERY HISTORY, fresh RAG insights, and full KB summary), \"\n",
        "                \"generate 1 to 3 DRAFT web search queries. These DRAFT queries MUST be genuinely novel and distinct from the FULL SESSION QUERY HISTORY. \"\n",
        "                \"They should aim to address key unanswered gaps or explore new, logical next steps for the research. \"\n",
        "                \"These are initial candidates and will be further refined. Output as a JSON list of strings.\"\n",
        "            )\n",
        "\n",
        "            response_text_phase1, _ = await self._call_llm(PLANNER_AGENT_SYSTEM_PROMPT, user_content_phase1, agent_type=\"Planner\")\n",
        "            thoughts_phase1, output_str_phase1 = parse_llm_response(response_text_phase1)\n",
        "\n",
        "            self.research_log.append({\"agent\": \"Planner_Phase1\",\n",
        "                                      \"thoughts_snippet\": thoughts_phase1[:500]+\"...\", \"output\": output_str_phase1,\n",
        "                                      \"pre_planning_rag_query\": pre_planning_rag_query_used_phase1,\n",
        "                                      \"pre_planning_rag_results_count\": len(pre_planning_rag_results_for_log),\n",
        "                                      \"cycle_info_to_llm\": cycle_awareness_prompt,\n",
        "                                      \"session_history_provided_len\": len(unique_session_queries),\n",
        "                                      \"timestamp\": time.time()})\n",
        "            print(f\"    Planner Phase 1 Thoughts (snippet): {thoughts_phase1[:200]}...\")\n",
        "            print(f\"    Planner Phase 1 Output (Candidate Queries Raw): {output_str_phase1}\")\n",
        "\n",
        "            if not output_str_phase1 or output_str_phase1.startswith(\"ERROR: LLM call failed\"):\n",
        "                error_msg = f\"Planner Phase 1 LLM call failed. Error: {output_str_phase1}\"\n",
        "                print(error_msg); self.knowledge_base.add_error(\"Planner_Phase1\", error_msg, {\"raw_output\": output_str_phase1})\n",
        "                self.research_log[-1][\"error_details\"] = error_msg\n",
        "                return []\n",
        "\n",
        "            initial_candidate_queries = []\n",
        "            try:\n",
        "                parsed_candidates = json.loads(output_str_phase1)\n",
        "                if isinstance(parsed_candidates, list):\n",
        "                    initial_candidate_queries = [q for q in parsed_candidates if isinstance(q, str) and q.strip()]\n",
        "                elif isinstance(parsed_candidates, dict) and \"queries\" in parsed_candidates and isinstance(parsed_candidates[\"queries\"], list):\n",
        "                     initial_candidate_queries = [q for q in parsed_candidates[\"queries\"] if isinstance(q, str) and q.strip()]\n",
        "                else:\n",
        "                    print(f\"   Planner Phase 1 output was not a direct list of queries: {type(parsed_candidates)}. Attempting to parse from string if applicable.\")\n",
        "                    if isinstance(output_str_phase1, str) and output_str_phase1.strip().startswith(\"[\") and output_str_phase1.strip().endswith(\"]\"):\n",
        "                        pass\n",
        "                    else:\n",
        "                         initial_candidate_queries = [line.strip(\"- \").strip() for line in output_str_phase1.splitlines() if line.strip() and len(line.strip()) > 10]\n",
        "\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"   ERROR: Planner Phase 1 output not valid JSON: '{output_str_phase1}'. Error: {e}. Attempting line-by-line recovery.\")\n",
        "                if not output_str_phase1.startswith(\"ERROR:\") and \"\\n\" in output_str_phase1:\n",
        "                    initial_candidate_queries = [line.strip(\"- \").strip() for line in output_str_phase1.splitlines() if line.strip() and len(line.strip()) > 10]\n",
        "                    if initial_candidate_queries: print(f\"     Recovered candidates from lines: {initial_candidate_queries}\")\n",
        "\n",
        "            if not initial_candidate_queries:\n",
        "                print(\"   Planner Phase 1 did not yield any candidate queries after parsing/recovery. Ending planner turn.\")\n",
        "                return []\n",
        "\n",
        "            initial_candidate_queries = initial_candidate_queries[:3]\n",
        "            print(f\"   Planner Phase 1 generated {len(initial_candidate_queries)} candidate(s) for refinement: {initial_candidate_queries}\")\n",
        "\n",
        "            refined_queries_for_cycle = []\n",
        "            if not initial_candidate_queries: return []\n",
        "\n",
        "            print(\"\\n   Planner Phase 2: Preparing batch RAG feedback for candidate queries...\")\n",
        "\n",
        "            candidate_details_for_prompt_list = []\n",
        "            candidate_rag_feedback_map_for_log = {}\n",
        "\n",
        "            for cand_idx, candidate_query in enumerate(initial_candidate_queries):\n",
        "                candidate_prompt_part = f\"--- Candidate Query {cand_idx + 1} ---\\n\"\n",
        "                candidate_prompt_part += f\"Candidate: \\\"{candidate_query}\\\"\\n\"\n",
        "\n",
        "                rag_feedback_str_for_candidate = f\"RAG Feedback for Candidate {cand_idx + 1}: No specific internal RAG documents found highly relevant to this candidate, or RAG search was not applicable/failed.\"\n",
        "                candidate_rag_results_for_this_cand: List[Dict[str, Any]] = []\n",
        "                if FAISS_AVAILABLE and self.faiss_manager and hasattr(self.faiss_manager, 'search_global_index_async') and self.faiss_manager.index and self.faiss_manager.index.ntotal > 0:\n",
        "                    print(f\"     Planner Phase 2: Performing RAG for candidate '{candidate_query[:70]}...' (top_k={PLANNER_CANDIDATE_REFINE_RAG_TOP_K})\")\n",
        "                    candidate_rag_results = await self.faiss_manager.search_global_index_async(\n",
        "                        candidate_query,\n",
        "                        top_k=PLANNER_CANDIDATE_REFINE_RAG_TOP_K\n",
        "                    )\n",
        "                    candidate_rag_results_for_this_cand = candidate_rag_results\n",
        "                    if candidate_rag_results:\n",
        "                        insights_parts = [f\"INTERNAL RAG INSIGHTS for Candidate Query {cand_idx + 1} (\\\"{candidate_query[:70]}...\\\") -- Consider these to refine or discard the candidate:\"]\n",
        "                        for res_idx, res in enumerate(candidate_rag_results):\n",
        "                            insights_parts.append(f\"  - RAG Doc {res_idx+1} (ID {res.get('internal_doc_id', 'N/A')}, Sim: {res.get('similarity_score',0.0):.2f}): {res.get('summary', 'N/A')[:200]}...\")\n",
        "                        rag_feedback_str_for_candidate = \"\\n\".join(insights_parts)\n",
        "                        print(f\"     Gathered {len(candidate_rag_results)} RAG results for candidate: '{candidate_query[:50]}...'\")\n",
        "                else:\n",
        "                    print(f\"     Planner Phase 2 RAG for candidate '{candidate_query[:50]}...' skipped. Conditions not met.\")\n",
        "\n",
        "                candidate_prompt_part += rag_feedback_str_for_candidate + \"\\n\"\n",
        "                candidate_details_for_prompt_list.append(candidate_prompt_part)\n",
        "                candidate_rag_feedback_map_for_log[candidate_query] = [\n",
        "                    {\n",
        "                        \"doc_id\": r.get(\"internal_doc_id\"),\n",
        "                        \"summary_preview\": r.get(\"summary\", r.get(\"content_preview\", \"\"))[:250]+\"...\",\n",
        "                        \"score\": f\"{r.get('similarity_score', 0.0):.3f}\"\n",
        "                    }\n",
        "                    for r in candidate_rag_results_for_this_cand\n",
        "                ]\n",
        "\n",
        "            batch_refinement_prompt_parts = [\n",
        "                f\"Original User Query (Overall Research Goal): {self.knowledge_base.original_query}\\n\",\n",
        "                f\"{cycle_awareness_prompt}\\n\",\n",
        "                f\"High-Level Knowledge Base Summary Snippet:\\n{kb_summary_for_phase1[:1000]}...\\n\\n\",\n",
        "                \"TASK: You are an expert research strategist. You previously generated the following DRAFT candidate web search queries. For each candidate query listed below, I have performed a targeted internal RAG search and provided its specific 'Internal RAG Feedback'.\",\n",
        "                \"Your goal is to CRITICALLY ASSESS each candidate against its RAG feedback AND the overall research context (including cycle status) to produce a final, highly effective web search query OR to explicitly DISCARD the candidate if it's redundant or no longer strategic.\",\n",
        "                \"INSTRUCTIONS FOR PROCESSING EACH CANDIDATE QUERY:\",\n",
        "                \"For each 'Candidate Query' presented below:\",\n",
        "                \"1. SCRUTINIZE its 'Internal RAG Feedback'. Does this feedback directly and comprehensively answer the candidate query? \"\n",
        "                \"   - IF YES (RAG *fully* answers candidate): The refined output for this candidate MUST be an EMPTY STRING \\\"\\\". This indicates the query is no longer needed for web search.\",\n",
        "                \"2. If RAG feedback is relevant but only partially answers, or if it reveals important new nuances, sub-topics, or specific entities related to the candidate query:\",\n",
        "                \"   - REFINE the candidate query. The refined query MUST be a significant improvement:\",\n",
        "                \"     a. Make it MORE SPECIFIC than the original candidate, drilling down into unaddressed details.\",\n",
        "                \"     b. It MUST target aspects NOT explicitly and sufficiently covered by the provided RAG feedback for THIS candidate.\",\n",
        "                \"     c. It should aim to uncover NEW information that builds upon, clarifies, or challenges the RAG insights.\",\n",
        "                \"     d. Example Refinement: If candidate was 'AI in healthcare' and RAG feedback detailed 'AI for diagnostics', a refined query could be 'ethical implications of AI diagnostics in underfunded hospitals' or 'comparative accuracy of AI vs human radiologists for specific rare conditions mentioned in RAG'.\",\n",
        "                \"3. If RAG feedback is sparse, irrelevant, or non-existent for the candidate query:\",\n",
        "                \"   - Re-evaluate the candidate's value for WEB SEARCH based on the 'Original User Query', 'High-Level Knowledge Base Summary Snippet', and importantly, the 'CURRENT CYCLE STATUS'.\",\n",
        "                \"   - IF STILL VALUABLE (especially if early/mid-cycles, or fills a critical gap for consolidation if late-cycles): You may keep the candidate query as is, or make minor improvements for clarity and focus.\",\n",
        "                \"   - IF NO LONGER STRATEGICALLY VALUABLE (e.g., too broad for late cycles, or the overall KB summary implies it's indirectly covered, or it doesn't align with endgame consolidation needs): The refined output for this candidate should be an EMPTY STRING \\\"\\\".\",\n",
        "                \"4. Ensure all final refined queries are distinct and target truly unknown information for WEB SEARCH.\",\n",
        "                \"\\nOUTPUT FORMAT: CRITICALLY IMPORTANT!\",\n",
        "                \"Provide your output as a SINGLE JSON LIST of strings. This list MUST contain one string element for each candidate query you were given, in the exact same order.\",\n",
        "                \"Each string element must be either the refined web search query or an empty string \\\"\\\" if the candidate is discarded.\",\n",
        "                \"Example (if 3 candidates were given): [\\\"refined query for candidate 1\\\", \\\"\\\", \\\"refined query for candidate 3 (if kept or tweaked)\\\"]\\n\"\n",
        "            ]\n",
        "            batch_refinement_prompt_parts.extend(candidate_details_for_prompt_list)\n",
        "            user_content_phase2 = \"\\n\".join(batch_refinement_prompt_parts)\n",
        "\n",
        "            print(f\"   Planner Phase 2: Calling LLM for batch refinement of {len(initial_candidate_queries)} queries (Cycle {current_cycle_num}/{max_cycles_for_run}). User content len: {len(user_content_phase2)}\")\n",
        "            response_text_phase2, _ = await self._call_llm(PLANNER_AGENT_SYSTEM_PROMPT, user_content_phase2, agent_type=\"Planner\")\n",
        "            thoughts_phase2, output_str_phase2 = parse_llm_response(response_text_phase2)\n",
        "\n",
        "            log_entry_details_for_refinement_logging = []\n",
        "            for cq_log_idx, original_cand_query_log in enumerate(initial_candidate_queries):\n",
        "                rag_feedback_for_log = candidate_rag_feedback_map_for_log.get(original_cand_query_log, [{\"summary_preview\": \"N/A - RAG feedback capture error for this query in log\"}])\n",
        "                log_entry_details_for_refinement_logging.append({\n",
        "                    \"candidate_query_index\": cq_log_idx,\n",
        "                    \"candidate_query\": original_cand_query_log,\n",
        "                    \"rag_feedback_provided_to_llm_count\": len(rag_feedback_for_log),\n",
        "                    \"rag_feedback_snippets_preview\": [item['summary_preview'][:100]+\"...\" for item in rag_feedback_for_log[:2]]\n",
        "                })\n",
        "\n",
        "            self.research_log.append({\n",
        "                \"agent\": \"Planner_Phase2_BatchRefine\",\n",
        "                \"input_details_for_refinement\": log_entry_details_for_refinement_logging,\n",
        "                \"thoughts_snippet\": thoughts_phase2[:500]+\"...\",\n",
        "                \"refined_output_raw\": output_str_phase2,\n",
        "                \"cycle_info_to_llm\": cycle_awareness_prompt,\n",
        "                \"timestamp\": time.time()\n",
        "            })\n",
        "            print(f\"    Planner Batch Refinement Thoughts (snippet): {thoughts_phase2[:200]}...\")\n",
        "            print(f\"    Planner Batch Refinement Output (Raw JSON List Expected): {output_str_phase2}\")\n",
        "\n",
        "            if not output_str_phase2 or output_str_phase2.startswith(\"ERROR: LLM call failed\"):\n",
        "                error_msg = f\"Planner Phase 2 (Batch Refinement) LLM call failed. Error: {output_str_phase2}\"\n",
        "                print(error_msg); self.knowledge_base.add_error(\"Planner_Phase2_BatchRefine\", error_msg, {\"raw_output\": output_str_phase2})\n",
        "                self.research_log[-1][\"error_details\"] = error_msg\n",
        "                print(\"   Fallback: Using initial candidate queries due to refinement failure.\")\n",
        "                return initial_candidate_queries\n",
        "\n",
        "            try:\n",
        "                parsed_refined_queries = json.loads(output_str_phase2)\n",
        "                if isinstance(parsed_refined_queries, list) and len(parsed_refined_queries) == len(initial_candidate_queries):\n",
        "                    for i, refined_q_str in enumerate(parsed_refined_queries):\n",
        "                        original_candidate = initial_candidate_queries[i]\n",
        "                        if isinstance(refined_q_str, str) and refined_q_str.strip():\n",
        "                            final_q = refined_q_str.strip()\n",
        "                            print(f\"       Refined/Validated Query (from candidate '{original_candidate[:50]}...'): {final_q}\")\n",
        "                            if final_q not in self.knowledge_base.completed_sub_queries and \\\n",
        "                               final_q not in self.knowledge_base.pending_sub_queries and \\\n",
        "                               final_q not in refined_queries_for_cycle:\n",
        "                                refined_queries_for_cycle.append(final_q)\n",
        "                                self.knowledge_base.add_dependent_query(\n",
        "                                    final_q,\n",
        "                                    parent_queries=[original_candidate],\n",
        "                                    reason=f\"Batch-refined by Planner (Cycle {current_cycle_num}) after RAG feedback on '{original_candidate[:50]}...'.\"\n",
        "                                )\n",
        "                        else:\n",
        "                            print(f\"       Candidate query '{original_candidate[:100]}...' was DISCARDED by Planner during batch refinement.\")\n",
        "                else:\n",
        "                    err_detail = f\"Output was not a list of expected length. Expected {len(initial_candidate_queries)}, Got: {len(parsed_refined_queries) if isinstance(parsed_refined_queries, list) else type(parsed_refined_queries)}\"\n",
        "                    print(f\"   ERROR: Planner Batch Refinement output error. {err_detail}. Raw: {output_str_phase2}\")\n",
        "                    self.knowledge_base.add_error(\"Planner_Phase2_BatchRefine\", \"Output list length mismatch\", {\"raw_output\": output_str_phase2, \"details\": err_detail})\n",
        "                    print(\"   Fallback: Using initial candidate queries due to refinement output format error.\")\n",
        "                    return initial_candidate_queries\n",
        "\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"   ERROR: Planner Batch Refinement output not valid JSON: '{output_str_phase2}'. Error: {e}\")\n",
        "                corrected_list_str = await self._attempt_json_self_correction_with_llm(output_str_phase2, \"Planner Phase 2 query list\", expected_type=\"list\")\n",
        "                try:\n",
        "                    parsed_refined_queries_corrected = json.loads(corrected_list_str)\n",
        "                    if isinstance(parsed_refined_queries_corrected, list) and len(parsed_refined_queries_corrected) == len(initial_candidate_queries):\n",
        "                        print(\"    Self-correction for Planner Phase 2 output successful (JSON list).\")\n",
        "                        for i, refined_q_str in enumerate(parsed_refined_queries_corrected):\n",
        "                            original_candidate = initial_candidate_queries[i]\n",
        "                            if isinstance(refined_q_str, str) and refined_q_str.strip():\n",
        "                                final_q = refined_q_str.strip()\n",
        "                                if final_q not in self.knowledge_base.completed_sub_queries and \\\n",
        "                                   final_q not in self.knowledge_base.pending_sub_queries and \\\n",
        "                                   final_q not in refined_queries_for_cycle:\n",
        "                                    refined_queries_for_cycle.append(final_q)\n",
        "                                    self.knowledge_base.add_dependent_query(final_q, [original_candidate], f\"Batch-refined (self-corrected) by Planner (Cycle {current_cycle_num}) after RAG feedback on '{original_candidate[:50]}...'.\")\n",
        "                            else: print(f\"       Candidate query '{original_candidate[:100]}...' was DISCARDED (after correction).\")\n",
        "                    else:\n",
        "                        raise ValueError(\"Corrected output still not a list of expected length.\")\n",
        "                except Exception as e_corr:\n",
        "                    print(f\"    ERROR: Self-correction for Planner Phase 2 output also failed: {e_corr}. Raw: {output_str_phase2}, Corrected Attempt: {corrected_list_str}\")\n",
        "                    self.knowledge_base.add_error(\"Planner_Phase2_BatchRefine\", f\"JSONDecodeError after correction: {e_corr}\", {\"raw_output\": output_str_phase2, \"corrected_attempt\": corrected_list_str})\n",
        "                    print(\"   Fallback: Using initial candidate queries due to refinement JSON error (even after correction).\")\n",
        "                    return initial_candidate_queries\n",
        "\n",
        "            print(f\"   Planner Phase 2: Final selected queries for this cycle: {refined_queries_for_cycle}\")\n",
        "            return refined_queries_for_cycle\n",
        "\n",
        "        except Exception as e_outer:\n",
        "            error_msg = f\"Critical error in Two-Phase Planner (V4): {e_outer}\"; print(error_msg); traceback.print_exc()\n",
        "            self.research_log.append({\"agent\": \"Planner_TwoPhase_Batch_Outer\", \"error_details\": error_msg, \"exception_obj\": str(e_outer), \"timestamp\": time.time()})\n",
        "            if self.knowledge_base: self.knowledge_base.add_error(\"Planner_TwoPhase_Batch_Outer\", \"Critical turn error\", {\"exception\": str(e_outer)})\n",
        "            return []\n",
        "        if not GOOGLE_SEARCH_API_KEY or not GOOGLE_SEARCH_ENGINE_ID:\n",
        "            error_msg = \"ERROR: Google Search API Key or Search Engine ID is not configured.\"\n",
        "            print(f\"      {error_msg}\")\n",
        "            if self.knowledge_base:\n",
        "                self.knowledge_base.add_error(\"SearchAnalyzer_LiveSearch\", error_msg, {\"query\": query, \"detail\": \"API Key/Engine ID missing\"})\n",
        "            return error_msg # Return error string\n",
        "\n",
        "        try:\n",
        "            # This is a synchronous call, so we run it in a thread\n",
        "            # Ensure 'build' is imported if GOOGLE_API_PYTHON_CLIENT_AVAILABLE is True\n",
        "            # from googleapiclient.discovery import build # Already checked/imported at top level\n",
        "\n",
        "            def search_sync():\n",
        "                service = build(\"customsearch\", \"v1\", developerKey=GOOGLE_SEARCH_API_KEY)\n",
        "                # Note: Using .cse().list() for Programmable Search Engine\n",
        "                res = service.cse().list(q=query, cx=GOOGLE_SEARCH_ENGINE_ID, num=num_results).execute()\n",
        "                return res.get('items', [])\n",
        "\n",
        "            search_items = await asyncio.to_thread(search_sync)\n",
        "\n",
        "            formatted_results = []\n",
        "            if search_items:\n",
        "                for item_idx, item in enumerate(search_items):\n",
        "                    # Ensure essential fields are present, provide defaults or skip if critical ones are missing\n",
        "                    link = item.get(\"link\")\n",
        "                    title = item.get(\"title\")\n",
        "                    snippet = item.get(\"snippet\")\n",
        "\n",
        "                    if not link or not title:\n",
        "                        print(f\"        Skipping search result {item_idx+1} due to missing link or title.\")\n",
        "                        continue\n",
        "\n",
        "                    formatted_results.append({\n",
        "                        \"source_url\": link,\n",
        "                        \"source_title\": title,\n",
        "                        \"key_extracted_info\": snippet or \"No snippet available.\", # Use snippet as key_info\n",
        "                        \"snippet_from_search_engine\": snippet or \"No snippet available.\" # For consistency\n",
        "                    })\n",
        "                print(f\"      Live Google Search returned {len(formatted_results)} formatted results for '{query}'.\")\n",
        "            else:\n",
        "                print(f\"      Live Google Search returned no results for '{query}'.\")\n",
        "            return formatted_results\n",
        "        except Exception as e:\n",
        "            error_msg = f\"ERROR executing live Google Search for '{query}': {type(e).__name__} - {e}\"\n",
        "            print(f\"      {error_msg}\")\n",
        "            traceback.print_exc() # For more detailed debugging during development\n",
        "            if self.knowledge_base:\n",
        "                self.knowledge_base.add_error(\"SearchAnalyzer_LiveSearch\", error_msg, {\"query\": query, \"exception_type\": type(e).__name__})\n",
        "            return error_msg # Return error string\n",
        "\n",
        "    async def search_analyzer_agent_turn(self, query_to_execute: str) -> Dict[str, Any]:\n",
        "        try:\n",
        "            print(f\"\\n---  Search & Analyzer Turn (Query: '{query_to_execute}', Model-Executed Google Search V2) ---\")\n",
        "            if not self.knowledge_base:\n",
        "                error_payload = {\"error\": \"KnowledgeBase not initialized for SearchAnalyzer\", \"query_executed\": query_to_execute}\n",
        "                self.research_log.append({\"agent\": \"SearchAnalyzer\", \"error_details\": error_payload[\"error\"], \"timestamp\": time.time()})\n",
        "                return error_payload\n",
        "            if not self.vertex_client:\n",
        "                error_payload = {\"error\": \"Vertex AI Client not initialized for SearchAnalyzer\", \"query_executed\": query_to_execute}\n",
        "                # ... (standard error logging)\n",
        "                if self.knowledge_base: self.knowledge_base.add_error(\"SearchAnalyzer\", error_payload[\"error\"], error_payload)\n",
        "                return error_payload\n",
        "\n",
        "            faiss_search_results_for_llm_context = []\n",
        "            if FAISS_AVAILABLE and self.faiss_manager and hasattr(self.faiss_manager, 'search_global_index_async') and self.faiss_manager.index and self.faiss_manager.index.ntotal > 0:\n",
        "                print(f\"   DEBUG_SA_MODEL_SEARCH_V2: Querying Internal Faiss Index for: '{query_to_execute}' (top_k={RAG_TOP_K})\")\n",
        "                # ... (FAISS RAG search logic as in Turn 44) ...\n",
        "                rag_results = await self.faiss_manager.search_global_index_async(query_to_execute, top_k=RAG_TOP_K)\n",
        "                if rag_results:\n",
        "                    print(f\"   DEBUG_SA_MODEL_SEARCH_V2: Found {len(rag_results)} results from Global Faiss Index.\")\n",
        "                    for res in rag_results:\n",
        "                        faiss_search_results_for_llm_context.append({\n",
        "                            \"internal_doc_id\": res.get(\"internal_doc_id\"),\n",
        "                            \"summary_of_internal_doc_content\": res.get(\"summary\", res.get(\"content_preview\", \"N/A\"))[:1000],\n",
        "                            \"relevance_score_from_faiss\": res.get(\"similarity_score\")\n",
        "                        })\n",
        "            else:\n",
        "                print(f\"   DEBUG_SA_MODEL_SEARCH_V2: Internal RAG search skipped or yielded no results for '{query_to_execute}'.\")\n",
        "\n",
        "            user_content_for_llm = (\n",
        "                f\"Please perform a Google Search for the following query and then analyze the results as per your role, incorporating any relevant internal Faiss findings provided below:\\n\"\n",
        "                f\"Search Query to Execute via Google Search Tool: \\\"{query_to_execute}\\\"\\n\\n\"\n",
        "                \"INTERNAL KNOWLEDGE BASE (FAISS INDEX) FINDINGS (Context for your analysis):\\n\"\n",
        "                f\"{json.dumps(faiss_search_results_for_llm_context, indent=2) if faiss_search_results_for_llm_context else 'No relevant documents were found in the internal Faiss index for this specific query.'}\\n\\n\"\n",
        "                \"YOUR TASK:\\n\"\n",
        "                \"1. Use the Google Search tool with the 'Search Query to Execute via Google Search Tool'.\\n\"\n",
        "                \"2. Critically evaluate the search results you obtain (these will be part of your response if the tool call is successful internally by the model).\\n\"\n",
        "                \"3. Synthesize these findings along with any relevant 'INTERNAL KNOWLEDGE BASE (FAISS INDEX) FINDINGS'.\\n\"\n",
        "                \"4. Produce the comprehensive JSON output as specified by your role (SEARCH_ANALYZER_AGENT_SYSTEM_PROMPT). Ensure the 'search_results_summary' field in your JSON contains the information from the Google Search you performed, and 'google_search_needed_and_performed' is true.\"\n",
        "            )\n",
        "\n",
        "            print(\"   DEBUG_SA_MODEL_SEARCH_V2: Calling SearchAnalyzer LLM (expecting model-executed search & synthesis)...\")\n",
        "            response_text, tool_call_details_api = await self._call_llm(\n",
        "                SEARCH_ANALYZER_AGENT_SYSTEM_PROMPT, # The main synthesis prompt\n",
        "                user_content_for_llm,\n",
        "                agent_type=\"SearchAnalyzer\",\n",
        "                use_search_tool=True # Critical: This enables the tool for the LLM\n",
        "            )\n",
        "\n",
        "            if tool_call_details_api:\n",
        "                print(f\"   WARNING_SA_MODEL_SEARCH_V2: _call_llm returned API tool call details: {tool_call_details_api}. This was not expected for model-executed search. LLM might be trying to delegate back.\")\n",
        "\n",
        "            thoughts, final_output_str = parse_llm_response(response_text)\n",
        "            # ... (The rest of search_analyzer_agent_turn from Turn 44, including logging, JSON parsing, self-correction, and KB update, and RAG ingestion call)\n",
        "            log_entry = {\n",
        "                \"agent\": \"SearchAnalyzer_ModelExecutedSearch_V2\", \"query_executed\": query_to_execute,\n",
        "                \"thoughts_snippet\": thoughts[:500]+\"...\",\n",
        "                \"llm_full_response_text\": response_text,\n",
        "                \"llm_parsed_final_answer\": final_output_str[:500]+\"...\",\n",
        "                \"api_tool_call_returned_by_call_llm\": tool_call_details_api,\n",
        "                \"timestamp\": time.time()\n",
        "            }\n",
        "            self.research_log.append(log_entry)\n",
        "            print(f\"   DEBUG_SA_MODEL_SEARCH_V2:  SearchAnalyzer Thoughts: {thoughts[:200]}...\")\n",
        "            print(f\"   DEBUG_SA_MODEL_SEARCH_V2:  SearchAnalyzer Parsed Final Answer: {final_output_str[:300]}...\")\n",
        "\n",
        "            if not final_output_str or final_output_str.startswith(\"ERROR:\"):\n",
        "                error_msg = f\"SearchAnalyzer LLM call failed or returned error. Error: {final_output_str}\"\n",
        "                print(f\"ERROR_SA_MODEL_SEARCH_V2: {error_msg}\")\n",
        "                error_payload = {\"error\": error_msg, \"raw_error_message\": final_output_str, \"query_executed\": query_to_execute}\n",
        "                if self.knowledge_base: self.knowledge_base.add_error(\"SearchAnalyzer_Synth\", error_payload[\"error\"], error_payload)\n",
        "                log_entry[\"error_details\"] = error_msg\n",
        "                return error_payload\n",
        "\n",
        "            data = None\n",
        "            try:\n",
        "                data = json.loads(final_output_str)\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"INFO_SA_MODEL_SEARCH_V2: Initial JSON parsing failed. Error: {e}. Attempting self-correction...\")\n",
        "                corrected_json_string = await self._attempt_json_self_correction_with_llm(final_output_str, query_to_execute, expected_type=\"object\")\n",
        "                try:\n",
        "                    data = json.loads(corrected_json_string)\n",
        "                    print(\"   DEBUG_SA_MODEL_SEARCH_V2: Self-correction successful.\")\n",
        "                except Exception as e_correction_parse:\n",
        "                    error_msg = f\"Failed to parse JSON after self-correction: {e_correction_parse}\"\n",
        "                    error_payload = {\"error\": error_msg, \"original_raw_output\": final_output_str, \"corrected_attempt\": corrected_json_string, \"query_executed\": query_to_execute}\n",
        "                    if self.knowledge_base: self.knowledge_base.add_error(\"SearchAnalyzer_Parse\", error_msg, error_payload)\n",
        "                    log_entry[\"error_details\"] = error_msg\n",
        "                    return error_payload\n",
        "\n",
        "            if data is None:\n",
        "                 error_msg = \"SearchAnalyzer final output could not be parsed or corrected into JSON.\"\n",
        "                 error_payload = {\"error\": error_msg, \"raw_output\": final_output_str, \"query_executed\": query_to_execute}\n",
        "                 if self.knowledge_base: self.knowledge_base.add_error(\"SearchAnalyzer_Parse\", error_msg, error_payload)\n",
        "                 log_entry[\"error_details\"] = error_msg\n",
        "                 return error_payload\n",
        "\n",
        "            if not isinstance(data, dict):\n",
        "                error_msg = f\"SearchAnalyzer output was not a JSON object. Got type: {type(data)}\"\n",
        "                print(f\"ERROR_SA_MODEL_SEARCH_V2: {error_msg} Parsed: {str(data)[:200]}...\")\n",
        "                error_payload = {\"error\": error_msg, \"raw_output\": final_output_str, \"parsed_data_preview\": str(data)[:200], \"parsed_type\": str(type(data)), \"query_executed\": query_to_execute}\n",
        "                if self.knowledge_base: self.knowledge_base.add_error(\"SearchAnalyzer_Parse\", error_msg, error_payload)\n",
        "                log_entry[\"error_details\"] = error_msg\n",
        "                return error_payload\n",
        "\n",
        "            data[\"google_search_needed_and_performed\"] = data.get(\"google_search_needed_and_performed\", False) or use_search_tool # If use_search_tool was true, model should reflect that\n",
        "            if \"search_query_executed\" not in data or data[\"search_query_executed\"] != query_to_execute:\n",
        "                data[\"search_query_executed\"] = query_to_execute\n",
        "\n",
        "            list_fields_to_check = [\"internal_faiss_results_summary\", \"google_search_results_summary\", \"new_questions_or_leads_from_this_query\"]\n",
        "            for field in list_fields_to_check:\n",
        "                if not isinstance(data.get(field), list):\n",
        "                    print(f\"   WARNING_SA_MODEL_SEARCH_V2: Output for '{field}' not a list. Defaulting to []. Original: {data.get(field)}\")\n",
        "                    data[field] = []\n",
        "\n",
        "            self.knowledge_base.add_search_analysis(query_to_execute, data)\n",
        "            if data.get(\"google_search_results_summary\"):\n",
        "                 print(\"   DEBUG_SA_MODEL_SEARCH_V2: Triggering RAG ingestion for model-provided search results.\")\n",
        "                 await self._process_and_potentially_ingest_web_findings(data, query_to_execute)\n",
        "            return data\n",
        "\n",
        "        except Exception as e_outer_turn:\n",
        "            error_msg = f\"Critical error in SearchAnalyzer turn (Model-Executed Search V2): {e_outer_turn}\"; print(error_msg); traceback.print_exc()\n",
        "            error_payload = {\"error\": error_msg, \"exception_obj\": str(e_outer_turn), \"query_executed\": query_to_execute}\n",
        "            if self.knowledge_base: self.knowledge_base.add_error(\"SearchAnalyzer_Outer\", \"Critical turn error\", error_payload)\n",
        "            self.research_log.append({\"agent\": \"SearchAnalyzer_Outer_Critical\", \"query_executed\": query_to_execute, \"error_details\": error_msg, \"timestamp\": time.time()})\n",
        "            return error_payload\n",
        "\n",
        "\n",
        "    async def _get_interim_consolidation(self) -> str:\n",
        "        print(\"\\n---  Generating Interim Consolidated Report ---\")\n",
        "        if not self.knowledge_base:\n",
        "            return \"KnowledgeBase not initialized for interim report.\"\n",
        "        kb_summary = self.knowledge_base.get_summary_for_consolidator(max_len=15000)\n",
        "        user_content = (f\"Original User Query for this session: {self.knowledge_base.original_query}\\n\\n\"\n",
        "                        f\"Current Knowledge Base Summary (Session ID: {self.knowledge_base.session_id}):\\n{kb_summary}\\n\\n\"\n",
        "                        \"This is an INTERIM report. Please synthesize the current findings into a CONCISE summary \"\n",
        "                        \"of the research progress so far for the Original User Query. Focus on key actionable insights, \"\n",
        "                        \"what has been answered, and what major gaps remain. Keep it brief (2-4 paragraphs).\")\n",
        "        response_text, _ = await self._call_llm(CONSOLIDATOR_AGENT_SYSTEM_PROMPT,\n",
        "                                                user_content,\n",
        "                                                agent_type=\"Auxiliary_Lite_Task\",\n",
        "                                                stream_override=False)\n",
        "        thoughts, report_str = parse_llm_response(response_text)\n",
        "        if not report_str or report_str.startswith(\"ERROR: LLM call failed\"):\n",
        "            print(f\"Consolidator (interim) LLM call failed: {report_str}\")\n",
        "            return (f\"Interim Update (LLM synthesis failed for interim report - see logs for details):\\n\"\n",
        "                    f\"Original Query: {self.knowledge_base.original_query}\\n\"\n",
        "                    f\"Completed Queries: {len(self.knowledge_base.completed_sub_queries)}\\n\"\n",
        "                    f\"Pending Queries: {len(self.knowledge_base.pending_sub_queries)}\\n\"\n",
        "                    f\"Total Findings Logged: {len(self.knowledge_base.detailed_findings)}\\n\"\n",
        "                    f\"Global RAG Documents: {self.knowledge_base.global_faiss_doc_count}\\n\"\n",
        "                    f\"Review detailed logs for more specific progress.\")\n",
        "        return report_str\n",
        "\n",
        "    async def consolidator_agent_turn(self, stream_final_report_override: Optional[bool] = None) -> str:\n",
        "        print(\"\\n---  Consolidator Agent Turn (Final Report) ---\")\n",
        "        if not self.vertex_client or not self.knowledge_base:\n",
        "            error_msg = \"Consolidator: Vertex AI Client or KnowledgeBase not initialized.\"\n",
        "            log_entry = {\"agent\": \"Consolidator\", \"error_details\": error_msg, \"timestamp\": time.time()}\n",
        "            self.research_log.append(log_entry)\n",
        "            if self.knowledge_base: self.knowledge_base.add_error(\"Consolidator\", error_msg)\n",
        "            return f\"ERROR: {error_msg}\"\n",
        "\n",
        "        kb_summary_for_consolidator = self.knowledge_base.get_summary_for_consolidator()\n",
        "\n",
        "        user_content = (f\"Original User Query (Session ID: {self.knowledge_base.session_id}): {self.knowledge_base.original_query}\\n\\n\"\n",
        "                        f\"Full Knowledge Base Summary (all findings from executed queries and created artifacts):\\n{kb_summary_for_consolidator}\\n\\n\"\n",
        "                        \"Your task is to synthesize ALL available information from the Knowledge Base Summary into a final, comprehensive, well-structured, and deeply insightful answer to the Original User Query. \"\n",
        "                        \"If the primary goal was research, address all aspects of the original query, highlight key findings, different perspectives, and any limitations or gaps. \"\n",
        "                        \"If the primary goal was creation of design artifacts (indicated by populated 'created_artifacts' in the KB summary), your main task is to present these created artifacts clearly, perhaps with a brief introduction and conclusion. \"\n",
        "                        \"The output should be the final, polished answer intended for the end-user, within <final_answer> tags. Ensure the answer is directly readable and well-formatted (e.g., use Markdown for readability if appropriate for textual synthesis, but if presenting structured artifacts, ensure their structure is clear).\"\n",
        "                       )\n",
        "\n",
        "        # CORRECTED LINE: Use stream_final_report_override\n",
        "        should_stream_this_call = stream_final_report_override if stream_final_report_override is not None else True\n",
        "\n",
        "        response_text, _ = await self._call_llm(CONSOLIDATOR_AGENT_SYSTEM_PROMPT,\n",
        "                                                user_content,\n",
        "                                                agent_type=\"Consolidator\",\n",
        "                                                stream_override=should_stream_this_call)\n",
        "\n",
        "        thoughts, report_str = parse_llm_response(response_text)\n",
        "\n",
        "        log_entry = {\"agent\": \"Consolidator\",\n",
        "                     \"thoughts_snippet\": thoughts[:300] + \"...\" if thoughts and not should_stream_this_call else (\"Streamed thoughts if any.\" if should_stream_this_call else \"No thoughts.\"),\n",
        "                     \"final_report_preview\": report_str[:500]+\"...\" if report_str and not report_str.startswith(\"ERROR:\") else report_str,\n",
        "                     \"input_summary_length\": len(kb_summary_for_consolidator),\n",
        "                     \"timestamp\": time.time()}\n",
        "\n",
        "        if not report_str or report_str.startswith(\"ERROR: LLM call failed\"):\n",
        "            log_entry[\"error_details\"] = f\"Consolidator (final) LLM call failed: {report_str}\"\n",
        "            print(log_entry[\"error_details\"])\n",
        "            if self.knowledge_base: self.knowledge_base.add_error(\"Consolidator\", \"LLM call failed\", {\"raw_error\": report_str})\n",
        "            self.research_log.append(log_entry)\n",
        "            return (f\"ERROR: The final report could not be generated due to an LLM error.\\n\"\n",
        "                    f\"Details: {report_str}\\n\"\n",
        "                    f\"Please review the research log for {self.knowledge_base.session_id} for any partial findings.\")\n",
        "\n",
        "        self.research_log.append(log_entry)\n",
        "\n",
        "        if not should_stream_this_call:\n",
        "            print(f\" Consolidator Thoughts (snippet): {thoughts[:200]}...\")\n",
        "            print(\"\\n---  Final Report (Non-Streamed) ---\"); print(report_str); print(\"--- End of Report ---\")\n",
        "        return report_str\n",
        "\n",
        "# ContextAbove:\n",
        "#         # ... (previous consolidator_agent_turn method content) ...\n",
        "#         return report_str\n",
        "\n",
        "    async def creator_agent_turn(self,\n",
        "                                 elaboration_task: str,\n",
        "                                 blueprint_summary: str,\n",
        "                                 current_creation_cycle: int, # This is the overall cycle for the task\n",
        "                                 max_creation_cycles: int # This is the budget of cycles for the entire creation process\n",
        "                                 ) -> Tuple[Optional[Dict[str, Any]], bool, str]:\n",
        "        if not self.knowledge_base:\n",
        "            return None, True, \"CreatorAgent Error: KnowledgeBase not initialized.\"\n",
        "\n",
        "        agent_name_plan = \"CreatorAgent_Planner\"\n",
        "        agent_name_exec = \"CreatorAgent_Executor\"\n",
        "        status_message = \"\"\n",
        "\n",
        "        # --- Planning Phase ---\n",
        "        if not self.knowledge_base.creator_plan and self.knowledge_base.creator_completed_subtasks_count == 0:\n",
        "            print(f\"\\n---  Creator Agent - Planning Phase (Cycle {current_creation_cycle} of task budget {max_creation_cycles}) ---\")\n",
        "            self.knowledge_base.created_artifacts = {} # Reset artifacts when new plan is made\n",
        "\n",
        "            # Calculate remaining cycles for execution after this planning step\n",
        "            remaining_cycles_for_execution = max_creation_cycles - 1 # current_creation_cycle is 0 for planning\n",
        "            if remaining_cycles_for_execution < 0: remaining_cycles_for_execution = 0\n",
        "\n",
        "            planning_user_content = (\n",
        "                f\"ELABORATION TASK (Original User Query):\\n{elaboration_task}\\n\\n\"\n",
        "                f\"EXISTING BLUEPRINT SUMMARY (Primary Resource - First 15k chars of KB summary):\\n{blueprint_summary[:15000]}...\\n\\n\"\n",
        "                f\"AVAILABLE CREATION CYCLES (for all sub-tasks to be planned): {remaining_cycles_for_execution}\\n\\n\"\n",
        "                \"Your task is to deconstruct the ELABORATION TASK into a sequence of manageable sub-tasks. \"\n",
        "                \"For each sub-task, identify 2-3 keywords/phrases for RAG against the full blueprint_summary. \"\n",
        "                \"Output a JSON object with a 'creation_plan' list as per your system prompt. \"\n",
        "                \"Ensure the number of sub-tasks is reasonable for the available creation cycles.\"\n",
        "            )\n",
        "            response_text, _ = await self._call_llm(\n",
        "                CREATOR_PLANNER_SYSTEM_PROMPT, planning_user_content, agent_type=\"CreatorAgent_Planner\",\n",
        "            )\n",
        "            thoughts, plan_json_str = parse_llm_response(response_text)\n",
        "            self.research_log.append({\n",
        "                \"agent\": agent_name_plan, \"thoughts_snippet\": thoughts[:300]+\"...\",\n",
        "                \"output_plan_str\": plan_json_str[:500]+\"...\", \"timestamp\": time.time()\n",
        "            })\n",
        "\n",
        "            parsed_plan_successfully = False\n",
        "            try:\n",
        "                if not plan_json_str or plan_json_str.startswith(\"ERROR:\"):\n",
        "                    raise ValueError(f\"Creator Planner LLM call failed or returned error: {plan_json_str}\")\n",
        "\n",
        "                plan_data = json.loads(plan_json_str)\n",
        "\n",
        "                if isinstance(plan_data, dict) and isinstance(plan_data.get(\"creation_plan\"), list) and plan_data[\"creation_plan\"]:\n",
        "                    self.knowledge_base.creator_plan = plan_data[\"creation_plan\"]\n",
        "                    parsed_plan_successfully = True\n",
        "                elif isinstance(plan_data, list): # LLM might return the list directly\n",
        "                    print(\"   Creator Planner: LLM returned a list directly, attempting to wrap in {'creation_plan': ...}\")\n",
        "                    self.knowledge_base.creator_plan = plan_data\n",
        "                    parsed_plan_successfully = True\n",
        "                elif isinstance(plan_data, dict) and \"sub_task_id\" in plan_data: # LLM might return a single sub-task object\n",
        "                    print(\"   Creator Planner: LLM returned a single sub-task object, attempting to wrap in a list.\")\n",
        "                    self.knowledge_base.creator_plan = [plan_data]\n",
        "                    parsed_plan_successfully = True\n",
        "\n",
        "                if parsed_plan_successfully and self.knowledge_base.creator_plan:\n",
        "                    status_message = f\"Creator Agent: Generated creation plan with {len(self.knowledge_base.creator_plan)} sub-tasks.\"\n",
        "                    print(f\"   {status_message}\")\n",
        "                else:\n",
        "                    status_message = \"Creator Agent: Planning phase resulted in an empty or invalid plan structure after attempting recovery.\"\n",
        "                    print(f\"   ERROR: {status_message} Raw output was: {plan_json_str[:500]}...\")\n",
        "                    self.knowledge_base.add_error(agent_name_plan, status_message, {\"raw_output\": plan_json_str})\n",
        "                    return None, True, status_message # Halt creation if planning fails definitively\n",
        "            except Exception as e:\n",
        "                status_message = f\"CreatorAgent Planner failed to parse plan: {e}. Output: {plan_json_str[:500]}...\"\n",
        "                print(f\"   ERROR: {status_message}\"); self.knowledge_base.add_error(agent_name_plan, status_message, {\"raw_output\": plan_json_str})\n",
        "                return None, True, status_message # Halt creation if planning fails\n",
        "\n",
        "        # --- Execution Phase ---\n",
        "        if not self.knowledge_base.creator_plan: # Check again, in case planning failed above\n",
        "            return None, True, \"CreatorAgent Error: No creation plan available to execute.\"\n",
        "\n",
        "        if self.knowledge_base.creator_completed_subtasks_count >= len(self.knowledge_base.creator_plan):\n",
        "            status_message = \"Creator Agent: All planned sub-tasks previously marked complete.\"\n",
        "            return {\"artifact_id\": \"finalization\", \"generated_content\": \"All creation sub-tasks complete.\"}, True, status_message\n",
        "\n",
        "        current_sub_task_index = self.knowledge_base.creator_completed_subtasks_count\n",
        "        sub_task = self.knowledge_base.creator_plan[current_sub_task_index]\n",
        "        sub_task_id = sub_task.get(\"sub_task_id\", f\"subtask_{current_sub_task_index}\")\n",
        "        sub_task_desc = sub_task.get(\"sub_task_description\", \"No description for sub-task.\")\n",
        "\n",
        "        print(f\"\\n---  Creator Agent - Execution Phase (Overall Cycle {current_creation_cycle} of budget {max_creation_cycles} - Sub-task {current_sub_task_index + 1}/{len(self.knowledge_base.creator_plan)}: {sub_task_desc[:100]}...) ---\")\n",
        "\n",
        "        rag_context_for_subtask = \"No specific RAG context retrieved for this sub-task (FAISS manager/index unavailable or no keywords/results).\"\n",
        "        if FAISS_AVAILABLE and self.faiss_manager and hasattr(self.faiss_manager, 'search_global_index_async') and self.faiss_manager.index and self.faiss_manager.index.ntotal > 0:\n",
        "            keywords = sub_task.get(\"required_blueprint_info_keywords\", [])\n",
        "            if keywords and isinstance(keywords, list) and all(isinstance(k, str) for k in keywords):\n",
        "                rag_query = f\"Regarding blueprint: details on {', '.join(keywords)} for task '{sub_task_desc[:70]}...'\"\n",
        "                print(f\"     Creator RAG Query for blueprint: {rag_query[:150]}...\")\n",
        "                rag_results = await self.faiss_manager.search_global_index_async(rag_query, top_k=CREATOR_AGENT_RAG_TOP_K)\n",
        "                if rag_results:\n",
        "                    rag_context_parts = [f\"Blueprint Snippet {i+1} (Doc ID: {r.get('internal_doc_id', 'N/A')}, Similarity: {r.get('similarity_score',0.0):.2f}):\\n{r.get('summary', 'N/A')}\" for i, r in enumerate(rag_results)]\n",
        "                    rag_context_for_subtask = \"\\n\\n\".join(rag_context_parts)\n",
        "                    print(f\"     Retrieved {len(rag_results)} blueprint snippets for sub-task (max {CREATOR_AGENT_RAG_TOP_K}).\")\n",
        "                else:\n",
        "                    print(f\"     No RAG results found for keywords: {keywords}\")\n",
        "            else:\n",
        "                print(\"     No valid keywords found for RAG query in sub-task.\")\n",
        "\n",
        "        execution_user_content = (\n",
        "            f\"OVERALL ELABORATION TASK (For Context):\\n{elaboration_task}\\n\\n\"\n",
        "            f\"CURRENT SUB-TASK ID: {sub_task_id}\\nCURRENT SUB-TASK DESCRIPTION:\\n{sub_task_desc}\\n\\n\"\n",
        "            f\"RELEVANT BLUEPRINT CONTEXT (Retrieved via RAG from full blueprint_summary - Use this as primary info):\\n{rag_context_for_subtask}\\n\\n\"\n",
        "            \"YOUR TASK: Generate the specific design artifact piece for the CURRENT SUB-TASK. Output a JSON object as per your system prompt.\"\n",
        "        )\n",
        "        response_text_exec, _ = await self._call_llm(\n",
        "            CREATOR_EXECUTOR_SYSTEM_PROMPT, execution_user_content, agent_type=\"CreatorAgent_Executor\"\n",
        "        )\n",
        "        thoughts_exec, artifact_piece_json_str = parse_llm_response(response_text_exec)\n",
        "\n",
        "        log_entry_details = {\n",
        "            \"agent\": agent_name_exec, \"sub_task_id\": sub_task_id,\n",
        "            \"thoughts_snippet\": thoughts_exec[:300]+\"...\",\n",
        "            \"output_artifact_piece_str\": artifact_piece_json_str[:500]+\"...\",\n",
        "            \"rag_query_keywords_used\": sub_task.get(\"required_blueprint_info_keywords\", []), \"timestamp\": time.time()\n",
        "        }\n",
        "        self.research_log.append(log_entry_details)\n",
        "\n",
        "        artifact_piece_data = None\n",
        "        try:\n",
        "            if not artifact_piece_json_str or artifact_piece_json_str.startswith(\"ERROR:\"):\n",
        "                raise ValueError(f\"Creator Executor LLM call failed or returned error: {artifact_piece_json_str}\")\n",
        "            artifact_piece_data = json.loads(artifact_piece_json_str)\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"INFO: Initial JSON parsing failed for Creator Executor. Error: {e}. Attempting self-correction...\")\n",
        "            corrected_json_string = await self._attempt_json_self_correction_with_llm(artifact_piece_json_str, sub_task_desc, expected_type=\"object\")\n",
        "            try:\n",
        "                artifact_piece_data = json.loads(corrected_json_string)\n",
        "                print(\"   Self-correction successful for Creator output.\")\n",
        "            except Exception as e_corr_parse:\n",
        "                status_message = f\"CreatorAgent Executor failed to parse artifact after self-correction for '{sub_task_id}': {e_corr_parse}. Original: {artifact_piece_json_str[:100]}... Corrected: {corrected_json_string[:100]}...\"\n",
        "                print(f\"   ERROR: {status_message}\")\n",
        "                self.knowledge_base.add_error(agent_name_exec, status_message, {\"raw_output\": artifact_piece_json_str, \"corrected_attempt\": corrected_json_string, \"sub_task\": sub_task})\n",
        "                self.knowledge_base.creator_completed_subtasks_count += 1\n",
        "                return None, (self.knowledge_base.creator_completed_subtasks_count >= len(self.knowledge_base.creator_plan)), status_message\n",
        "\n",
        "        if isinstance(artifact_piece_data, dict) and \"generated_content\" in artifact_piece_data:\n",
        "            actual_artifact_id = artifact_piece_data.get(\"artifact_id\", sub_task_id) # Prefer ID from LLM if provided and matches\n",
        "            self.knowledge_base.created_artifacts[actual_artifact_id] = artifact_piece_data\n",
        "            status_message = f\"Creator Agent: Successfully generated artifact piece for sub-task ID '{actual_artifact_id}'. Confidence: {artifact_piece_data.get('confidence_score', 'N/A')}\"\n",
        "            print(f\"   {status_message}\")\n",
        "            if artifact_piece_data.get(\"notes_or_questions_for_next_step\"):\n",
        "                print(f\"     LLM Notes/Questions: {artifact_piece_data['notes_or_questions_for_next_step']}\")\n",
        "            self.knowledge_base.creator_completed_subtasks_count += 1\n",
        "        elif artifact_piece_data is not None:\n",
        "            status_message = f\"Creator Agent: Executor output for '{sub_task_id}' was valid JSON but lacked 'generated_content' or wasn't a dict. Output: {str(artifact_piece_data)[:200]}...\"\n",
        "            print(f\"   ERROR: {status_message}\")\n",
        "            self.knowledge_base.add_error(agent_name_exec, status_message, {\"parsed_output\": artifact_piece_data, \"sub_task\": sub_task})\n",
        "            self.knowledge_base.creator_completed_subtasks_count += 1\n",
        "            return None, (self.knowledge_base.creator_completed_subtasks_count >= len(self.knowledge_base.creator_plan)), status_message\n",
        "        elif artifact_piece_data is None: # Parsing failed even after correction\n",
        "             status_message = f\"CreatorAgent Executor failed to parse artifact for '{sub_task_id}' even after correction attempt. Original Output: {artifact_piece_json_str[:200]}...\"\n",
        "             print(f\"   ERROR: {status_message}\")\n",
        "             self.knowledge_base.add_error(agent_name_exec, status_message, {\"raw_output\": artifact_piece_json_str, \"sub_task\": sub_task})\n",
        "             self.knowledge_base.creator_completed_subtasks_count += 1\n",
        "             return None, (self.knowledge_base.creator_completed_subtasks_count >= len(self.knowledge_base.creator_plan)), status_message\n",
        "\n",
        "        creation_is_complete = self.knowledge_base.creator_completed_subtasks_count >= len(self.knowledge_base.creator_plan)\n",
        "        if creation_is_complete:\n",
        "            status_message += \" All creation sub-tasks are now marked complete.\"\n",
        "            print(f\"   {status_message}\")\n",
        "\n",
        "        return artifact_piece_data, creation_is_complete, status_message\n",
        "\n",
        "\n",
        "\n",
        "    def _format_created_artifacts_to_markdown(self) -> str:\n",
        "        if not self.knowledge_base or not self.knowledge_base.created_artifacts:\n",
        "            return \"No artifacts were created in this session.\"\n",
        "\n",
        "        markdown_output = [\"# CognitoSynth - Created Design Artifacts\\n\"]\n",
        "        if self.knowledge_base.session_id:\n",
        "            markdown_output.append(f\"**Session ID:** {self.knowledge_base.session_id}\")\n",
        "        if self.knowledge_base.original_query:\n",
        "            markdown_output.append(f\"**Original Elaboration Task:** {self.knowledge_base.original_query}\\n\")\n",
        "\n",
        "        if self.knowledge_base.creator_plan:\n",
        "            markdown_output.append(f\"**Creation Plan Summary:** {len(self.knowledge_base.creator_plan)} sub-tasks planned, {self.knowledge_base.creator_completed_subtasks_count} attempted/completed.\\n\")\n",
        "\n",
        "        for artifact_id, artifact_data_obj in self.knowledge_base.created_artifacts.items():\n",
        "            markdown_output.append(f\"\\n---\\n## Artifact: `{artifact_id}`\\n\")\n",
        "            if isinstance(artifact_data_obj, dict):\n",
        "                generated_content = artifact_data_obj.get(\"generated_content\", \"No content generated for this artifact.\")\n",
        "                confidence = artifact_data_obj.get(\"confidence_score\", \"N/A\")\n",
        "                notes = artifact_data_obj.get(\"notes_or_questions_for_next_step\", \"\")\n",
        "\n",
        "                markdown_output.append(f\"**Confidence Score:** {confidence}\\n\")\n",
        "                if notes:\n",
        "                    markdown_output.append(f\"**LLM Notes/Questions:** {notes}\\n\")\n",
        "\n",
        "                markdown_output.append(\"\\n**Generated Content:**\\n\")\n",
        "                if isinstance(generated_content, str):\n",
        "                    is_diagram = False\n",
        "                    mermaid_keywords = [\"graph TD\", \"graph LR\", \"sequenceDiagram\", \"classDiagram\", \"stateDiagram\", \"erDiagram\", \"journey\", \"gantt\", \"pie\"]\n",
        "                    plantuml_keywords = [\"@startuml\"]\n",
        "\n",
        "                    if any(keyword in generated_content for keyword in mermaid_keywords):\n",
        "                        markdown_output.append(f\"```mermaid\\n{generated_content}\\n```\")\n",
        "                        is_diagram = True\n",
        "                    elif any(keyword.lower() in generated_content.lower() for keyword in plantuml_keywords):\n",
        "                        markdown_output.append(f\"```plantuml\\n{generated_content}\\n```\")\n",
        "                        is_diagram = True\n",
        "\n",
        "                    if not is_diagram:\n",
        "                        if '\\n' in generated_content or len(generated_content) > 80:\n",
        "                             markdown_output.append(f\"```text\\n{generated_content}\\n```\")\n",
        "                        else:\n",
        "                             markdown_output.append(generated_content)\n",
        "                elif isinstance(generated_content, (dict, list)):\n",
        "                    markdown_output.append(f\"```json\\n{json.dumps(generated_content, indent=2, ensure_ascii=False)}\\n```\")\n",
        "                else:\n",
        "                    markdown_output.append(f\"```text\\n{str(generated_content)}\\n```\")\n",
        "            else:\n",
        "                markdown_output.append(f\"```text\\n{str(artifact_data_obj)}\\n```\")\n",
        "\n",
        "        return \"\\n\".join(markdown_output)\n",
        "\n",
        "    def _summarize_research_log_for_display(self, max_entries=25, max_len_per_entry=400) -> str:\n",
        "        if not self.research_log:\n",
        "            return \"Research log is empty for this session.\"\n",
        "\n",
        "        effective_max_entries = max_entries\n",
        "\n",
        "        summary = f\"Research Log Highlights for Session '{self.knowledge_base.session_id if self.knowledge_base else 'N/A'}' (most recent {effective_max_entries} entries):\\n\"\n",
        "        entries_to_show = self.research_log[-effective_max_entries:]\n",
        "\n",
        "        for i, entry in enumerate(reversed(entries_to_show)):\n",
        "            ts_raw = entry.get(\"timestamp\", 0)\n",
        "            ts = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(ts_raw)) if ts_raw else \"N/A\"\n",
        "            agent = entry.get(\"agent\", \"UnknownAgent\")\n",
        "            output_preview = \"\"\n",
        "            error_info = entry.get(\"error_details\") or entry.get(\"error\")\n",
        "\n",
        "            if agent == \"Planner_Phase1\":\n",
        "                output_preview = f\"Candidates: {str(entry.get('output', 'N/A'))[:150]}\"\n",
        "            elif agent == \"Planner_Phase2_BatchRefine\":\n",
        "                output_preview = f\"Refined: {str(entry.get('refined_output_raw', 'N/A'))[:150]}\"\n",
        "            elif agent.startswith(\"SearchAnalyzer\"):\n",
        "                query_exec = entry.get('query_executed', 'N/A')\n",
        "                rag_count = entry.get(\"rag_results_provided_to_llm_count\", 0)\n",
        "                output_preview = f\"Query: '{query_exec[:50]}...', RAG:{rag_count}\"\n",
        "                if entry.get(\"llm_final_output_preview\", \"N/A\") != \"N/A\":\n",
        "                     output_preview += f\" -> Synth: {entry.get('llm_final_output_preview', '')[:70]}...\"\n",
        "            elif agent.startswith(\"CreatorAgent_Planner\"):\n",
        "                output_preview = f\"Plan Output: {entry.get('output_plan_str', 'N/A')[:100]}...\"\n",
        "            elif agent.startswith(\"CreatorAgent_Executor\") or agent == \"CreatorAgent_CycleExecution\":\n",
        "                sub_task_id = entry.get('sub_task_id_attempted', entry.get('sub_task_id', 'N/A'))\n",
        "                content_preview = entry.get('output_preview', entry.get('output_artifact_piece_str', 'N/A'))\n",
        "                output_preview = f\"Sub-task: '{sub_task_id}', Output: {str(content_preview)[:100]}...\"\n",
        "            elif agent == \"Consolidator\":\n",
        "                output_preview = f\"Report Preview: {entry.get('final_report_preview', 'N/A')[:100]}...\"\n",
        "            elif agent == \"LLM_CALLER_VERTEX\":\n",
        "                 output_preview = str(error_info or \"LLM Call details in log\")\n",
        "            else:\n",
        "                output_preview = json.dumps({k:v for k,v in entry.items() if k != 'thoughts_snippet'})[:max_len_per_entry-50]\n",
        "\n",
        "            if error_info and agent not in [\"LLM_CALLER_VERTEX\"]:\n",
        "                 output_preview += f\" | ERROR: {str(error_info)[:100]}...\"\n",
        "\n",
        "            summary_line = f\"- [{ts}] {agent}: {output_preview}\"\n",
        "            summary += summary_line[:max_len_per_entry]\n",
        "            summary += \"...\\n\" if len(summary_line) > max_len_per_entry else \"\\n\"\n",
        "\n",
        "        if self.knowledge_base and self.knowledge_base.errors:\n",
        "            summary += f\"\\n--- KnowledgeBase Errors Logged This Session ({len(self.knowledge_base.errors)}) ---\\n\"\n",
        "            for err_idx, err in enumerate(self.knowledge_base.errors[-3:]): # Show last 3 KB errors\n",
        "                summary += f\"  KB Error {err_idx+1}: Agent '{err.get('agent', 'N/A')}' - {err.get('message', 'N/A')}\\n\"\n",
        "                if err.get('details') and isinstance(err.get('details'), dict) and err['details'].get('raw_output'):\n",
        "                    summary += f\"    LLM Raw Output causing error (preview): {str(err['details']['raw_output'])[:150]}...\\n\"\n",
        "                elif err.get('details'):\n",
        "                    summary += f\"    Details: {str(err.get('details'))[:150]}...\\n\"\n",
        "        return summary\n",
        "\n",
        "    async def run_deep_research(self, user_query: str,\n",
        "                                current_session_id: Optional[str] = None,\n",
        "                                stream_final_report_override: Optional[bool] = None,\n",
        "                                max_cycles_override: Optional[int] = None) -> str:\n",
        "        if not self.vertex_client:\n",
        "            return \"System Error: Vertex AI Client failed to initialize earlier. Cannot run research.\"\n",
        "\n",
        "        if not self.knowledge_base or (current_session_id and self.knowledge_base.session_id != current_session_id):\n",
        "            if current_session_id:\n",
        "                if not self._load_session_state(current_session_id):\n",
        "                    print(f\"Could not load session '{current_session_id}'. Starting new KB for this session ID with query: '{user_query}'.\")\n",
        "                    self.knowledge_base = KnowledgeBase(user_query, current_session_id)\n",
        "                    self.research_log = []\n",
        "                elif self.knowledge_base and self.knowledge_base.original_query != user_query:\n",
        "                     print(f\"Warning: Loaded session '{current_session_id}' (orig_query: '{self.knowledge_base.original_query}') being run with new query focus: '{user_query}'. Updating session's original query and resetting creator state.\")\n",
        "                     self.knowledge_base.original_query = user_query\n",
        "                     self.knowledge_base.pending_sub_queries = [user_query]\n",
        "                     self.knowledge_base.creator_plan = []\n",
        "                     self.knowledge_base.created_artifacts = {}\n",
        "                     self.knowledge_base.creator_completed_subtasks_count = 0\n",
        "            else:\n",
        "                session_id_new = sanitize_filename(user_query) + \"_\" + str(int(time.time()))\n",
        "                print(f\"No specific session ID. Initializing new KB for session: {session_id_new} with query: '{user_query}'\")\n",
        "                self.knowledge_base = KnowledgeBase(user_query, session_id_new)\n",
        "                self.research_log = []\n",
        "        elif self.knowledge_base and self.knowledge_base.original_query != user_query :\n",
        "            print(f\"Warning: Existing KB for session '{self.knowledge_base.session_id}' (orig_query: '{self.knowledge_base.original_query}') being run with new query focus: '{user_query}'. Updating KB original query and resetting creator state.\")\n",
        "            self.knowledge_base.original_query = user_query\n",
        "            self.knowledge_base.pending_sub_queries = [user_query]\n",
        "            self.knowledge_base.creator_plan = []\n",
        "            self.knowledge_base.created_artifacts = {}\n",
        "            self.knowledge_base.creator_completed_subtasks_count = 0\n",
        "\n",
        "        if not self.knowledge_base:\n",
        "            error_msg = \"System Error: KnowledgeBase could not be initialized for the research run.\"\n",
        "            print(error_msg); self.research_log.append({\"agent\": \"SystemRun\", \"error_details\": error_msg, \"timestamp\": time.time()})\n",
        "            return error_msg\n",
        "\n",
        "        print(f\"\\n Starting/Continuing Research/Creation for Query: \\\"{self.knowledge_base.original_query}\\\" (Session: {self.knowledge_base.session_id})\")\n",
        "\n",
        "        if FAISS_AVAILABLE and (not self.faiss_manager or not self.faiss_manager.index):\n",
        "            print(\"Faiss Manager/index not ready. Initializing...\")\n",
        "            self._initialize_faiss_manager_globally(force_new_index=False)\n",
        "\n",
        "        if self.faiss_manager and self.faiss_manager.index:\n",
        "             self.knowledge_base.global_faiss_doc_count = self.faiss_manager.index.ntotal\n",
        "        elif self.knowledge_base:\n",
        "             self.knowledge_base.global_faiss_doc_count = 0\n",
        "\n",
        "        current_overall_cycle = 0\n",
        "        max_total_cycles_for_this_task = max_cycles_override if max_cycles_override is not None else INITIAL_MAX_CYCLES_IF_NO_OTHER_STOP\n",
        "\n",
        "        is_elaboration_task = \"elaborate on this architecture\" in user_query.lower() or \\\n",
        "                              (\"leveraging the established\" in user_query.lower() and \"blueprint\" in user_query.lower()) or \\\n",
        "                              (\"design the internal workings\" in user_query.lower() and \"CIAL\" in user_query) or \\\n",
        "                              (\"illustrative learning pathway\" in user_query.lower())\n",
        "\n",
        "        max_planner_cycles = max_total_cycles_for_this_task\n",
        "        if is_elaboration_task and \"not to conduct new web searches\" in user_query.lower():\n",
        "            max_planner_cycles = 0\n",
        "            print(f\"Task identified as blueprint elaboration with NO NEW WEB SEARCHES. Max Planner cycles: {max_planner_cycles}. Total cycles for task: {max_total_cycles_for_this_task}.\")\n",
        "        elif is_elaboration_task:\n",
        "            max_planner_cycles = min(1, max_total_cycles_for_this_task)\n",
        "            print(f\"Task identified as blueprint elaboration. Max Planner cycles: {max_planner_cycles}. Total cycles for task: {max_total_cycles_for_this_task}.\")\n",
        "\n",
        "\n",
        "        if IPYTHON_AVAILABLE and current_overall_cycle == 0:\n",
        "            clear_output(wait=True)\n",
        "            print(f\" Initializing for: \\\"{self.knowledge_base.original_query}\\\" (Session: {self.knowledge_base.session_id})\")\n",
        "            print(f\"Global Faiss Index: {self.knowledge_base.global_faiss_doc_count} documents. Max total cycles for task: {max_total_cycles_for_this_task}\")\n",
        "            if not hasattr(self, '_initial_state_displayed_this_run') or not self._initial_state_displayed_this_run:\n",
        "                interim_report_display = await self._get_interim_consolidation()\n",
        "                display(HTML(f\"<pre style='white-space: pre-wrap; font-family: monospace; background-color: #f0f0f0; padding: 10px; border-radius: 5px;'><b>INITIAL STATE - Session: {self.knowledge_base.session_id}</b><br/>{interim_report_display}</pre>\"))\n",
        "                print(f\"\\n{self._summarize_research_log_for_display(max_entries=5, max_len_per_entry=200)}\\n\")\n",
        "                self._initial_state_displayed_this_run = True\n",
        "\n",
        "        search_queries_from_planner = [\"initial_dummy_query_to_start_loop\"]\n",
        "        research_phase_active = True if max_planner_cycles > 0 else False\n",
        "        research_phase_cycles_run_this_turn = 0\n",
        "\n",
        "        if max_planner_cycles == 0 and is_elaboration_task:\n",
        "            print(\"Skipping Research Phase as max_planner_cycles is 0 for this elaboration task.\")\n",
        "            research_phase_active = False\n",
        "            search_queries_from_planner = []\n",
        "\n",
        "        while research_phase_cycles_run_this_turn < max_planner_cycles and search_queries_from_planner and research_phase_active:\n",
        "            current_overall_cycle += 1\n",
        "            research_phase_cycles_run_this_turn +=1\n",
        "            print(f\"\\n=====  RESEARCH CYCLE {current_overall_cycle}/{max_total_cycles_for_this_task} (Planner Phase {research_phase_cycles_run_this_turn}/{max_planner_cycles}) =====\")\n",
        "            start_cycle_time = time.time()\n",
        "            search_queries_from_planner = await self.planner_agent_turn(current_overall_cycle, max_total_cycles_for_this_task)\n",
        "\n",
        "            if not search_queries_from_planner:\n",
        "                planner_error_logged = any(\n",
        "                    \"Planner\" in entry.get(\"agent\",\"\") and\n",
        "                    (entry.get(\"error_details\") or (isinstance(entry.get(\"output\"), str) and entry.get(\"output\",\"\").startswith(\"ERROR:\")))\n",
        "                    for entry in reversed(self.research_log[-3:])\n",
        "                )\n",
        "                if planner_error_logged: print(\" Planner Agent encountered an error. Halting research phase.\")\n",
        "                else: print(\" Planner: No more web search queries suggested. Ending research phase.\")\n",
        "                research_phase_active = False; break\n",
        "\n",
        "            print(f\" Planner suggested {len(search_queries_from_planner)} web search queries: {search_queries_from_planner}\")\n",
        "            for i, query in enumerate(list(search_queries_from_planner)):\n",
        "                if not query or not query.strip(): continue\n",
        "                print(f\"\\n   >>> Processing Web/RAG Query {i+1}/{len(search_queries_from_planner)}: '{query}'\")\n",
        "                if query not in self.knowledge_base.completed_sub_queries and query not in self.knowledge_base.pending_sub_queries:\n",
        "                    self.knowledge_base.add_dependent_query(query, [], \"From Planner for current cycle.\")\n",
        "                analysis_result = await self.search_analyzer_agent_turn(query)\n",
        "                if isinstance(analysis_result, dict) and \"error\" not in analysis_result:\n",
        "                    if analysis_result.get(\"google_search_needed_and_performed\") and analysis_result.get(\"google_search_results_summary\"):\n",
        "                        await self._process_and_potentially_ingest_web_findings(analysis_result, query)\n",
        "                await asyncio.sleep(0.5)\n",
        "\n",
        "            cycle_duration = time.time() - start_cycle_time\n",
        "            self._save_current_session_state()\n",
        "            if IPYTHON_AVAILABLE:\n",
        "                clear_output(wait=True); print(f\" Research for: \\\"{self.knowledge_base.original_query}\\\" - Cycle {current_overall_cycle} ENDED\")\n",
        "                interim_report = await self._get_interim_consolidation(); display(HTML(f\"<pre style='white-space: pre-wrap;'><b>INTERIM (After Cycle {current_overall_cycle})</b><br/>{interim_report}</pre>\"))\n",
        "                print(f\"\\n{self._summarize_research_log_for_display(max_entries=5, max_len_per_entry=200)}\\n\")\n",
        "            print(f\"=====  End of RESEARCH CYCLE {current_overall_cycle} (Duration: {cycle_duration:.2f}s) =====\")\n",
        "            if not self.knowledge_base.pending_sub_queries and search_queries_from_planner:\n",
        "                 print(\"All planner queries processed, no other pending. Ending research phase early.\");\n",
        "                 research_phase_active = False; break\n",
        "            await asyncio.sleep(1)\n",
        "\n",
        "        final_report_str = \"\"\n",
        "        creator_phase_was_active = False\n",
        "\n",
        "        if is_elaboration_task and (not research_phase_active or research_phase_cycles_run_this_turn >= max_planner_cycles):\n",
        "            print(\"\\n Research phase concluded or skipped. Task identified as blueprint elaboration. Triggering Creator Agent... \")\n",
        "            creator_phase_was_active = True\n",
        "            creation_complete_flag = False\n",
        "\n",
        "            if not self.knowledge_base.creator_plan and self.knowledge_base.creator_completed_subtasks_count == 0:\n",
        "                print(\"   Creator Agent: Performing initial planning sub-task...\")\n",
        "                if current_overall_cycle < max_total_cycles_for_this_task:\n",
        "                    current_overall_cycle += 1\n",
        "                    print(f\"   (This planning step counts as overall cycle {current_overall_cycle}/{max_total_cycles_for_this_task})\")\n",
        "                    _, _, plan_status_msg = await self.creator_agent_turn(\n",
        "                        elaboration_task=self.knowledge_base.original_query,\n",
        "                        blueprint_summary=self.knowledge_base.get_summary_for_consolidator(max_len=75000),\n",
        "                        current_creation_cycle=0,\n",
        "                        max_creation_cycles=(max_total_cycles_for_this_task - research_phase_cycles_run_this_turn)\n",
        "                    )\n",
        "                    print(f\"   Creator Planner Status: {plan_status_msg}\")\n",
        "                    self._save_current_session_state()\n",
        "                    if not self.knowledge_base.creator_plan:\n",
        "                        final_report_str = json.dumps({\"creator_agent_error\": \"Failed to generate a creation plan. \" + plan_status_msg})\n",
        "                        creation_complete_flag = True\n",
        "                else:\n",
        "                    final_report_str = json.dumps({\"creator_agent_error\": \"Not enough cycles remaining for Creator Agent planning.\"})\n",
        "                    creation_complete_flag = True\n",
        "\n",
        "\n",
        "            if self.knowledge_base.creator_plan and not creation_complete_flag:\n",
        "                print(f\"   Creator Agent: Plan ready with {len(self.knowledge_base.creator_plan)} sub-tasks. Starting execution phase...\")\n",
        "\n",
        "                while current_overall_cycle < max_total_cycles_for_this_task and not creation_complete_flag:\n",
        "                    current_overall_cycle += 1\n",
        "                    print(f\"\\n=====  CREATION CYCLE {current_overall_cycle}/{max_total_cycles_for_this_task} (Creator Execution Sub-Task) =====\")\n",
        "                    start_cycle_time = time.time()\n",
        "\n",
        "                    generated_artifact_piece, creation_is_complete_now, status_msg = await self.creator_agent_turn(\n",
        "                        elaboration_task=self.knowledge_base.original_query,\n",
        "                        blueprint_summary=self.knowledge_base.get_summary_for_consolidator(max_len=75000),\n",
        "                        current_creation_cycle=current_overall_cycle,\n",
        "                        max_creation_cycles=max_total_cycles_for_this_task\n",
        "                    )\n",
        "                    creation_complete_flag = creation_is_complete_now\n",
        "                    print(f\"   Creator Execution Status: {status_msg}\")\n",
        "\n",
        "                    self.research_log.append({\n",
        "                        \"agent\": \"CreatorAgent_CycleExecution\", \"cycle\": current_overall_cycle,\n",
        "                        \"sub_task_id_attempted\": generated_artifact_piece.get(\"artifact_id\", \"N/A\") if isinstance(generated_artifact_piece, dict) else \"N/A\",\n",
        "                        \"output_preview\": str(generated_artifact_piece)[:500]+\"...\",\n",
        "                        \"creation_complete_flag_returned\": creation_is_complete_now,\n",
        "                        \"status_message_from_creator\": status_msg, \"timestamp\": time.time()\n",
        "                    })\n",
        "                    cycle_duration = time.time() - start_cycle_time\n",
        "                    self._save_current_session_state()\n",
        "                    if IPYTHON_AVAILABLE:\n",
        "                        clear_output(wait=True); print(f\" Creation for: \\\"{self.knowledge_base.original_query}\\\" - Cycle {current_overall_cycle} ENDED\")\n",
        "                        interim_report = await self._get_interim_consolidation(); display(HTML(f\"<pre style='white-space: pre-wrap;'><b>INTERIM (After Creation Cycle {current_overall_cycle})</b><br/>{interim_report}</pre>\"))\n",
        "                        print(f\"\\n{self._summarize_research_log_for_display(max_entries=5, max_len_per_entry=200)}\\n\")\n",
        "                    print(f\"=====  End of CREATION CYCLE {current_overall_cycle} (Duration: {cycle_duration:.2f}s) =====\")\n",
        "                    if creation_is_complete_now: break\n",
        "                    await asyncio.sleep(1)\n",
        "\n",
        "            if self.knowledge_base.created_artifacts:\n",
        "                final_report_str = self._format_created_artifacts_to_markdown()\n",
        "\n",
        "                for artifact_id, artifact_data_obj in self.knowledge_base.created_artifacts.items():\n",
        "                    if isinstance(artifact_data_obj, dict) and \"generated_content\" in artifact_data_obj:\n",
        "                        content_to_save = artifact_data_obj[\"generated_content\"]\n",
        "                        if isinstance(content_to_save, str) and (\"diagram\" in artifact_id.lower() or \"mermaid\" in content_to_save.lower() or \"graph TD\" in content_to_save or \"@startuml\" in content_to_save.lower()):\n",
        "                            ext = \".mmd\" if (\"mermaid\" in content_to_save.lower() or \"graph TD\" in content_to_save) else \".puml\" if \"@startuml\" in content_to_save.lower() else \".txt\"\n",
        "                            filename = os.path.join(SESSIONS_DIR, self.knowledge_base.session_id, f\"{sanitize_filename(artifact_id)}{ext}\")\n",
        "                            try:\n",
        "                                with open(filename, \"w\", encoding='utf-8') as f_out: f_out.write(content_to_save)\n",
        "                                print(f\"   Artifact '{artifact_id}' saved to: {filename}\")\n",
        "                            except Exception as e_save_art: print(f\"Error saving artifact {artifact_id}: {e_save_art}\")\n",
        "\n",
        "            elif creator_phase_was_active and not final_report_str:\n",
        "                 final_report_str = json.dumps({\"creator_agent_error\": \"Creator agent was active but produced no compiled artifacts.\"})\n",
        "\n",
        "        if not creator_phase_was_active or not final_report_str:\n",
        "            print(f\"\\n=====  Generating Final Consolidated Report via Consolidator for Session: {self.knowledge_base.session_id} =====\")\n",
        "            final_report_str = await self.consolidator_agent_turn(stream_final_report_override=stream_final_report_override)\n",
        "\n",
        "        self._save_current_session_state()\n",
        "        if hasattr(self, '_initial_state_displayed_this_run'):\n",
        "            del self._initial_state_displayed_this_run\n",
        "        return final_report_str\n",
        "\n",
        "\n",
        "\n",
        "async def main_colab_runner_async():\n",
        "    if not VERTEX_PROJECT_ID or not VERTEX_LOCATION:\n",
        "        print(\"Error: VERTEX_PROJECT_ID and VERTEX_LOCATION must be set.\")\n",
        "        return\n",
        "\n",
        "    print(\"Initializing Enhanced Deep Research System with FAISS RAG...\")\n",
        "\n",
        "    print(\"\\n--- Research Session Management ---\")\n",
        "\n",
        "    sessions = list_previous_sessions() # This will now print extensive debug info\n",
        "    print(f\"DEBUG (main_colab_runner_async): list_previous_sessions() returned: {sessions}\") # Log what list_previous_sessions returned\n",
        "\n",
        "    load_session_id = None\n",
        "\n",
        "    force_fresh_global_rag = False\n",
        "\n",
        "    action = input(\"Choose action: [N]ew session, [L]oad session, [F]resh start (clears ALL global RAG & sessions): \").strip().upper()\n",
        "\n",
        "    if action == 'L':\n",
        "        if not sessions:\n",
        "            print(\"No previous sessions found to load. Starting a new session.\")\n",
        "            action = 'N'\n",
        "        else:\n",
        "            print(\"Available sessions:\")\n",
        "            for i, (sid, q, dt) in enumerate(sessions): print(f\"  {i+1}. {sid} (Query: '{q}' - Last modified: {dt})\")\n",
        "            try:\n",
        "                choice_str = input(f\"Enter number of session to load (1-{len(sessions)}), or 0 for new: \").strip()\n",
        "                if not choice_str: action = 'N'; print(\"No choice, starting new session.\")\n",
        "                else:\n",
        "                    choice = int(choice_str) -1\n",
        "                    if choice == -1 :\n",
        "                         action = 'N'; print(\"Starting new session as per choice 0.\")\n",
        "                    elif 0 <= choice < len(sessions):\n",
        "                        load_session_id = sessions[choice][0]\n",
        "                        print(f\"Attempting to load session: {load_session_id}\")\n",
        "                    else:\n",
        "                        print(\"Invalid choice. Starting a new session.\")\n",
        "                        action = 'N'\n",
        "            except ValueError:\n",
        "                print(\"Invalid input. Starting a new session.\")\n",
        "                action = 'N'\n",
        "    elif action == 'F':\n",
        "        confirm_fresh = input(\"WARNING: This will DELETE ALL global RAG data (Faiss index, document store) AND ALL past session logs. Are you absolutely sure? (yes/no): \").strip().lower()\n",
        "        if confirm_fresh == 'yes':\n",
        "            force_fresh_global_rag = True\n",
        "            print(\"Proceeding with a FRESH START. All global RAG data and past sessions will be cleared.\")\n",
        "            if os.path.exists(SESSIONS_DIR):\n",
        "                import shutil\n",
        "                for item_name in os.listdir(SESSIONS_DIR):\n",
        "                    item_path = os.path.join(SESSIONS_DIR, item_name)\n",
        "                    if os.path.isdir(item_path):\n",
        "                        if item_name != \"global_rag\":\n",
        "                            try:\n",
        "                                shutil.rmtree(item_path)\n",
        "                                print(f\"  Deleted session directory: {item_path}\")\n",
        "                            except Exception as e_shutil:\n",
        "                                print(f\"  Error deleting session dir {item_path}: {e_shutil}\")\n",
        "            action = 'N'\n",
        "        else:\n",
        "            print(\"Fresh start aborted. Defaulting to new session.\")\n",
        "            action = 'N'\n",
        "\n",
        "    # Initialize DeepResearchSystem AFTER potentially setting force_fresh_global_rag\n",
        "    # load_previous_session_id is correctly passed here now.\n",
        "    research_system = DeepResearchSystem(\n",
        "        project_id=VERTEX_PROJECT_ID,\n",
        "        location=VERTEX_LOCATION,\n",
        "        load_previous_session_id=load_session_id if action == 'L' else None,\n",
        "        force_new_faiss_and_global_stores=force_fresh_global_rag\n",
        "    )\n",
        "\n",
        "    # Crucial check: if vertex_client is None after __init__, stop.\n",
        "    if not research_system.vertex_client:\n",
        "        print(\"STOPPING: DeepResearchSystem Vertex client failed to initialize (likely due to the ValueError during __init__). Cannot proceed.\")\n",
        "        return\n",
        "\n",
        "    user_query_for_session = \"\"\n",
        "    current_session_id_for_run = load_session_id if action == 'L' and load_session_id else None\n",
        "\n",
        "\n",
        "    if action == 'N' or not current_session_id_for_run :\n",
        "        try:\n",
        "            user_query_for_session = input(\"Please enter your research query for the new session: \").strip()\n",
        "            if not user_query_for_session:\n",
        "                print(\"No query entered. Using default query.\")\n",
        "                user_query_for_session = \"What are the latest advancements in quantum computing and their potential impact on cryptography?\"\n",
        "            current_session_id_for_run = None\n",
        "        except EOFError:\n",
        "            print(\"No input received (EOFError). Using default query for new session.\")\n",
        "            user_query_for_session = \"Latest AI impact on renewable energy\"\n",
        "\n",
        "    elif current_session_id_for_run and research_system.knowledge_base:\n",
        "        user_query_for_session = research_system.knowledge_base.original_query\n",
        "        print(f\"Continuing research for loaded session '{current_session_id_for_run}' with query: \\\"{user_query_for_session}\\\"\")\n",
        "        override_q = input(f\"Current query is '{user_query_for_session}'. Enter a new query to refine or override, or press Enter to continue with current: \").strip()\n",
        "        if override_q:\n",
        "            user_query_for_session = override_q\n",
        "            print(f\"Query for this run overridden to: '{user_query_for_session}'\")\n",
        "    else:\n",
        "        print(\"Error in session setup or KB not loaded. Exiting.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    if FAISS_AVAILABLE and research_system.faiss_manager:\n",
        "        ingest_choice = input(\"Do you want to ingest new local documents into the Global RAG store before starting research? (y/N): \").strip().lower()\n",
        "        if ingest_choice == 'y':\n",
        "            num_docs_to_ingest = 0\n",
        "            try:\n",
        "                num_docs_to_ingest_str = input(\"How many sample documents to create and ingest? (e.g., 1-3, or 0 to skip): \").strip()\n",
        "                if not num_docs_to_ingest_str: num_docs_to_ingest = 0\n",
        "                else: num_docs_to_ingest = int(num_docs_to_ingest_str)\n",
        "\n",
        "            except ValueError:\n",
        "                print(\"Invalid number. Skipping ingestion.\")\n",
        "\n",
        "            if num_docs_to_ingest > 0:\n",
        "                sample_docs_for_rag = []\n",
        "                print(\"\\n--- Preparing for Manual Document Ingestion ---\")\n",
        "                llm_suggestions_available = bool(research_system.vertex_client)\n",
        "                if not llm_suggestions_available:\n",
        "                    print(\"WARNING: Vertex client not available for automatic ID/summary generation. Manual input will be required for ID and summary.\")\n",
        "\n",
        "                for i in range(num_docs_to_ingest):\n",
        "                    print(f\"\\n--- Document {i+1}/{num_docs_to_ingest} ---\")\n",
        "                    doc_content = \"\"\n",
        "                    while not doc_content:\n",
        "                        doc_content = input(f\"Paste content for sample document {i+1} (min 50 chars): \").strip()\n",
        "                        if not doc_content:\n",
        "                            print(\"Content cannot be empty.\")\n",
        "                        elif len(doc_content) < 50:\n",
        "                            print(\"Content too short, please provide more substantial text (min 50 chars).\")\n",
        "                            doc_content = \"\" # Reset to loop\n",
        "\n",
        "                    # Default suggestions (if LLM fails or not available)\n",
        "                    default_suggested_id = f\"manual_{sanitize_filename(doc_content[:25], 25)}_{int(time.time() * 1000) % 100000}\" # Added ms\n",
        "                    default_suggested_summary = doc_content[:120] + \"...\"\n",
        "\n",
        "                    final_id_suggestion = default_suggested_id\n",
        "                    final_summary_suggestion = default_suggested_summary\n",
        "\n",
        "                    if llm_suggestions_available:\n",
        "                        print(\"   Generating suggestions for ID and summary using LLM...\")\n",
        "                        suggestion_prompt = (\n",
        "                            f\"Given the following document content, suggest a concise, descriptive ID (max 40 chars, \"\n",
        "                            f\"alphanumeric and hyphens only, e.g., 'key-concept-short-title') and a brief summary (max 150 chars).\\n\\n\"\n",
        "                            f\"DOCUMENT CONTENT (first 1000 chars):\\n{doc_content[:1000]}\\n\\n\"\n",
        "                            f\"Output ONLY in this JSON format: {{\\\"suggested_id\\\": \\\"your-id\\\", \\\"suggested_summary\\\": \\\"Your summary.\\\"}}\"\n",
        "                        )\n",
        "                        try:\n",
        "                            response_text, _ = await research_system._call_llm(\n",
        "                                system_prompt_text=\"You are an assistant that generates document IDs and summaries.\",\n",
        "                                user_content_text=suggestion_prompt,\n",
        "                                agent_type=\"Auxiliary_Lite_Task\"\n",
        "                            )\n",
        "                            _, parsed_suggestions_str = parse_llm_response(response_text)\n",
        "                            if parsed_suggestions_str:\n",
        "                                suggestions = json.loads(parsed_suggestions_str)\n",
        "                                if isinstance(suggestions, dict):\n",
        "                                    llm_id_sugg = suggestions.get(\"suggested_id\")\n",
        "                                    llm_sum_sugg = suggestions.get(\"suggested_summary\")\n",
        "                                    if llm_id_sugg:\n",
        "                                        final_id_suggestion = sanitize_filename(llm_id_sugg, 40)\n",
        "                                    if llm_sum_sugg:\n",
        "                                        final_summary_suggestion = llm_sum_sugg[:150]\n",
        "                                print(f\"   LLM Suggestion - ID: {final_id_suggestion}, Summary: {final_summary_suggestion}\")\n",
        "                        except Exception as e_suggest:\n",
        "                            print(f\"   Could not get LLM suggestions: {e_suggest}. Using default suggestions based on content.\")\n",
        "                            # final_id_suggestion and final_summary_suggestion already hold defaults\n",
        "\n",
        "                    # ID handling\n",
        "                    doc_id_to_use = final_id_suggestion\n",
        "                    if doc_id_to_use in research_system.master_document_store:\n",
        "                        print(f\"   Suggested ID '{doc_id_to_use}' already exists.\")\n",
        "                        user_doc_id_input = \"\"\n",
        "                        while not user_doc_id_input or user_doc_id_input in research_system.master_document_store:\n",
        "                            user_doc_id_input = input(f\"   Please enter a unique ID (or press Enter to slightly modify suggestion): \").strip()\n",
        "                            if not user_doc_id_input: # User pressed Enter\n",
        "                                doc_id_to_use = f\"{final_id_suggestion}_{int(time.time()*1000)%1000}\" # Append ms to make it unique\n",
        "                                if doc_id_to_use not in research_system.master_document_store:\n",
        "                                    print(f\"   Using modified unique ID: {doc_id_to_use}\")\n",
        "                                    break\n",
        "                                else: # Should be very rare\n",
        "                                    print(f\"   Modified ID '{doc_id_to_use}' also exists. Please enter manually.\")\n",
        "                                    user_doc_id_input = \"force_manual\" # to break inner and re-prompt outer\n",
        "                            elif user_doc_id_input in research_system.master_document_store:\n",
        "                                print(f\"   ID '{user_doc_id_input}' already exists. Please choose another.\")\n",
        "                            else: # User provided a new, unique ID\n",
        "                                doc_id_to_use = user_doc_id_input\n",
        "                                break\n",
        "                        if not user_doc_id_input and doc_id_to_use in research_system.master_document_store: # If fallback modification still failed\n",
        "                             doc_id_to_use = input(\"   Fallback ID also taken. Please enter a fully unique ID: \").strip() # Final manual attempt\n",
        "                             while not doc_id_to_use or doc_id_to_use in research_system.master_document_store:\n",
        "                                 doc_id_to_use = input(\"   ID cannot be empty or duplicate. Please enter a unique ID: \").strip()\n",
        "\n",
        "\n",
        "                    else: # Suggested ID is unique\n",
        "                        user_override_id = input(f\"   Use suggested ID '{doc_id_to_use}'? (Y/n for manual override): \").strip().lower()\n",
        "                        if user_override_id == 'n':\n",
        "                            temp_id = \"\"\n",
        "                            while not temp_id or temp_id in research_system.master_document_store:\n",
        "                                temp_id = input(f\"   Enter your unique ID: \").strip()\n",
        "                                if not temp_id: print(\"   ID cannot be empty.\")\n",
        "                                elif temp_id in research_system.master_document_store: print(f\"   ID '{temp_id}' already exists.\")\n",
        "                            doc_id_to_use = temp_id\n",
        "                        else:\n",
        "                            print(f\"   Using ID: {doc_id_to_use}\")\n",
        "\n",
        "                    doc_id = doc_id_to_use\n",
        "\n",
        "                    # Summary handling\n",
        "                    doc_summary_to_use = final_summary_suggestion\n",
        "                    user_override_summary = input(f\"   Use suggested summary '{doc_summary_to_use[:70]}...'? (Y/n for manual override): \").strip().lower()\n",
        "                    if user_override_summary == 'n':\n",
        "                        doc_summary_to_use = input(f\"   Enter your summary (max 300 chars): \")[:300].strip() or doc_summary_to_use # Fallback to suggestion if empty\n",
        "                    else:\n",
        "                        print(f\"   Using summary: {doc_summary_to_use[:70]}...\")\n",
        "\n",
        "                    doc_summary = doc_summary_to_use\n",
        "\n",
        "\n",
        "                    if doc_content and doc_id: # doc_id should always be set now\n",
        "                        sample_docs_for_rag.append({\"id\": doc_id, \"content\": doc_content, \"summary\": doc_summary})\n",
        "                    else:\n",
        "                        print(\"Critical error: Content or ID is missing after processing. Skipping this document.\")\n",
        "\n",
        "                if sample_docs_for_rag:\n",
        "                    await research_system.ingest_documents_into_rag(sample_docs_for_rag)\n",
        "                    print(f\"Completed ingestion of {len(sample_docs_for_rag)} documents.\")\n",
        "                else:\n",
        "                    print(\"No valid documents provided for ingestion.\")\n",
        "\n",
        "    else:\n",
        "        print(\"Faiss not available or Faiss manager not initialized. Skipping RAG document ingestion option.\")\n",
        "\n",
        "    print(f\"\\nTarget User Query for this run: \\\"{user_query_for_session}\\\"\")\n",
        "    overall_start_time = time.time()\n",
        "    final_report_str = \"Research did not complete successfully.\"\n",
        "\n",
        "    try:\n",
        "        final_report_str = await research_system.run_deep_research(\n",
        "            user_query_for_session,\n",
        "            current_session_id=current_session_id_for_run,\n",
        "            stream_final_report_override=True\n",
        "        )\n",
        "    except Exception as e_run:\n",
        "        print(f\"\\nAn UNEXPECTED error occurred during the research run: {e_run}\")\n",
        "        traceback.print_exc()\n",
        "    finally:\n",
        "        overall_end_time = time.time()\n",
        "        active_session_id_final = \"N/A\"\n",
        "        if research_system.knowledge_base:\n",
        "            active_session_id_final = research_system.knowledge_base.session_id\n",
        "\n",
        "        print(f\"\\n Deep research process for session '{active_session_id_final}' completed in {overall_end_time - overall_start_time:.2f} seconds.\")\n",
        "\n",
        "        if research_system.knowledge_base:\n",
        "            print(f\"\\n--- Final State for Session: {active_session_id_final} ---\")\n",
        "            print(f\"Original Query (this session): {research_system.knowledge_base.original_query}\")\n",
        "            print(f\"Query Focus (this specific run): {user_query_for_session}\")\n",
        "            print(f\"Completed {len(research_system.knowledge_base.completed_sub_queries)} sub-queries.\")\n",
        "            print(f\"Pending {len(research_system.knowledge_base.pending_sub_queries)} sub-queries: {research_system.knowledge_base.pending_sub_queries[:3]}...\")\n",
        "\n",
        "            print(\"\\n--- Research Log Summary (Final) ---\")\n",
        "            print(research_system._summarize_research_log_for_display(max_entries=100, max_len_per_entry=500))\n",
        "\n",
        "            if research_system.knowledge_base.errors:\n",
        "                print(\"\\n--- KnowledgeBase Errors Logged During Session ---\")\n",
        "                for err_idx, err_entry in enumerate(research_system.knowledge_base.errors):\n",
        "                    print(f\"  KB Error {err_idx+1}: Agent: {err_entry['agent']}, Msg: {err_entry['message']}\")\n",
        "                    if err_entry.get('details'): print(f\"    Details: {str(err_entry['details'])[:200]}...\")\n",
        "\n",
        "            print(f\"\\nAll output files for session '{active_session_id_final}' are in: {os.path.join(SESSIONS_DIR, active_session_id_final)}\")\n",
        "            print(f\"Global RAG data (shared across sessions) is in: {GLOBAL_RAG_DIR}\")\n",
        "        else:\n",
        "            print(\"KnowledgeBase was not available at the end of the run for final summary.\")\n",
        "\n",
        "\n",
        "def list_previous_sessions() -> List[Tuple[str, str, str]]:\n",
        "    print(f\"DEBUG (list_previous_sessions): Checking for sessions in directory: '{os.path.abspath(SESSIONS_DIR)}'\") # Log absolute path\n",
        "    previous_sessions = []\n",
        "    if not os.path.exists(SESSIONS_DIR):\n",
        "        print(f\"DEBUG (list_previous_sessions): Sessions directory '{SESSIONS_DIR}' not found at all.\")\n",
        "        return []\n",
        "    if not os.path.isdir(SESSIONS_DIR):\n",
        "        print(f\"DEBUG (list_previous_sessions): Path '{SESSIONS_DIR}' exists but is not a directory.\")\n",
        "        return []\n",
        "\n",
        "    items_in_sessions_dir = []\n",
        "    try:\n",
        "        items_in_sessions_dir = os.listdir(SESSIONS_DIR)\n",
        "        print(f\"DEBUG (list_previous_sessions): Found {len(items_in_sessions_dir)} items in '{SESSIONS_DIR}': {items_in_sessions_dir}\")\n",
        "    except Exception as e_listdir:\n",
        "        print(f\"DEBUG (list_previous_sessions): Error listing directory '{SESSIONS_DIR}': {e_listdir}\")\n",
        "        return []\n",
        "\n",
        "    for item_name in items_in_sessions_dir:\n",
        "        session_dir_path = os.path.join(SESSIONS_DIR, item_name)\n",
        "        print(f\"DEBUG (list_previous_sessions): Checking item '{item_name}' at path '{session_dir_path}'\")\n",
        "        if os.path.isdir(session_dir_path) and item_name != \"global_rag\":\n",
        "            kb_path = os.path.join(session_dir_path, \"knowledge_base.json\")\n",
        "            print(f\"DEBUG (list_previous_sessions):  Item '{item_name}' is a directory (not global_rag). Checking for KB: '{kb_path}'\")\n",
        "            if os.path.exists(kb_path):\n",
        "                print(f\"DEBUG (list_previous_sessions):   Found knowledge_base.json for session '{item_name}'.\")\n",
        "                try:\n",
        "                    with open(kb_path, 'r', encoding='utf-8') as f: kb_data = json.load(f)\n",
        "                    query = kb_data.get(\"original_query\", \"Unknown Query\")\n",
        "                    mod_time_epoch = os.path.getmtime(kb_path)\n",
        "                    mod_time_readable = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(mod_time_epoch))\n",
        "                    previous_sessions.append((item_name, query, mod_time_readable, mod_time_epoch)) # Add epoch for sorting\n",
        "                    print(f\"DEBUG (list_previous_sessions):    Successfully loaded KB for '{item_name}'. Query: '{query[:50]}...', ModTime: {mod_time_readable}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"DEBUG (list_previous_sessions):    Could not read/parse KB for session {item_name}: {e}\")\n",
        "                    previous_sessions.append((item_name, \"Error reading query\", \"N/A\", 0))\n",
        "            else:\n",
        "                print(f\"DEBUG (list_previous_sessions):   knowledge_base.json NOT FOUND for session '{item_name}'.\")\n",
        "        else:\n",
        "            print(f\"DEBUG (list_previous_sessions):  Item '{item_name}' is NOT a session directory (or is global_rag).\")\n",
        "\n",
        "    if previous_sessions:\n",
        "        previous_sessions.sort(key=lambda x: x[3], reverse=True) # Sort by epoch timestamp\n",
        "        # Remove epoch timestamp before returning, if not needed elsewhere\n",
        "        previous_sessions_final = [(sid, q, dt_readable) for sid, q, dt_readable, _ in previous_sessions]\n",
        "        print(f\"DEBUG (list_previous_sessions): Returning {len(previous_sessions_final)} sessions.\")\n",
        "        return previous_sessions_final\n",
        "    else:\n",
        "        print(f\"DEBUG (list_previous_sessions): No valid previous sessions found after checking all items.\")\n",
        "        return []\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    try:\n",
        "        import nest_asyncio\n",
        "        nest_asyncio.apply()\n",
        "        print(\"nest_asyncio applied (if in a compatible environment like Jupyter/Colab).\")\n",
        "    except ImportError:\n",
        "        print(\"nest_asyncio not found. If running in Jupyter/Colab, install for optimal asyncio behavior.\")\n",
        "    except RuntimeError as e_nest:\n",
        "        print(f\"nest_asyncio runtime issue: {e_nest}. Proceeding, but be aware of potential asyncio conflicts if already in an event loop.\")\n",
        "\n",
        "    try:\n",
        "        asyncio.run(main_colab_runner_async())\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nResearch process interrupted by user (KeyboardInterrupt).\")\n",
        "    except RuntimeError as e_async_run:\n",
        "        if \"cannot be called from a running event loop\" in str(e_async_run).lower() or \\\n",
        "           \"asyncio.run() cannot be called from a running event loop\" in str(e_async_run).lower():\n",
        "            print(\"\\nERROR: asyncio.run() failed. This often happens in environments like Jupyter notebooks that manage their own event loop.\")\n",
        "            print(\"Consider running the main_colab_runner_async() function directly if you are in such an environment and nest_asyncio is not fully effective, e.g., by calling `await main_colab_runner_async()` in a cell.\")\n",
        "        else:\n",
        "            print(f\"\\nA main asyncio RuntimeError occurred: {e_async_run}\")\n",
        "            traceback.print_exc()\n",
        "    except Exception as e_main_global:\n",
        "        print(f\"\\nAn unexpected error occurred in the main execution block: {e_main_global}\")\n",
        "        traceback.print_exc()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "worgtempy02 (22 Jun 2025, 22:52:50)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}